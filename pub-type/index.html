<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<meta name="author" content="Yannis Avrithis">
		<meta name="description" content="Yannis Avrithis - Home page">
		<meta name="keywords" content="Yannis Avrithis computer vision machine learning deep learning image search indexing retrieval">
		<title>Yannis Avrithis - Publications (by type)</title>

		<!-- bootstrap -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
		<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

		<!-- fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:300" rel="stylesheet">

		<!-- mathjax -->
		<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML" async></script>

		<!-- font awesome -->
		<script defer src="../web/js/fa.js"></script>

		<!-- styles -->
		<link rel="stylesheet" href="../web/css/home.css">

		<!-- scripts -->
		<script src="../web/js/bs-docs.min.js"></script>
		<script src="../web/js/email.js"></script>
		<script src="../web/js/scroll.js"></script>

		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="180x180" href="../web/ico/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../web/ico/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../web/ico/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">

	</head>
	<body data-spy="scroll" data-target="#nav">
		<header class="navbar navbar-expand navbar-dark flex-column flex-md-row bd-navbar">
			<a class="navbar-brand mr-0 mr-md-2 brand" href="../">Y</a>
			<div class="navbar-nav-scroll">
				<ul class="navbar-nav bd-navbar-nav flex-row">
					<li class="nav-item">
						<a class="nav-link" href="../">Home</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="../cv">Resume</a>
					</li>
					<li class="nav-item dropdown">
						<a class="nav-link dropdown-toggle active" href="#" role="button" data-toggle="dropdown" aria-expanded="false">
							Publications
						</a>
						<div class="dropdown-menu">
							<a class="dropdown-item" href="../pub">By year</a>
							<div class="dropdown-divider"></div>
							<a class="dropdown-item" href="../pub-cite">By citations</a>
							<a class="dropdown-item active" href="../pub-type">By type</a>
							<a class="dropdown-item" href="../pub-auth">By author</a>
						</div>
					</li>
					</li>
					<li class="nav-item dropdown">
						<a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-expanded="false">
							Code/Data
						</a>
						<div class="dropdown-menu">
							<a class="dropdown-item" href="../code">By type</a>
							<div class="dropdown-divider"></div>
							<a class="dropdown-item" href="../code-star">By stars</a>
							<a class="dropdown-item" href="../code-year">By year</a>
							<a class="dropdown-item" href="../code-auth">By author</a>
						</div>
					</li>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="../time">Timeline</a>
					</li>
				</ul>
			</div>
			<ul class="navbar-nav flex-row ml-md-auto d-none d-md-flex">
				<li class="nav-item">
					<script>ema2(ema_net(), "nav-link p-2", "Email");</script>
						<i class="fal fa-at"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://arxiv.org/search/?searchtype=author&query=Avrithis%2C+Y" title="arXiv">
						<i class="faa fa-arxiv"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://dblp.org/pers/hd/a/Avrithis:Yannis" title="DBLP">
						<i class="faa fa-dblp"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://scholar.google.com/citations?user=AF2SxG0AAAAJ&sortby=pubdate" title="Google Scholar">
						<i class="faa fa-google-scholar"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://www.semanticscholar.org/author/Yannis-Avrithis/1744904" title="Semantic Scholar">
						<i class="faa fa-semantic-scholar"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://www.linkedin.com/in/yannisavrithis/" title="LinkedIn">
						<i class="fab fa-linkedin-in"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://github.com/iavr" title="GitHub">
						<i class="fab fa-github"></i>
					</a>
				</li>
			</ul>
		</header>


<div class="container-fluid" id="pub">
	<div class="row">

		<div class="col-md-auto side-bar">
			<nav id="nav">

				<div class="show-md">
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#conf-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#conf">
							<span class="mr">Conferences</span>
							<i class="fal fa-books"></i>
						</a>
						<div class="collapse show nav side-nav" id="conf-col">
							<a class="nav-link" href="#conf-2024">2024</a>
							<a class="nav-link" href="#conf-2023">2023</a>
							<a class="nav-link" href="#conf-2022">2022</a>
							<a class="nav-link" href="#conf-2021">2021</a>
							<a class="nav-link" href="#conf-2020">2020</a>
							<a class="nav-link" href="#conf-2019">2019</a>
							<a class="nav-link" href="#conf-2018">2018</a>
							<a class="nav-link" href="#conf-2017">2017</a>
							<a class="nav-link" href="#conf-2016">2016</a>
							<a class="nav-link" href="#conf-2015">2015</a>
							<a class="nav-link" href="#conf-2014">2014</a>
							<a class="nav-link" href="#conf-2013">2013</a>
							<a class="nav-link" href="#conf-2012">2012</a>
							<a class="nav-link" href="#conf-2011">2011</a>
							<a class="nav-link" href="#conf-2010">2010</a>
							<a class="nav-link" href="#conf-2009">2009</a>
							<a class="nav-link" href="#conf-2008">2008</a>
							<a class="nav-link" href="#conf-2007">2007</a>
							<a class="nav-link" href="#conf-2006">2006</a>
							<a class="nav-link" href="#conf-2005">2005</a>
							<a class="nav-link" href="#conf-2004">2004</a>
							<a class="nav-link" href="#conf-2003">2003</a>
							<a class="nav-link" href="#conf-2002">2002</a>
							<a class="nav-link" href="#conf-2001">2001</a>
							<a class="nav-link" href="#conf-2000">2000</a>
							<a class="nav-link" href="#conf-1999">1999</a>
							<a class="nav-link" href="#conf-1998">1998</a>
							<a class="nav-link" href="#conf-1997">1997</a>
							<a class="nav-link" href="#conf-1993">1993</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#report-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#report">
							<span class="mr">Preprints</span>
							<i class="fal fa-memo"></i>
						</a>
						<div class="collapse show nav side-nav" id="report-col">
							<a class="nav-link" href="#report-2024">2024</a>
							<a class="nav-link" href="#report-2023">2023</a>
							<a class="nav-link" href="#report-2022">2022</a>
							<a class="nav-link" href="#report-2021">2021</a>
							<a class="nav-link" href="#report-2020">2020</a>
							<a class="nav-link" href="#report-2019">2019</a>
							<a class="nav-link" href="#report-2018">2018</a>
							<a class="nav-link" href="#report-2017">2017</a>
							<a class="nav-link" href="#report-2016">2016</a>
							<a class="nav-link" href="#report-2008">2008</a>
							<a class="nav-link" href="#report-2007">2007</a>
							<a class="nav-link" href="#report-2006">2006</a>
							<a class="nav-link" href="#report-1999">1999</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#book-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#book">
							<span class="mr">Chapters</span>
							<i class="fal fa-book-section"></i>
						</a>
						<div class="collapse show nav side-nav" id="book-col">
							<a class="nav-link" href="#book-2022">2022</a>
							<a class="nav-link" href="#book-2011">2011</a>
							<a class="nav-link" href="#book-2009">2009</a>
							<a class="nav-link" href="#book-2008">2008</a>
							<a class="nav-link" href="#book-2006">2006</a>
							<a class="nav-link" href="#book-2005">2005</a>
							<a class="nav-link" href="#book-2002">2002</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#journ-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#journ">
							<span class="mr">Journals</span>
							<i class="fal fa-book-open"></i>
						</a>
						<div class="collapse show nav side-nav" id="journ-col">
							<a class="nav-link" href="#journ-2021">2021</a>
							<a class="nav-link" href="#journ-2020">2020</a>
							<a class="nav-link" href="#journ-2019">2019</a>
							<a class="nav-link" href="#journ-2016">2016</a>
							<a class="nav-link" href="#journ-2015">2015</a>
							<a class="nav-link" href="#journ-2014">2014</a>
							<a class="nav-link" href="#journ-2013">2013</a>
							<a class="nav-link" href="#journ-2011">2011</a>
							<a class="nav-link" href="#journ-2009">2009</a>
							<a class="nav-link" href="#journ-2008">2008</a>
							<a class="nav-link" href="#journ-2007">2007</a>
							<a class="nav-link" href="#journ-2006">2006</a>
							<a class="nav-link" href="#journ-2003">2003</a>
							<a class="nav-link" href="#journ-2001">2001</a>
							<a class="nav-link" href="#journ-2000">2000</a>
							<a class="nav-link" href="#journ-1999">1999</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#thesis-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#thesis">
							<span class="mr">Theses</span>
							<i class="fal fa-scroll"></i>
						</a>
						<div class="collapse show nav side-nav" id="thesis-col">
							<a class="nav-link" href="#thesis-2020">2020</a>
							<a class="nav-link" href="#thesis-2001">2001</a>
							<a class="nav-link" href="#thesis-1994">1994</a>
							<a class="nav-link" href="#thesis-1993">1993</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#vol-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#vol">
							<span class="mr">Volumes</span>
							<i class="fal fa-book"></i>
						</a>
						<div class="collapse show nav side-nav" id="vol-col">
							<a class="nav-link" href="#vol-2009">2009</a>
							<a class="nav-link" href="#vol-2007">2007</a>
							<a class="nav-link" href="#vol-2006">2006</a>
						</div>
					</div>
				</div>

				<div class="hide-md">
					<div class="nav side-nav">
						<a class="nav-link" href="#conf">
							<span class="icon-in"><i class="fal fa-books"></i></span>
						</a>
						<a class="sm nav-link" href="#conf-2024">24</a>
						<a class="sm nav-link" href="#conf-2023">23</a>
						<a class="sm nav-link" href="#conf-2022">22</a>
						<a class="sm nav-link" href="#conf-2021">21</a>
						<a class="sm nav-link" href="#conf-2020">20</a>
						<a class="sm nav-link" href="#conf-2019">19</a>
						<a class="sm nav-link" href="#conf-2018">18</a>
						<a class="sm nav-link" href="#conf-2017">17</a>
						<a class="sm nav-link" href="#conf-2016">16</a>
						<a class="sm nav-link" href="#conf-2015">15</a>
						<a class="sm nav-link" href="#conf-2014">14</a>
						<a class="sm nav-link" href="#conf-2013">13</a>
						<a class="sm nav-link" href="#conf-2012">12</a>
						<a class="sm nav-link" href="#conf-2011">11</a>
						<a class="sm nav-link" href="#conf-2010">10</a>
						<a class="sm nav-link" href="#conf-2009">09</a>
						<a class="sm nav-link" href="#conf-2008">08</a>
						<a class="sm nav-link" href="#conf-2007">07</a>
						<a class="sm nav-link" href="#conf-2006">06</a>
						<a class="sm nav-link" href="#conf-2005">05</a>
						<a class="sm nav-link" href="#conf-2004">04</a>
						<a class="sm nav-link" href="#conf-2003">03</a>
						<a class="sm nav-link" href="#conf-2002">02</a>
						<a class="sm nav-link" href="#conf-2001">01</a>
						<a class="sm nav-link" href="#conf-2000">00</a>
						<a class="sm nav-link" href="#conf-1999">99</a>
						<a class="sm nav-link" href="#conf-1998">98</a>
						<a class="sm nav-link" href="#conf-1997">97</a>
						<a class="sm nav-link" href="#conf-1993">93</a>
						<a class="nav-link" href="#report">
							<span class="icon-in"><i class="fal fa-memo"></i></span>
						</a>
						<a class="sm nav-link" href="#report-2024">24</a>
						<a class="sm nav-link" href="#report-2023">23</a>
						<a class="sm nav-link" href="#report-2022">22</a>
						<a class="sm nav-link" href="#report-2021">21</a>
						<a class="sm nav-link" href="#report-2020">20</a>
						<a class="sm nav-link" href="#report-2019">19</a>
						<a class="sm nav-link" href="#report-2018">18</a>
						<a class="sm nav-link" href="#report-2017">17</a>
						<a class="sm nav-link" href="#report-2016">16</a>
						<a class="sm nav-link" href="#report-2008">08</a>
						<a class="sm nav-link" href="#report-2007">07</a>
						<a class="sm nav-link" href="#report-2006">06</a>
						<a class="sm nav-link" href="#report-1999">99</a>
						<a class="nav-link" href="#book">
							<span class="icon-in"><i class="fal fa-book-section"></i></span>
						</a>
						<a class="sm nav-link" href="#book-2022">22</a>
						<a class="sm nav-link" href="#book-2011">11</a>
						<a class="sm nav-link" href="#book-2009">09</a>
						<a class="sm nav-link" href="#book-2008">08</a>
						<a class="sm nav-link" href="#book-2006">06</a>
						<a class="sm nav-link" href="#book-2005">05</a>
						<a class="sm nav-link" href="#book-2002">02</a>
						<a class="nav-link" href="#journ">
							<span class="icon-in"><i class="fal fa-book-open"></i></span>
						</a>
						<a class="sm nav-link" href="#journ-2021">21</a>
						<a class="sm nav-link" href="#journ-2020">20</a>
						<a class="sm nav-link" href="#journ-2019">19</a>
						<a class="sm nav-link" href="#journ-2016">16</a>
						<a class="sm nav-link" href="#journ-2015">15</a>
						<a class="sm nav-link" href="#journ-2014">14</a>
						<a class="sm nav-link" href="#journ-2013">13</a>
						<a class="sm nav-link" href="#journ-2011">11</a>
						<a class="sm nav-link" href="#journ-2009">09</a>
						<a class="sm nav-link" href="#journ-2008">08</a>
						<a class="sm nav-link" href="#journ-2007">07</a>
						<a class="sm nav-link" href="#journ-2006">06</a>
						<a class="sm nav-link" href="#journ-2003">03</a>
						<a class="sm nav-link" href="#journ-2001">01</a>
						<a class="sm nav-link" href="#journ-2000">00</a>
						<a class="sm nav-link" href="#journ-1999">99</a>
						<a class="nav-link" href="#thesis">
							<span class="icon-in"><i class="fal fa-scroll"></i></span>
						</a>
						<a class="sm nav-link" href="#thesis-2020">20</a>
						<a class="sm nav-link" href="#thesis-2001">01</a>
						<a class="sm nav-link" href="#thesis-1994">94</a>
						<a class="sm nav-link" href="#thesis-1993">93</a>
						<a class="nav-link" href="#vol">
							<span class="icon-in"><i class="fal fa-book"></i></span>
						</a>
						<a class="sm nav-link" href="#vol-2009">09</a>
						<a class="sm nav-link" href="#vol-2007">07</a>
						<a class="sm nav-link" href="#vol-2006">06</a>
					</div>
				</div>

			</nav>
		</div>

		<main class="col-md">
			<div class="container ver-pad">				<div class="alert warn">
					<h2>
						<a class="anchor" id="copyright"></a>
						Copyright notice
					</h2>
					<div class="just">
						This material is presented to ensure timely dissemination of scholarly and technical work. Copyright and all rights therein are retained by authors or by other copyright holders. Anyone copying this information is expected to adhere to the terms and constraints invoked by each author's copyright. In most cases, these works may not be reposted without the explicit permission of the copyright holder.
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="conf"></a>
					<span class="mr">Conference proceedings</span>
					<i class="fal fa-books"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="conf-2024"></a>
					2024
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C137"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C137" id="tog-C137">
							<i class="left-60 tog far fa-chevron-down"></i>
							CA-Stream: Attention-Based Pooling for Interpretable Image Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Torres Figueroa, H. Zhang, R. Sicre, S. Ayache, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">XAI4CV/CVPR&nbsp;2024</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C137.cvpr-xai4cv24.ca-stream.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C137.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-C137">
						<div class="pub-ref">
							In Proc. <em>3rd Workshop on Explainable AI for Computer Vision</em><br>
							part of <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Seattle, WA, US  <span class="bull"></span> Jun 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C137.cvpr-xai4cv24.ca-stream.svg"><img src="../data/pub/thumb/wide/conf/C137.cvpr-xai4cv24.ca-stream.svg" alt="C137 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Explanations obtained from transformer-based architectures, in the form of raw attention, can be seen as a class agnostic saliency map. Additionally, attention-based pooling serves as a form of masking in feature space. Motivated by this observation, we design an attention-based pooling mechanism intended to replace global average pooling during inference. This mechanism, called Cross Attention Stream (CA-Stream), comprises a stream of cross attention blocks interacting with features at different network levels. CA-Stream enhances interpretability properties in existing image recognition models, while preserving their recognition properties.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C137,
   title = {{CA}-Stream: Attention-based pooling for interpretable image recognition},
   author = {Torres Figueroa, Felipe and Zhang, Hanwei and Sicre, Ronan and Ayache, Stephane and Avrithis, Yannis},
   booktitle = {Proceedings of 3rd Workshop on Explainable AI for Computer Vision (XAI4CV), part of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Seattle, WA, US},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C136"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C136" id="tog-C136">
							<i class="left-60 tog far fa-chevron-down"></i>
							Composed Image Retrieval for Remote Sensing
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">B. Psomas, I. Kakogeorgiou, N. Efthymiadis, O. Chum, Y. Avrithis, K. Karantzalos</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">IGARSS&nbsp;2024</span>
							<span class="xtra but mr">Oral</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C136.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-C136">
						<div class="pub-ref">
							In Proc. <em>IEEE International Geoscience and Remote Sensing Symposium</em><br>
							Athens, Greece  <span class="bull"></span> Jul 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C136.igarss24.cir-rs.svg"><img src="../data/pub/thumb/wide/conf/C136.igarss24.cir-rs.svg" alt="C136 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The surge in data volume within the field of remote sensing has necessitated efficient methods for retrieving relevant information from extensive image archives. Conventional unimodal queries, whether visual or textual, are insufficient and restrictive. To address this limitation, we introduce the task of composed image retrieval in remote sensing, allowing users to combine query images with a textual part that modifies attributes such as color, texture, context, or more, thereby enhancing the expressivity of the query.
								</p>
								<p>
									We demonstrate that a vision-language model possesses sufficient descriptive power and, when coupled with the proposed fusion method, eliminates the necessity for further learning. We present a new evaluation benchmark focused on shape, color, density, and quantity modifications. Our work not only sets the state-of-the-art for this task, but also serves as a foundational step in addressing a gap in the field of remote sensing image retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C136,
   title = {Composed Image Retrieval for Remote Sensing},
   author = {Psomas, Bill and Kakogeorgiou, Ioannis and Efthymiadis, Nikos and Chum, Ondrej and Avrithis, Yannis and Karantzalos, Konstantinos},
   booktitle = {Proceedings of IEEE International Geoscience and Remote Sensing Symposium (IGARSS) (Oral)},
   month = {7},
   address = {Athens, Greece},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C135"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C135" id="tog-C135">
							<i class="left-60 tog far fa-chevron-down"></i>
							On Train-Test Class Overlap and Detection for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, J. Yoon, T. Hwang, S. Choi, Y.H. Gu, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CVPR&nbsp;2024</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C135.cvpr24.gld-clean.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C135.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C135.cvpr24.gld-clean-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
								<a class="lnk mr2" href="../code/#rgldv2-clean" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-C135">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Seattle, WA, US  <span class="bull"></span> Jun 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C135.cvpr24.gld-clean.svg"><img src="../data/pub/thumb/wide/conf/C135.cvpr24.gld-clean.svg" alt="C135 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris, the most popular evaluation set. By comparing the original and the new $\mathcal{R}$GLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.
								</p>
								<p>
									What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new $\mathcal{R}$GLDv2-clean. Our dataset is available at <a href="https://github.com/dealicious-inc/RGLDv2-clean">https://github.com/dealicious-inc/RGLDv2-clean</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C135,
   title = {On Train-Test Class Overlap and Detection for Image Retrieval},
   author = {Song, Chull Hwan and Yoon, Jooyoung and Hwang, Taebaek and Choi, Shunghyun and Gu, Yeong Hyeon and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Seattle, WA, US},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C134"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C134" id="tog-C134">
							<i class="left-60 tog far fa-chevron-down"></i>
							<span class="mr">Is Imagenet Worth 1 Video? Learning Strong Image Encoders from 1 Long Unlabelled Video</span>
							<a class="lnk award" href="https://blog.iclr.cc/2024/05/06/iclr-2024-outstanding-paper-awards/" title="Outstanding Paper Honorable Mention">
								<i class="fal fa-award"></i>
							</a>
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, M.N. Rizve, J. Carreira, Y.M. Asano, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openreview.net/forum?id=Yen1lGns2o" title="Open access">ICLR&nbsp;2024</a>
							<a class="xtra but mr" href="https://iclr.cc/virtual/2024/oral/19752">Oral</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C134.iclr24.walking-tours.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C134.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C134.iclr24.walking-tours-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C134.iclr24.walking-tours-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/video/conf/C134.iclr24.walking-tours.mp4" title="Video">
								<i class="fal fa-video"></i>
							</a>
								<a class="lnk mr2" href="../code/#dora" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
								<a class="lnk mr2" href="../code/#wtours" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://shashankvkt.github.io/dora" title="Project home">
									<i class="fal fa-home"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C134">
						<div class="pub-ref">
							In Proc. <em>International Conference on Learning Representations</em><br>
							Vienna, Austria  <span class="bull"></span> May 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C134.iclr24.walking-tours.svg"><img src="../data/pub/thumb/wide/conf/C134.iclr24.walking-tours.svg" alt="C134 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.
								</p>
								<p>
									Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA, leads to attention maps that DiscOver and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C134,
   title = {Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video},
   author = {Venkataramanan, Shashanka and Rizve, Mamshad Nayeem and Carreira, Jo\~ao and Asano, Yuki M. and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Learning Representations (ICLR) (Oral). Outstanding Paper Honorable Mention},
   month = {5},
   address = {Vienna, Austria},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C133"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C133" id="tog-C133">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Learning Paradigm for Interpretable Gradients
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Torres Figueroa, H. Zhang, R. Sicre, Y. Avrithis, S. Ayache</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.scitepress.org/PublicationsDetail.aspx?ID=X1QcboTFsSU=&t=1" title="Open access">VISAPP&nbsp;2024</a>
							<span class="xtra but mr">Oral</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C133.visapp24.gbp.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C133.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C133.visapp24.gbp-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:prdVHNxh-e8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/cdb5d53de98e800806c956d8c52827a06ec131ca" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C133">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision Theory and Applications</em><br>
							Rome, Italy  <span class="bull"></span> Feb 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C133.visapp23.gbp.svg"><img src="../data/pub/thumb/wide/conf/C133.visapp23.gbp.svg" alt="C133 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper studies interpretability of convolutional networks by means of saliency maps. Most approaches based on Class Activation Maps (CAM) combine information from fully connected layers and gradient through variants of backpropagation. However, it is well understood that gradients are noisy and alternatives like guided backpropagation have been proposed to obtain better visualization at inference. In this work, we present a novel training approach to improve the quality of gradients for interpretability. In particular, we introduce a regularization loss such that the gradient with respect to the input image obtained by standard backpropagation is similar to the gradient obtained by guided backpropagation. We find that the resulting gradient is qualitatively less noisy and improves quantitatively the interpretability properties of different networks, using several interpretability methods.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C133,
   title = {A Learning Paradigm for Interpretable Gradients},
   author = {Torres Figueroa, Felipe and Zhang, Hanwei and Sicre, Ronan and Avrithis, Yannis and Ayache, Stephane},
   booktitle = {Proceedings of International Conference on Computer Vision Theory and Applications (VISAPP) (Oral)},
   month = {2},
   address = {Rome, Italy},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C132"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C132" id="tog-C132">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive Manifold for Imbalanced Transductive Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/WACV2024/html/Lazarou_Adaptive_Manifold_for_Imbalanced_Transductive_Few-Shot_Learning_WACV_2024_paper.html" title="Open access">WACV&nbsp;2024</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C132.wacv24.few-imbalanced.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C132.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C132.wacv24.few-imbalanced-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C132.wacv24.few-imbalanced-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#am" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:PaBasH6fAo0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15597785050321487604" title="Citations @ Google Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C132">
						<div class="pub-ref">
							In Proc. <em>IEEE Winter Conference on Applications of Computer Vision</em><br>
							Waikoloa, HI, US  <span class="bull"></span> Jan 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C132.wacv24.few-imbalanced.png"><img src="../data/pub/thumb/wide/conf/C132.wacv24.few-imbalanced.png" alt="C132 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Transductive few-shot learning algorithms have showed substantially superior performance over their inductive counterparts by leveraging the unlabeled queries at inference. However, the vast majority of transductive methods are evaluated on perfectly class-balanced benchmarks. It has been shown that they undergo remarkable drop in performance under a more realistic, imbalanced setting.
								</p>
								<p>
									To this end, we propose a novel algorithm to address imbalanced transductive few-shot learning, named Adaptive Manifold. Our algorithm exploits the underlying manifold of the labeled examples and unlabeled queries by using manifold similarity to predict the class probability distribution of every query. It is parameterized by one centroid per class and a set of manifold parameters that determine the manifold. All parameters are optimized by minimizing a loss function that can be tuned towards class-balanced or imbalanced distributions. The manifold similarity shows substantial improvement over Euclidean distance, especially in the 1-shot setting.
								</p>
								<p>
									Our algorithm outperforms all other state of the art methods in three benchmark datasets, namely miniImageNet, tieredImageNet and CUB, and two different backbones, namely ResNet-18, WideResNet-28-10. In certain cases, our algorithm outperforms the previous state of the art by as much as 4.2%.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C132,
   title = {Adaptive manifold for imbalanced transductive few-shot learning},
   author = {Lazarou, Michalis and Avrithis, Yannis and Stathaki, Tania},
   booktitle = {Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)},
   month = {1},
   address = {Waikoloa, HI, US},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2023"></a>
					2023
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C131"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C131" id="tog-C131">
							<i class="left-60 tog far fa-chevron-down"></i>
							Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, E. Kijak, L. Amsaleg, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/c3532dd633e600e9f6db57aa7ae0c858-Abstract-Conference.html" title="Open access">NeurIPS&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C131.neurips23.multimix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C131.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C131.neurips23.multimix-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/nips/VenkataramananK23" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:7H_MAutzIkAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11801241650801276671" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/05e97971aa58b8b99f19b92ddf980a3c8148e701" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/05e97971aa58b8b99f19b92ddf980a3c8148e701#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C131">
						<div class="pub-ref">
							In Proc. <em>Conference on Neural Information Processing Systems</em><br>
							New Orleans, LA, US  <span class="bull"></span> Dec 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C131.neurips23.multimix.png"><img src="../data/pub/thumb/wide/conf/C131.neurips23.multimix.png" alt="C131 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Mixup refers to interpolation-based data augmentation, originally motivated as a way to go beyond empirical risk minimization (ERM). Its extensions mostly focus on the definition of interpolation and the space (input or embedding) where it takes place, while the augmentation process itself is less studied. In most methods, the number of generated examples is limited to the mini-batch size and the number of examples being interpolated is limited to two (pairs), in the input space.
								</p>
								<p>
									We make progress in this direction by introducing MultiMix, which generates an arbitrarily large number of interpolated examples beyond the mini-batch size, and interpolates the entire mini-batch in the embedding space. Effectively, we sample on the entire convex hull of the mini-batch rather than along linear segments between pairs of examples.
								</p>
								<p>
									On sequence data we further extend to Dense MultiMix. We densely interpolate features and target labels at each spatial location and also apply the loss densely. To mitigate the lack of dense labels, we inherit labels from examples and weight interpolation factors by attention as a measure of confidence.
								</p>
								<p>
									Overall, we increase the number of loss terms per mini-batch by orders of magnitude at little additional cost. This is only possible because of interpolating in the embedding space. We empirically show that our solutions yield significant improvement over state-of-the-art mixup methods on four different benchmarks, despite interpolation being only linear. By analyzing the embedding space, we show that the classes are more tightly clustered and uniformly spread over the embedding space, thereby explaining the improved behavior.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C131,
   title = {Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples},
   author = {Venkataramanan, Shashanka and Kijak, Ewa and Amsaleg, Laurent and Avrithis, Yannis},
   booktitle = {Proceedings of Conference on Neural Information Processing Systems (NeurIPS)},
   month = {12},
   address = {New Orleans, LA, US},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C130"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C130" id="tog-C130">
							<i class="left-60 tog far fa-chevron-down"></i>
							Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Engin, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/ICCV2023W/CLVL/html/Engin_Zero-Shot_and_Few-Shot_Video_Question_Answering_with_Multi-Modal_Prompts_ICCVW_2023_paper.html" title="Open access">CLVL/ICCV&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C130.iccv-clvl23.vitis.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C130.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C130.iccv-clvl23.vitis-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C130.iccv-clvl23.vitis-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C130.iccv-clvl23.vitis-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/video/conf/C130.iccv-clvl23.vitis.mp4" title="Video">
								<i class="fal fa-video"></i>
							</a>
								<a class="lnk mr2" href="../code/#vitis" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://engindeniz.github.io/vitis" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccvw60793.2023.00298" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccvw/EnginA23" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:PYBJJbyH-FwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/185e79641a8e7b18ac5a73b8c3cb82fdee3a0c6d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C130">
						<div class="pub-ref">
							In Proc. <em>5th Workshop on Closing the Loop Between Vision and Language</em><br>
							part of <em>International Conference on Computer Vision</em><br>
							Paris, France  <span class="bull"></span> Oct 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C130.iccv-clvl23.vitis.png"><img src="../data/pub/thumb/wide/conf/C130.iccv-clvl23.vitis.png" alt="C130 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Recent vision-language models are driven by large-scale pretrained models. However, adapting pretrained models on limited data presents challenges such as overfitting, catastrophic forgetting, and the cross-modal gap between vision and language. We introduce a parameter-efficient method to address these challenges, combining multimodal prompt learning and a transformer-based mapping network, while keeping the pretrained models frozen. Our experiments on several video question answering benchmarks demonstrate the superiority of our approach in terms of performance and parameter efficiency on both zero-shot and few-shot settings. Our code is available at <a href="https://engindeniz.github.io/vitis">https://engindeniz.github.io/vitis</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C130,
   title = {Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts},
   author = {Engin, Deniz and Avrithis, Yannis},
   booktitle = {Proceedings of 5th Workshop on Closing the Loop Between Vision and Language (CLVL), part of International Conference on Computer Vision (ICCV)},
   month = {10},
   address = {Paris, France},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C129"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C129" id="tog-C129">
							<i class="left-60 tog far fa-chevron-down"></i>
							Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">B. Psomas, I. Kakogeorgiou, K. Karantzalos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/ICCV2023/html/Psomas_Keep_It_SimPool_Who_Said_Supervised_Transformers_Suffer_from_Attention_ICCV_2023_paper.html" title="Open access">ICCV&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C129.iccv23.simpool.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C129.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C129.iccv23.simpool-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C129.iccv23.simpool-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#simpool" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv51070.2023.00493" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/PsomasKKA23" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:7wO8s98CvbsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/efdef5b804e9038cee1f3ad1b719bec2ac488d5d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C129">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Paris, France  <span class="bull"></span> Oct 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C129.iccv23.simpool.svg"><img src="../data/pub/thumb/wide/conf/C129.iccv23.simpool.svg" alt="C129 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem? In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: <a href="https://github.com/billpsomas/simpool">https://github.com/billpsomas/simpool</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C129,
   title = {Keep It {SimPool}: Who Said Supervised Transformers Suffer from Attention Deficit?},
   author = {Psomas, Bill and Kakogeorgiou, Ioannis and Karantzalos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {10},
   address = {Paris, France},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C128"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C128" id="tog-C128">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive Anchors Label Propagation for Transductive Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, G. Ren, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip49359.2023.10223033" title="DOI">ICIP&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C128.icip23.few-a2lp.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C128.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C128.icip23.few-a2lp-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#a2lp" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-C128">
						<div class="pub-ref">
							In Proc. <em>International Conference on Image Processing</em><br>
							Kuala Lumpur, Malaysia  <span class="bull"></span> Oct 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C128.icip23.few-a2lp.png"><img src="../data/pub/thumb/wide/conf/C128.icip23.few-a2lp.png" alt="C128 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning addresses the issue of classifying images using limited labeled data. Exploiting unlabeled data through the use of transductive inference methods such as label propagation has been shown to improve the performance of few-shot learning significantly. Label propagation infers pseudo-labels for unlabeled data by utilizing a constructed graph that exploits the underlying manifold structure of the data. However, a limitation of the existing label propagation approaches is that the positions of all data points are fixed and might be sub-optimal so that the algorithm is not as effective as possible. In this work, we propose a novel algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function optimizing their positions in the manifold in the process. Our novel algorithm, Adaptive Anchor Label Propagation, outperforms the standard label propagation algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings respectively. We provide experimental results highlighting the merits of our algorithm on four widely used few-shot benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and WideResNet-28-10. The source code can be found at <a href="https://github.com/MichalisLazarou/A2LP">https://github.com/MichalisLazarou/A2LP</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C128,
   title = {Adaptive Anchors Label Propagation for Transductive Few-Shot Learning},
   author = {Lazarou, Michalis and Avrithis, Yannis and Ren, Guangyu and Stathaki, Tania},
   booktitle = {Proceedings of International Conference on Image Processing (ICIP)},
   month = {10},
   address = {Kuala Lumpur, Malaysia},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C127"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C127" id="tog-C127">
							<i class="left-60 tog far fa-chevron-down"></i>
							Generating Part-Aware Editable 3D Shapes without 3D Supervision
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Tertikas, D. Paschalidou, B. Pan, J.J. Park, M.A. Uy, I. Emiris, Y. Avrithis, L. Guibas</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/CVPR2023/html/Tertikas_Generating_Part-Aware_Editable_3D_Shapes_Without_3D_Supervision_CVPR_2023_paper.html" title="Open access">CVPR&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C127.cvpr23.part-nerf.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C127.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C127.cvpr23.part-nerf-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C127.cvpr23.part-nerf-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C127.cvpr23.part-nerf-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#partnerf" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://ktertikas.github.io/part_nerf" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.youtube.com/watch?v=H5jJryZzRs8" title="Video">
									<i class="fal fa-video"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr52729.2023.00434" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/TertikasPPPUEAG23" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:BzfGm06jWhQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=138001040224023621" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/55b7f46239ad707883c73f996b3ae15310d43b93" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/55b7f46239ad707883c73f996b3ae15310d43b93#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C127">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Vancouver, Canada  <span class="bull"></span> Jun 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C127.cvpr23.part-nerf.png"><img src="../data/pub/thumb/wide/conf/C127.cvpr23.part-nerf.png" alt="C127 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Impressive progress in generative models and implicit representations gave rise to methods that can generate 3D shapes of high quality. However, being able to locally control and edit shapes is another essential property that can unlock several content creation applications. Local control can be achieved with part-aware models, but existing methods require 3D supervision and cannot produce textures. In this work, we devise PartNeRF, a novel part-aware generative model for editable 3D shape synthesis that does not require any explicit 3D supervision. Our model generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables several editing operations such as applying transformations on parts, mixing parts from different objects etc. To ensure distinct, manipulable parts we enforce a hard assignment of rays to parts that makes sure that the color of each ray is only determined by a single NeRF. As a result, altering one part does not affect the appearance of the others. Evaluations on various ShapeNet categories demonstrate the ability of our model to generate editable 3D objects of improved fidelity, compared to previous part-based generative approaches that require 3D supervision or models relying on NeRFs.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C127,
   title = {Generating Part-Aware Editable 3D Shapes Without 3D Supervision},
   author = {Tertikas, Konstantinos and Paschalidou, Despoina and Pan, Boxiao and Park, Jeong Joon and Uy, Mikaela Angelina and Emiris, Ioannis and Avrithis, Yannis and Guibas, Leonidas},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Vancouver, Canada},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C126"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C126" id="tog-C126">
							<i class="left-60 tog far fa-chevron-down"></i>
							Boosting Vision Transformers for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, J. Yoon, S. Choi, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/WACV2023/html/Song_Boosting_Vision_Transformers_for_Image_Retrieval_WACV_2023_paper.html" title="Open access">WACV&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C126.wacv23.dtop.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C126.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C126.wacv23.dtop-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C126.wacv23.dtop-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/wacv56688.2023.00019" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wacv/SongYCA23" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ubry08Y2EpUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17332429352829652076" title="Citations @ Google Scholar">16</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0b97927c58bd410b26407a598712381fb137c5cf" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0b97927c58bd410b26407a598712381fb137c5cf#citing-papers" title="Citations @ Semantic Scholar">15</a>
						</div>

					</div>
					<div class="collapse" id="col-C126">
						<div class="pub-ref">
							In Proc. <em>IEEE Winter Conference on Applications of Computer Vision</em><br>
							Waikoloa, HI, US  <span class="bull"></span> Jan 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C126.wacv23.dtop.png"><img src="../data/pub/thumb/wide/conf/C126.wacv23.dtop.png" alt="C126 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation. Public code is available at <a href="https://github.com/dealicious-inc/DToP">https://github.com/dealicious-inc/DToP</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C126,
   title = {Boosting vision transformers for image retrieval},
   author = {Song, Chull Hwan and Yoon, Jooyoung and Choi, Shunghyun and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)},
   month = {1},
   address = {Waikoloa, HI, US},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2022"></a>
					2022
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C125"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C125" id="tog-C125">
							<i class="left-60 tog far fa-chevron-down"></i>
							What to Hide from Your Students: Attention-Guided Masked Image Modeling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">I. Kakogeorgiou, S. Gidaris, B. Psomas, Y. Avrithis, A. Bursuc, K. Karantzalos, N. Komodakis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1439_ECCV_2022_paper.php" title="Open access">ECCV&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C125.eccv22.attmask.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C125.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C125.eccv22.attmask-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C125.eccv22.attmask-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C125.eccv22.attmask-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#attmask" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-031-20056-4_18" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eccv/KakogeorgiouGPA22" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Aul-kAQHnToC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13621702207944750833" title="Citations @ Google Scholar">70</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/19b18f4efffe221f952054286669c7988a40c079" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/19b18f4efffe221f952054286669c7988a40c079#citing-papers" title="Citations @ Semantic Scholar">70</a>
						</div>

					</div>
					<div class="collapse" id="col-C125">
						<div class="pub-ref">
							In Proc. <em>European Conference on Computer Vision</em><br>
							Tel Aviv, Isreal  <span class="bull"></span> Oct 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg"><img src="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg" alt="C125 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Transformers and masked language modeling are quickly being adopted and explored in computer vision as vision transformers and masked image modeling (MIM). In this work, we argue that image token masking differs from token masking in text, due to the amount and correlation of tokens in an image. In particular, to generate a challenging pretext task for MIM, we advocate a shift from random masking to informed masking. We develop and exhibit this idea in the context of distillation-based MIM, where a teacher transformer encoder generates an attention map, which we use to guide masking for the student.
								</p>
								<p>
									We thus introduce a novel masking strategy, called attention-guided masking (AttMask), and we demonstrate its effectiveness over random masking for dense distillation-based MIM as well as plain distillation-based self-supervised learning on classification tokens. We confirm that AttMask accelerates the learning process and improves the performance on a variety of downstream tasks. We provide the implementation code at <a href="https://github.com/gkakogeorgiou/attmask">https://github.com/gkakogeorgiou/attmask</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C125,
   title = {What to Hide from Your Students: Attention-Guided Masked Image Modeling},
   author = {Kakogeorgiou, Ioannis and Gidaris, Spyros and Psomas, Bill and Avrithis, Yannis and Bursuc, Andrei and Karantzalos, Konstantinos and Komodakis, Nikos},
   booktitle = {Proceedings of European Conference on Computer Vision (ECCV)},
   month = {10},
   address = {Tel Aviv, Isreal},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C124"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C124" id="tog-C124">
							<i class="left-60 tog far fa-chevron-down"></i>
							AlignMixup: Improving Representations by Interpolating Aligned Features
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, E. Kijak, L. Amsaleg, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/CVPR2022/html/Venkataramanan_AlignMixup_Improving_Representations_by_Interpolating_Aligned_Features_CVPR_2022_paper.html" title="Open access">CVPR&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C124.cvpr22.alignmix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C124.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C124.cvpr22.alignmix-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C124.cvpr22.alignmix-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C124.cvpr22.alignmix-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#alignmix" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr52688.2022.01858" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/VenkataramananK22" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:3htObqc8RwsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15402867223438087088" title="Citations @ Google Scholar">62</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/78c51917366158513433f857cdaa5bce69798f61" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/78c51917366158513433f857cdaa5bce69798f61#citing-papers" title="Citations @ Semantic Scholar">40</a>
						</div>

					</div>
					<div class="collapse" id="col-C124">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							New Orleans, LA, US  <span class="bull"></span> Jun 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C124.cvpr22.alignmix.svg"><img src="../data/pub/thumb/wide/conf/C124.cvpr22.alignmix.svg" alt="C124 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Mixup is a powerful data augmentation method that interpolates between two or more examples in the input or feature space and between the corresponding target labels. Many recent mixup methods focus on cutting and pasting two or more objects into one image, which is more about efficient processing than interpolation. However, how to best interpolate images is not well defined. In this sense, mixup has been connected to autoencoders, because often autoencoders "interpolate well", for instance generating an image that continuously deforms into another.
								</p>
								<p>
									In this work, we revisit mixup from the interpolation perspective and introduce AlignMix, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. Interestingly, this gives rise to a situation where mixup retains mostly the geometry or pose of one image and the texture of the other, connecting it to style transfer. More than that, we show that an autoencoder can still improve representation learning under mixup, without the classifier ever seeing decoded images. AlignMix outperforms state-of-the-art mixup methods on five different benchmarks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C124,
   title = {{AlignMixup}: Improving representations by interpolating aligned features},
   author = {Venkataramanan, Shashanka and Kijak, Ewa and Amsaleg, Laurent and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {New Orleans, LA, US},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C123"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C123" id="tog-C123">
							<i class="left-60 tog far fa-chevron-down"></i>
							It Takes Two to Tango: Mixup for Deep Metric Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, B. Psomas, E. Kijak, L. Amsaleg, K. Karantzalos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openreview.net/forum?id=ZKy2X3dgPA" title="Open access">ICLR&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C123.iclr22.metrix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C123.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C123.iclr22.metrix-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C123.iclr22.metrix-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#metrix" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://iclr.cc/virtual/2022/poster/6337" title="Video">
									<i class="fal fa-video"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iclr/VenkataramananP22" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a7721bf626f4996117dbb88b385be2e12462e7e6" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a7721bf626f4996117dbb88b385be2e12462e7e6#citing-papers" title="Citations @ Semantic Scholar">18</a>
						</div>

					</div>
					<div class="collapse" id="col-C123">
						<div class="pub-ref">
							In Proc. <em>International Conference on Learning Representations</em><br>
							Virtual  <span class="bull"></span> Apr 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C123.iclr22.metrix.svg"><img src="../data/pub/thumb/wide/conf/C123.iclr22.metrix.svg" alt="C123 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On the other hand, modern data augmentation methods for classification consider two or more examples at a time. The combination of the two ideas is under-studied.
								</p>
								<p>
									In this work, we aim to bridge this gap and improve representations using mixup, which is a powerful data augmentation approach interpolating two or more examples and corresponding target labels at a time. This task is challenging because, unlike classification, the loss functions used in metric learning are not additive over examples, so the idea of interpolating target labels is not straightforward. To the best of our knowledge, we are the first to investigate mixing both examples and target labels for deep metric learning. We develop a generalized formulation that encompasses existing metric learning loss functions and modify it to accommodate for mixup, introducing Metric Mix, or Metrix. We also introduce a new metric---utilization---to demonstrate that by mixing examples during training, we are exploring areas of the embedding space beyond the training classes, thereby improving representations. To validate the effect of improved representations, we show that mixing inputs, intermediate representations or embeddings along with target labels significantly outperforms state-of-the-art metric learning methods on four benchmark deep metric learning datasets.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C123,
   title = {It Takes Two to Tango: Mixup for Deep Metric Learning},
   author = {Venkataramanan, Shashanka and Psomas, Bill and Kijak, Ewa and Amsaleg, Laurent and Karantzalos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Learning Representations (ICLR)},
   month = {4},
   address = {Virtual},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C122"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C122" id="tog-C122">
							<i class="left-60 tog far fa-chevron-down"></i>
							All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, H.J. Han, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/WACV2022/html/Song_All_the_Attention_You_Need_Global-Local_Spatial-Channel_Attention_for_Image_WACV_2022_paper.html" title="Open access">WACV&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C122.wacv22.glam.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C122.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C122.wacv22.glam-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C122.wacv22.glam-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/wacv51458.2022.00051" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wacv/SongHA22" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:OP4eGU-M3BUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14959374972088271626" title="Citations @ Google Scholar">36</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d564e0690d9eec8fcaf4f868b333d7ca4728db23" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d564e0690d9eec8fcaf4f868b333d7ca4728db23#citing-papers" title="Citations @ Semantic Scholar">21</a>
						</div>

					</div>
					<div class="collapse" id="col-C122">
						<div class="pub-ref">
							In Proc. <em>IEEE Winter Conference on Applications of Computer Vision</em><br>
							Waikoloa, HI, US  <span class="bull"></span> Jan 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C122.wacv22.glam.svg"><img src="../data/pub/thumb/wide/conf/C122.wacv22.glam.svg" alt="C122 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.
								</p>
								<p>
									We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C122,
   title = {All the attention you need: Global-local, spatial-channel attention for image retrieval},
   author = {Song, Chull Hwan and Han, Hye Joo and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)},
   month = {1},
   address = {Waikoloa, HI, US},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C121"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C121" id="tog-C121">
							<i class="left-60 tog far fa-chevron-down"></i>
							Tensor Feature Hallucination for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, T. Stathaki, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/WACV2022/html/Lazarou_Tensor_Feature_Hallucination_for_Few-Shot_Learning_WACV_2022_paper.html" title="Open access">WACV&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C121.wacv22.few-gen.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C121.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C121.wacv22.few-gen-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C121.wacv22.few-gen-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C121.wacv22.few-gen-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#tfh" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/wacv51458.2022.00211" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wacv/LazarouSA22" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:yMeIxYmEMEAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14985959446469864706" title="Citations @ Google Scholar">22</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/cb48a9854e7a398b589ba5b10dc7fa2095611ad5" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/cb48a9854e7a398b589ba5b10dc7fa2095611ad5#citing-papers" title="Citations @ Semantic Scholar">16</a>
						</div>

					</div>
					<div class="collapse" id="col-C121">
						<div class="pub-ref">
							In Proc. <em>IEEE Winter Conference on Applications of Computer Vision</em><br>
							Waikoloa, HI, US  <span class="bull"></span> Jan 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C121.wacv22.few-gen.png"><img src="../data/pub/thumb/wide/conf/C121.wacv22.few-gen.png" alt="C121 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning addresses the challenge of learning how to address novel tasks given not just limited supervision but limited data as well. An attractive solution is synthetic data generation. However, most such methods are overly sophisticated, focusing on high-quality, realistic data in the input space. It is unclear whether adapting them to the few-shot regime and using them for the downstream task of classification is the right approach. Previous works on synthetic data generation for few-shot classification focus on exploiting complex models, e.g. a Wasserstein GAN with multiple regularizers or a network that transfers latent diversities from known to novel classes.
								</p>
								<p>
									We follow a different approach and investigate how a simple and straightforward synthetic data generation method can be used effectively. We make two contributions, namely we show that: (1) using a simple loss function is more than enough for training a feature generator in the few-shot setting; and (2) learning to generate tensor features instead of vector features is superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show that our method sets a new state of the art, outperforming more sophisticated few-shot data augmentation methods. The source code can be found at https://github.com/MichalisLazarou/TFH_fewshot.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C121,
   title = {Tensor feature hallucination for few-shot learning},
   author = {Lazarou, Michalis and Stathaki, Tania and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)},
   month = {1},
   address = {Waikoloa, HI, US},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2021"></a>
					2021
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C120"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C120" id="tog-C120">
							<i class="left-60 tog far fa-chevron-down"></i>
							Patch Replacement: a Transformation-Based Method to Improve Robustness Against Adversarial Attacks
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, Y. Avrithis, T. Furon, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/3475731.3484955" title="Electronic edition">TAI/ACM-MM&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C120.acmw21.patch.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C120.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C120.acmw21.patch-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1145/3475731.3484955" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mm/ZhangAFA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Ehil0879vHcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=120332308439381614" title="Citations @ Google Scholar">1</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/54ff2b0652a4e5d77edfb9b925a4dc52b0794e82" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/54ff2b0652a4e5d77edfb9b925a4dc52b0794e82#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C120">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Trustworthy AI for Multimedia Computing</em><br>
							part of <em>ACM Multimedia Conference</em><br>
							Chengdu, China  <span class="bull"></span> Oct 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C120.acmw21.patch.svg"><img src="../data/pub/thumb/wide/conf/C120.acmw21.patch.svg" alt="C120 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Deep Neural Networks (DNNs) are robust against intra-class variability of images, pose variations and random noise, but vulnerable to imperceptible adversarial perturbations that are well-crafted precisely to mislead. While random noise even of relatively large magnitude can hardly affect predictions, adversarial perturbations of very small magnitude can make a classifier fail completely.
								</p>
								<p>
									To enhance robustness, we introduce a new adversarial defense called patch replacement, which transforms both the input images and their intermediate features at early layers to make adversarial perturbations behave similarly to random noise. We decompose images/features into small patches and quantize them according to a codebook learned from legitimate training images. This maintains the semantic information of legitimate images, while removing as much as possible the effect of adversarial perturbations.
								</p>
								<p>
									Experiments show that patch replacement improves robustness against both white-box and gray-box attacks, compared with other transformation-based defenses. It has a low computational cost since it does not need training or fine-tuning the network. Importantly, in the white-box scenario, it increases the robustness, while other transformation-based defenses do not.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C120,
   title = {Patch replacement: A transformation-based method to improve robustness against adversarial attacks},
   author = {Zhang, Hanwei and Avrithis, Yannis and Furon, Teddy and Amsaleg, Laurent},
   booktitle = {Proceedings of International Workshop on Trustworthy AI for Multimedia Computing (TAI), part of ACM Multimedia Conference (ACM-MM)},
   month = {10},
   address = {Chengdu, China},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C119"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C119" id="tog-C119">
							<i class="left-60 tog far fa-chevron-down"></i>
							On the Hidden Treasure of Dialog in Video Question Answering
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Engin, N.Q.K. Duong, F. Schnitzler, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/ICCV2021/html/Engin_On_the_Hidden_Treasure_of_Dialog_in_Video_Question_Answering_ICCV_2021_paper.html" title="Open access">ICCV&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C119.iccv21.vqa.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C119.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C119.iccv21.vqa-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C119.iccv21.vqa-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C119.iccv21.vqa-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#videoqa" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://engindeniz.github.io/dialogsummary-videoqa" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv48922.2021.00207" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/EnginSDA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:OR75R8vi5nAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3528884108230180565" title="Citations @ Google Scholar">10</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/263faba50f2b23f87bb6ff66f11e2ef497d72d49" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/263faba50f2b23f87bb6ff66f11e2ef497d72d49#citing-papers" title="Citations @ Semantic Scholar">8</a>
						</div>

					</div>
					<div class="collapse" id="col-C119">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Virtual  <span class="bull"></span> Oct 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C119.iccv21.vqa.svg"><img src="../data/pub/thumb/wide/conf/C119.iccv21.vqa.svg" alt="C119 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									High-level understanding of stories in video such as movies and TV shows from raw data is extremely challenging. Modern video question answering (VideoQA) systems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases. In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, without using question-specific human annotation or human-made plot summaries. It even outperforms human evaluators who have never watched any whole episode before.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C119,
   title = {On the hidden treasure of dialog in video question answering},
   author = {Engin, Deniz and Duong, Ngoc Q. K. and Schnitzler, Fran\c{c}ois and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {10},
   address = {Virtual},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C118"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C118" id="tog-C118">
							<i class="left-60 tog far fa-chevron-down"></i>
							Iterative Label Cleaning for Transductive and Semi-Supervised Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, T. Stathaki, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/ICCV2021/html/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.html" title="Open access">ICCV&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C118.iccv21.few-ss.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C118.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C118.iccv21.few-ss-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C118.iccv21.few-ss-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C118.iccv21.few-ss-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#ilpc" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv48922.2021.00863" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/LazarouSA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ODE9OILHJdcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16543608052080167907" title="Citations @ Google Scholar">53</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/b5bc34dbb82ec8abffb360583c5d9c4f3674ea2d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/b5bc34dbb82ec8abffb360583c5d9c4f3674ea2d#citing-papers" title="Citations @ Semantic Scholar">40</a>
						</div>

					</div>
					<div class="collapse" id="col-C118">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Virtual  <span class="bull"></span> Oct 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C118.iccv21.few-ss.svg"><img src="../data/pub/thumb/wide/conf/C118.iccv21.few-ss.svg" alt="C118 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. These problems are closely related because there is little or no adaptation of the representation in novel tasks.
								</p>
								<p>
									Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iterately improving the quality of pseudo-labels. Our solution sets new state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C118,
   title = {Iterative label cleaning for transductive and semi-supervised few-shot learning},
   author = {Lazarou, Michalis and Stathaki, Tania and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {10},
   address = {Virtual},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C117"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C117" id="tog-C117">
							<i class="left-60 tog far fa-chevron-down"></i>
							Asymmetric Metric Learning for Knowledge Transfer
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Budnik, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content/CVPR2021/html/Budnik_Asymmetric_Metric_Learning_for_Knowledge_Transfer_CVPR_2021_paper.html" title="Open access">CVPR&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C117.cvpr21.aml.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C117.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C117.cvpr21.aml-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C117.cvpr21.aml-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C117.cvpr21.aml-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#aml" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr46437.2021.00813" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/BudnikA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:mNrWkgRL2YcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9333606311348889691" title="Citations @ Google Scholar">35</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/77d2963441c35514fcc10465b46025cc94797a76" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/77d2963441c35514fcc10465b46025cc94797a76#citing-papers" title="Citations @ Semantic Scholar">30</a>
						</div>

					</div>
					<div class="collapse" id="col-C117">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Virtual  <span class="bull"></span> Jun 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C117.cvpr21.aml.svg"><img src="../data/pub/thumb/wide/conf/C117.cvpr21.aml.svg" alt="C117 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on fine-grained classification. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.
								</p>
								<p>
									We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss functions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are represented by the same model. We find that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. Interestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even outperform the teacher.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C117,
   title = {Asymmetric metric learning for knowledge transfer},
   author = {Budnik, Mateusz and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Virtual},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2020"></a>
					2020
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C116"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C116" id="tog-C116">
							<i class="left-60 tog far fa-chevron-down"></i>
							Local Propagation for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/9412178" title="Electronic edition">ICPR&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C116.icpr20.few-local.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C116.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C116.icpr20.few-local-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C116.icpr20.few-local-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/icpr48806.2021.9412178" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icpr/LifchitzAP20a" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:HtS1dXgVpQUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17166411077427694191" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d2c845a685b38bc42f40469e8ee257c47d4a184d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d2c845a685b38bc42f40469e8ee257c47d4a184d#citing-papers" title="Citations @ Semantic Scholar">7</a>
						</div>

					</div>
					<div class="collapse" id="col-C116">
						<div class="pub-ref">
							In Proc. <em>International Conference on Pattern Recognition</em><br>
							Virtual  <span class="bull"></span> Dec 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C116.icpr20.few-local.svg"><img src="../data/pub/thumb/wide/conf/C116.icpr20.few-local.svg" alt="C116 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The challenge in few-shot learning is that available data is not enough to capture the underlying distribution. To mitigate this, two emerging directions are (a) using local image representations, essentially multiplying the amount of data by a constant factor, and (b) using more unlabeled data, for instance by transductive inference, jointly on a number of queries. In this work, we bring these two ideas together, introducing local propagation. We treat local image features as independent examples, we build a graph on them and we use it to propagate both the features themselves and the labels, known and unknown. Interestingly, since there is a number of features per image, even a single query gives rise to transductive inference. As a result, we provide a universally safe choice for few-shot inference under both non-transductive and transductive settings, improving accuracy over corresponding methods. This is in contrast to existing solutions, where one needs to choose the method depending on the quantity of available data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C116,
   title = {Local Propagation for Few-Shot Learning},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine},
   booktitle = {Proceedings of International Conference on Pattern Recognition (ICPR)},
   month = {12},
   address = {Virtual},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C115"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C115" id="tog-C115">
							<i class="left-60 tog far fa-chevron-down"></i>
							Few-Shot Few-Shot Learning and the Role of Spatial Attention
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/9412902" title="Electronic edition">ICPR&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C115.icpr20.few-att.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C115.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C115.icpr20.few-att-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C115.icpr20.few-att-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/icpr48806.2021.9412902" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icpr/LifchitzAP20" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:HIFyuExEbWQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18107956312413459430" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7415be1325a8c0efd2d0660a876303f85bdd84c8" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7415be1325a8c0efd2d0660a876303f85bdd84c8#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C115">
						<div class="pub-ref">
							In Proc. <em>International Conference on Pattern Recognition</em><br>
							Virtual  <span class="bull"></span> Dec 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C115.icpr20.few-att.png"><img src="../data/pub/thumb/wide/conf/C115.icpr20.few-att.png" alt="C115 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning is often motivated by the ability of humans to learn new tasks from few examples. However, standard few-shot classification benchmarks assume that the representation is learned on a limited amount of base class data, ignoring the amount of prior knowledge that a human may have accumulated before learning new tasks. At the same time, even if a powerful representation is available, it may happen in some domain that base class data are limited or non-existent. This motivates us to study a problem where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. We adapt the representation in two stages, namely on the few base class data if available and on the even fewer data of new tasks. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class data are few, the network cannot learn where to focus implicitly. We also show that a pre-trained network may be easily adapted to novel classes, without meta-learning.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C115,
   title = {Few-Shot Few-Shot Learning and the role of Spatial Attention},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine},
   booktitle = {Proceedings of International Conference on Pattern Recognition (ICPR)},
   month = {12},
   address = {Virtual},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C114"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C114" id="tog-C114">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph Convolutional Networks for Learning with Few Clean and Many Noisy Labels
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum, C. Schmid</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1060_ECCV_2020_paper.php" title="Open access">ECCV&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C114.eccv20.few-noisy.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C114.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C114.eccv20.few-noisy-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C114.eccv20.few-noisy-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#gcc" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-030-58607-2_17" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eccv/IscenTACS20" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:WJVC3Jt7v1AC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3928299844756290038" title="Citations @ Google Scholar">22</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/df37c2ff506419748548210f9d776e1ec68b4e40" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/df37c2ff506419748548210f9d776e1ec68b4e40#citing-papers" title="Citations @ Semantic Scholar">15</a>
						</div>

					</div>
					<div class="collapse" id="col-C114">
						<div class="pub-ref">
							In Proc. <em>European Conference on Computer Vision</em><br>
							Virtual  <span class="bull"></span> Aug 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C114.eccv20.few-noisy.png"><img src="../data/pub/thumb/wide/conf/C114.eccv20.few-noisy.png" alt="C114 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier, which learns to discriminate clean from noisy examples using a weighted binary cross-entropy loss function. The GCN-inferred "clean" probability is then exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data, as well as standard few-shot classification where only few clean examples are used.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C114,
   title = {Graph convolutional networks for learning with few clean and many noisy labels},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej and Schmid, Cordelia},
   booktitle = {Proceedings of European Conference on Computer Vision (ECCV)},
   month = {8},
   address = {Virtual},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C113"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C113" id="tog-C113">
							<i class="left-60 tog far fa-chevron-down"></i>
							Rethinking Deep Active Learning: Using Unlabeled Data at Model Training
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, M. Budnik, Y. Avrithis, G. Gravier</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/9412716" title="Electronic edition">ICPR&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C113.icpr20.active.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C113.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C113.icpr20.active-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C113.icpr20.active-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C113.icpr20.active-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#ssal" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/icpr48806.2021.9412716" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icpr/SimeoniBAG20" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:mlAyqtXpCwEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18087806471833715062" title="Citations @ Google Scholar">80</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/f1828f512c65086be066db49dba5c1688703c2a6" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/f1828f512c65086be066db49dba5c1688703c2a6#citing-papers" title="Citations @ Semantic Scholar">67</a>
						</div>

					</div>
					<div class="collapse" id="col-C113">
						<div class="pub-ref">
							In Proc. <em>International Conference on Pattern Recognition</em><br>
							Virtual  <span class="bull"></span> Dec 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C113.icpr20.active.svg"><img src="../data/pub/thumb/wide/conf/C113.icpr20.active.svg" alt="C113 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C113,
   title = {Rethinking deep active learning: Using unlabeled data at model training},
   author = {Sim\'eoni, Oriane and Budnik, Mateusz and Avrithis, Yannis and Gravier, Guillaume},
   booktitle = {Proceedings of International Conference on Pattern Recognition (ICPR)},
   month = {12},
   address = {Virtual},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2019"></a>
					2019
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C112"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C112" id="tog-C112">
							<i class="left-60 tog far fa-chevron-down"></i>
							Label Propagation for Deep Semi-Supervised Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Iscen_Label_Propagation_for_Deep_Semi-Supervised_Learning_CVPR_2019_paper.html" title="Open access">CVPR&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C112.cvpr19.semi.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C112.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C112.cvpr19.semi-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#dlp" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2019.00521" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/IscenTAC19" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:6ZxmRoH8BuwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9368555603387579120" title="Citations @ Google Scholar">696</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/cafcdab811c7834c9c09960e09f9feb045efc945" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/cafcdab811c7834c9c09960e09f9feb045efc945#citing-papers" title="Citations @ Semantic Scholar">516</a>
						</div>

					</div>
					<div class="collapse" id="col-C112">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Long Beach, CA, US  <span class="bull"></span> Jun 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C112.cvpr19.semi.png"><img src="../data/pub/thumb/wide/conf/C112.cvpr19.semi.png" alt="C112 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Semi-supervised learning is becoming increasingly important because it can combine data carefully labeled by humans with abundant unlabeled data to train deep neural networks. Classic works on semi-supervised learning that have focused on transductive learning have not been fully exploited in the inductive framework followed by modern deep learning. The same holds for the manifold assumption--that similar examples should get the same prediction. In this work, we employ a transductive method that is based on the manifold assumption to make predictions on the entire dataset and use these predictions to generate pseudo-labels for the unlabeled data and train a deep neural network. In doing so, a nearest neighbor graph of the dataset is created based on the embeddings of the same network. Therefore our learning process iterates between these two steps. We improve performance on several datasets especially in the few labels regime and show that our work is complementary to current state of the art.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C112,
   title = {Label propagation for Deep Semi-supervised Learning},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Long Beach, CA, US},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C111"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C111" id="tog-C111">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dense Classification and Implanting for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard, A. Bursuc</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Lifchitz_Dense_Classification_and_Implanting_for_Few-Shot_Learning_CVPR_2019_paper.html" title="Open access">CVPR&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C111.cvpr19.few.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C111.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C111.cvpr19.few-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2019.00948" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/LifchitzAPB19" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:sNmaIFBj_lkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2424058237104279547" title="Citations @ Google Scholar">226</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6a5f98c6d96a9f8cc308dae202b62e2230716d34" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6a5f98c6d96a9f8cc308dae202b62e2230716d34#citing-papers" title="Citations @ Semantic Scholar">180</a>
						</div>

					</div>
					<div class="collapse" id="col-C111">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Long Beach, CA, US  <span class="bull"></span> Jun 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C111.cvpr19.few.svg"><img src="../data/pub/thumb/wide/conf/C111.cvpr19.few.svg" alt="C111 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Training deep neural networks from few examples is a highly challenging and key problem for many computer vision tasks. In this context, we are targeting knowledge transfer from a set with abundant data to other sets with few available examples. We propose two simple and effective solutions: (i) dense classification over feature maps, which for the first time studies local activations in the domain of few-shot learning, and (ii) implanting, that is, attaching new neurons to a previously trained network to learn new, task-specific features. On miniImageNet, we improve the prior state-of-the-art on few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way 1-shot, 5-shot and 10-shot settings respectively.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C111,
   title = {Dense Classification and Implanting for Few-shot Learning},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine and Bursuc, Andrei},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Long Beach, CA, US},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C110"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C110" id="tog-C110">
							<i class="left-60 tog far fa-chevron-down"></i>
							Local Features and Visual Words Emerge in Activations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Simeoni_Local_Features_and_Visual_Words_Emerge_in_Activations_CVPR_2019_paper.html" title="Open access">CVPR&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C110.cvpr19.spatial.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C110.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C110.cvpr19.spatial-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#dsm" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2019.01192" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/SimeoniAC19" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_5tno0g5mFcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=8007077270735204964" title="Citations @ Google Scholar">75</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/2188bdd251a557952c254a7653f9893a0a1201e2" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/2188bdd251a557952c254a7653f9893a0a1201e2#citing-papers" title="Citations @ Semantic Scholar">64</a>
						</div>

					</div>
					<div class="collapse" id="col-C110">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Long Beach, CA, US  <span class="bull"></span> Jun 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C110.cvpr19.spatial.svg"><img src="../data/pub/thumb/wide/conf/C110.cvpr19.spatial.svg" alt="C110 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We propose a novel method of spatial verification for image retrieval. Initial ranking is based on image descriptors extracted from convolutional neural network activations by global pooling, as in recent state-of-the-art work. However, the same sparse 3D activation tensor is also approximated by a collection of local features. These local features are then robustly matched to approximate the optimal alignment of the tensors. This happens without any network modification, additional layers or training. No local feature detection happens on the original image; no local feature descriptors and no visual vocabulary are needed throughout the whole process.
								</p>
								<p>
									We experimentally show that the proposed method achieves the state-of-the-art performance on standard benchmarks across different network architectures and different global pooling methods. Advantages of combining efficient nearest neighbor retrieval with global descriptors and spatial verification is even more pronounced by spatially verified diffusion.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C110,
   title = {Local Features and Visual Words Emerge in Activations},
   author = {Sim\'eoni, Oriane and Avrithis, Yannis and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Long Beach, CA, US},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2018"></a>
					2018
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C109"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C109" id="tog-C109">
							<i class="left-60 tog far fa-chevron-down"></i>
							Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, Y. Avrithis, G. Tolias, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-030-20890-5_20" title="Electronic edition">ACCV&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C109.accv18.hybrid.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C109.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-030-20890-5_20" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/accv/IscenATFC18" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:GtLg2Ama23sC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14247171994069345455" title="Citations @ Google Scholar">8</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/747e0b08626554018cbab6b1ad3b4049b361efb5" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/747e0b08626554018cbab6b1ad3b4049b361efb5#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C109">
						<div class="pub-ref">
							In Proc. <em>Asian Conference on Computer Vision</em><br>
							Perth, Western Australia  <span class="bull"></span> Dec 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C109.accv18.hybrid.svg"><img src="../data/pub/thumb/wide/conf/C109.accv18.hybrid.svg" alt="C109 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									State of the art image retrieval performance is achieved with CNN features and manifold ranking using a k-NN similarity graph that is pre-computed off-line. The two most successful existing approaches are temporal filtering, where manifold ranking amounts to solving a sparse linear system online, and spectral filtering, where eigen-decomposition of the adjacency matrix is performed off-line and then manifold ranking amounts to dot-product search online. The former suffers from expensive queries and the latter from significant space overhead. Here we introduce a novel, theoretically well-founded hybrid filtering approach allowing full control of the space-time trade-off between these two extremes. Experimentally, we verify that our hybrid method delivers results on par with the state of the art, with lower memory demands compared to spectral filtering approaches and faster compared to temporal filtering.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C109,
   title = {Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking},
   author = {Iscen, Ahmet and Avrithis, Yannis and Tolias, Giorgos and Furon, Teddy and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of Asian Conference on Computer Vision (ACCV)},
   month = {12},
   address = {Perth, Western Australia},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C108"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C108" id="tog-C108">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mining on Manifolds: Metric Learning without Labels
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Mining_on_Manifolds_CVPR_2018_paper.html" title="Open access">CVPR&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C108.cvpr18.mom.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C108.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C108.cvpr18.mom-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#mom" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2018.00797" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/IscenTAC18" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_axFR9aDTf0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13423863468293341196" title="Citations @ Google Scholar">129</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/8bf6a40787c76527351ac50d92ce9d0b98b8a542" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/8bf6a40787c76527351ac50d92ce9d0b98b8a542#citing-papers" title="Citations @ Semantic Scholar">107</a>
						</div>

					</div>
					<div class="collapse" id="col-C108">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Salt Lake City, UT, US  <span class="bull"></span> Jun 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png"><img src="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png" alt="C108 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss.
								</p>
								<p>
									The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C108,
   title = {Mining on Manifolds: Metric Learning without Labels},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Salt Lake City, UT, US},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C107"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C107" id="tog-C107">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting Oxford And Paris: Large-Scale Image Retrieval Benchmarking
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Radenovi, A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Radenovic_Revisiting_Oxford_and_CVPR_2018_paper.html" title="Open access">CVPR&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C107.cvpr18.oxford.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C107.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C107.cvpr18.oxford-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#revop" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://cmp.felk.cvut.cz/revisitop/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2018.00598" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/RadenovicITAC18" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:rmuvC79q63oC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=8587995924195333428" title="Citations @ Google Scholar">418</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/73a3576e54e4ad0a00280e8c2daab9ba119352b1" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/73a3576e54e4ad0a00280e8c2daab9ba119352b1#citing-papers" title="Citations @ Semantic Scholar">311</a>
						</div>

					</div>
					<div class="collapse" id="col-C107">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Salt Lake City, UT, US  <span class="bull"></span> Jun 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C107.cvpr18.oxford.svg"><img src="../data/pub/thumb/wide/conf/C107.cvpr18.oxford.svg" alt="C107 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.
								</p>
								<p>
									An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C107,
   title = {Revisiting {Oxford} and {Paris}: Large-Scale Image Retrieval Benchmarking},
   author = {Radenovi\'c, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Salt Lake City, UT, US},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C106"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C106" id="tog-C106">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fast Spectral Ranking for Similarity Search
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, Y. Avrithis, G. Tolias, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Iscen_Fast_Spectral_Ranking_CVPR_2018_paper.html" title="Open access">CVPR&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C106.cvpr18.fsr.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C106.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C106.cvpr18.fsr-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2018.00796" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/IscenATFC18" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/dde940da5d588f16e232e3a39cf8e92fdebf87fb" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/dde940da5d588f16e232e3a39cf8e92fdebf87fb#citing-papers" title="Citations @ Semantic Scholar">43</a>
						</div>

					</div>
					<div class="collapse" id="col-C106">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Salt Lake City, UT, US  <span class="bull"></span> Jun 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C106.cvpr18.fsr.svg"><img src="../data/pub/thumb/wide/conf/C106.cvpr18.fsr.svg" alt="C106 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. This makes the Euclidean nearest neighbor search biased for this task. Exploring the manifolds online remains expensive even if a nearest neighbor graph has been computed offline.
								</p>
								<p>
									This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. This is equivalent to linear graph filtering of a sparse signal in the frequency domain. To speed up online search, we compute an approximate Fourier basis of the graph offline. We improve the state of art on particular object retrieval datasets including the challenging Instre dataset containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C106,
   title = {Fast Spectral Ranking for Similarity Search},
   author = {Iscen, Ahmet and Avrithis, Yannis and Tolias, Giorgos and Furon, Teddy and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Salt Lake City, UT, US},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C105"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C105" id="tog-C105">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unsupervised Object Discovery for Instance Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/8354298" title="Electronic edition">WACV&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C105.wacv18.disco.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C105.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C105.wacv18.disco-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C105.wacv18.disco-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/wacv.2018.00194" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wacv/SimeoniITAC18" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ClCfbGk0d_YC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7269623352862388884" title="Citations @ Google Scholar">14</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0c446cbf7be8027409c42f26ceea9b2eaa1f2798" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0c446cbf7be8027409c42f26ceea9b2eaa1f2798#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C105">
						<div class="pub-ref">
							In Proc. <em>IEEE Winter Conference on Applications of Computer Vision</em><br>
							Lake Tahoe, NV/CA, US  <span class="bull"></span> Mar 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C105.wacv18.disco.svg"><img src="../data/pub/thumb/wide/conf/C105.wacv18.disco.svg" alt="C105 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Severe background clutter is challenging in many computer vision tasks, including large-scale image retrieval. Global descriptors, that are popular due to their memory and search efficiency, are especially prone to corruption by such a clutter. Eliminating the impact of the clutter on the image descriptor increases the chance of retrieving relevant images and prevents topic drift due to actually retrieving the clutter in the case of query expansion. In this work, we propose a novel salient region detection method. It captures, in an unsupervised manner, patterns that are both discriminative and common in the dataset. Saliency is based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. The descriptors derived from the salient regions improve particular object retrieval, most noticeably in a large collections containing small objects.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C105,
   title = {Unsupervised object discovery for instance recognition},
   author = {Sim\'eoni, Oriane and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)},
   month = {3},
   address = {Lake Tahoe, NV/CA, US},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2017"></a>
					2017
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C104"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C104" id="tog-C104">
							<i class="left-60 tog far fa-chevron-down"></i>
							Automatic Discovery of Discriminative Parts as a Quadratic Assignment Problem
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">R. Sicre, J. Rabin, Y. Avrithis, T. Furon, F. Jurie, E. Kijak</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/w18/html/Sicre_Automatic_Discovery_of_ICCV_2017_paper.html" title="Open access">CEFRL/ICCV&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C104.iccv-cefrl17.parts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C104.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccvw.2017.129" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccvw/SicreRAFJK17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:PoWvk5oyLR8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1221196602676618692" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/98bd843bd90c577f99f761d10ba05abb46ffa4ac" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/98bd843bd90c577f99f761d10ba05abb46ffa4ac#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C104">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Compact and Efficient Feature Representation and Learning</em><br>
							part of <em>International Conference on Computer Vision</em><br>
							Venice, Italy  <span class="bull"></span> Oct 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C104.iccv-cefrl17.parts.png"><img src="../data/pub/thumb/wide/conf/C104.iccv-cefrl17.parts.png" alt="C104 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Part-based image classification consists in representing categories by small sets of discriminative parts upon which a representation of the images is built. This paper addresses the question of how to automatically learn such parts from a set of labeled training images. We propose to cast the training of parts as a quadratic assignment problem in which optimal correspondences between image regions and parts are automatically learned. The paper analyses different assignment strategies and thoroughly evaluates them on two public datasets: Willow actions and MIT 67 scenes.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C104,
   title = {Automatic discovery of discriminative parts as a quadratic assignment problem},
   author = {Sicre, Ronan and Rabin, Julien and Avrithis, Yannis and Furon, Teddy and Jurie, Fr\'ed\'eric and Kijak, Ewa},
   booktitle = {Proceedings of International Workshop on Compact and Efficient Feature Representation and Learning (CEFRL), part of International Conference on Computer Vision (ICCV)},
   month = {10},
   address = {Venice, Italy},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C103"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C103" id="tog-C103">
							<i class="left-60 tog far fa-chevron-down"></i>
							Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Iscen_Efficient_Diffusion_on_CVPR_2017_paper.html" title="Open access">CVPR&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C103.cvpr17.diffuse.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C103.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C103.cvpr17.diffuse-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#diffusion" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
								<a class="lnk mr2" href="../code/#instre2" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://cmp.felk.cvut.cz/~iscenahm/_pages/diffusion.html" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2017.105" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/IscenTAFC17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Dip1O2bNi0gC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11128317942489453247" title="Citations @ Google Scholar">204</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/49a2327f7d86b7a079f4ecbd76f10e501d055c4d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/49a2327f7d86b7a079f4ecbd76f10e501d055c4d#citing-papers" title="Citations @ Semantic Scholar">159</a>
						</div>

					</div>
					<div class="collapse" id="col-C103">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Honolulu, Hawaii, US  <span class="bull"></span> Jul 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png"><img src="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png" alt="C103 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. A novel way to perform diffusion through a sparse linear system solver yields practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C103,
   title = {Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact {CNN} Representations},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Furon, Teddy and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {7},
   address = {Honolulu, Hawaii, US},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C102"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C102" id="tog-C102">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unsupervised Part Learning for Visual Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">R. Sicre, Y. Avrithis, E. Kijak, F. Jurie</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Sicre_Unsupervised_Part_Learning_CVPR_2017_paper.html" title="Open access">CVPR&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C102.cvpr17.uparts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C102.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C102.cvpr17.uparts-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2017.332" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/SicreAKJ17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Ug5p-4gJ2f0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14404666341134082725" title="Citations @ Google Scholar">24</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/73f082e241e86bbe365e4c6dfc6435a798752ec7" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/73f082e241e86bbe365e4c6dfc6435a798752ec7#citing-papers" title="Citations @ Semantic Scholar">20</a>
						</div>

					</div>
					<div class="collapse" id="col-C102">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Honolulu, Hawaii, US  <span class="bull"></span> Jul 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C102.cvpr17.uparts.svg"><img src="../data/pub/thumb/wide/conf/C102.cvpr17.uparts.svg" alt="C102 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Part-based image classification aims at representing categories by small sets of learned discriminative parts, upon which an image representation is built. Considered as a promising avenue a decade ago, this direction has been neglected since the advent of deep neural networks. In this context, this paper brings two contributions: first, this work proceeds one step further compared to recent part-based models (PBM), focusing on how to learn parts without using any labeled data. Instead of learning a set of parts per class, as generally performed in the PBM literature, the proposed approach both constructs a partition of a given set of images into visually similar groups, and subsequently learns a set of discriminative parts per group in a fully unsupervised fashion. This strategy opens the door to the use of PBM in new applications where labeled data are typically not available, such as instance-based image retrieval. Second, this paper shows that despite the recent success of end-to-end models, explicit part learning can still boost classification performance. We experimentally show that our learned parts can help building efficient image representations, which outperform state-of-the art Deep Convolutional Neural Networks (DCNN) on both classification and retrieval tasks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C102,
   title = {Unsupervised part learning for visual recognition},
   author = {Sicre, Ronan and Avrithis, Yannis and Kijak, Ewa and Jurie, Fr\'ed\'eric},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {7},
   address = {Honolulu, Hawaii, US},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C101"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C101" id="tog-C101">
							<i class="left-60 tog far fa-chevron-down"></i>
							Panorama to Panorama Matching for Location Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/3078971.3079033" title="Electronic edition">ICMR&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C101.icmr17.pano.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C101.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C101.icmr17.pano-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1145/3078971.3079033" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mir/IscenTAFC17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:edDO8Oi4QzsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1411166879151879416" title="Citations @ Google Scholar">31</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0c112c4c6ba55cea3a5131efa343a75d4c97b853" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0c112c4c6ba55cea3a5131efa343a75d4c97b853#citing-papers" title="Citations @ Semantic Scholar">25</a>
						</div>

					</div>
					<div class="collapse" id="col-C101">
						<div class="pub-ref">
							In Proc. <em>ACM International Conference on Multimedia Retrieval</em><br>
							Bucharest, Romania  <span class="bull"></span> Jun 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C101.icmr17.pano.svg"><img src="../data/pub/thumb/wide/conf/C101.icmr17.pano.svg" alt="C101 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Location recognition is commonly treated as visual instance retrieval on "street view" imagery. The dataset items and queries are panoramic views, i.e. groups of images taken at a single location. This work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C101,
   title = {Panorama to Panorama Matching for Location Recognition},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Furon, Teddy and Chum, Ond\v{r}ej},
   booktitle = {Proceedings of ACM International Conference on Multimedia Retrieval (ICMR)},
   month = {6},
   address = {Bucharest, Romania},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2016"></a>
					2016
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C100"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C100" id="tog-C100">
							<i class="left-60 tog far fa-chevron-down"></i>
							High-Dimensional Visual Similarity Search: k-d Generalized Randomized Forests
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, I. Emiris, G. Samaras</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/2949035.2949042" title="Electronic edition">CGI&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C100.cgi16.geraf.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C100.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C100.cgi16.geraf-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#geraf" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1145/2949035.2949042" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cgi/AvrithisES16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:i2xiXl-TujoC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13885473137850630231" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/92c903ca7ab92f344880f90d2c2420778913d8e7" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/92c903ca7ab92f344880f90d2c2420778913d8e7#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C100">
						<div class="pub-ref">
							In Proc. <em>the 33rd Computer Graphics International</em><br>
							Heraklion, Greece  <span class="bull"></span> Jun 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C100.cgi16.geraf.svg"><img src="../data/pub/thumb/wide/conf/C100.cgi16.geraf.svg" alt="C100 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We propose a new data-structure, the generalized randomized k-d forest, or k-d GeRaF, for approximate nearest neighbor searching in high dimensions. In particular, we introduce new randomization techniques to specify a set of independently constructed trees where search is performed simultaneously, hence increasing accuracy. We omit backtracking, and we optimize distance computations, thus accelerating queries. We release public domain software GeRaF and we compare it to existing implementations of state-of-the-art methods including BBD-trees, Locality Sensitive Hashing, randomized k-d forests, and product quantization. Experimental results indicate that our method would be the method of choice in dimensions around 1,000, and probably up to 10,000, and pointsets of cardinality up to a few hundred thousands or even one million; this range of inputs is encountered in many critical applications today. For instance, we handle a real dataset of 10^6 images represented in 960 dimensions with a query time of less than 1sec on average and 90% responses being true nearest neighbors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C100,
   title = {High-Dimensional Visual Similarity Search: {k-d} Generalized Randomized Forests},
   author = {Avrithis, Yannis and Emiris, Ioannis and Samaras, Georgios},
   booktitle = {Proceedings of the 33rd Computer Graphics International (CGI)},
   month = {6},
   address = {Heraklion, Greece},
   year = {2016},
   organization = {ACM},
   pages = {25--28}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2015"></a>
					2015
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C99"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C99" id="tog-C99">
							<i class="left-60 tog far fa-chevron-down"></i>
							Web-Scale Image Clustering Revisited
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Kalantidis, E. Anagnostopoulos, I. Emiris</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_iccv_2015/html/Avrithis_Web-Scale_Image_Clustering_ICCV_2015_paper.html" title="Open access">ICCV&nbsp;2015</a>
							<a class="xtra but mr" href="http://videolectures.net/iccv2015_kalantidis_image_clustering/">Oral</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C099.iccv15.iqm.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C99.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C099.iccv15.iqm-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C099.iccv15.iqm-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C099.iccv15.iqm-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#iqm" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv.2015.176" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/AvrithisKAE15" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:lmc2jWPfTJgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15368968510247268806,17259411402402411288" title="Citations @ Google Scholar">44</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/9e0652f647e52393399782c0cf49095d7167f571" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/9e0652f647e52393399782c0cf49095d7167f571#citing-papers" title="Citations @ Semantic Scholar">32</a>
						</div>

					</div>
					<div class="collapse" id="col-C99">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Santiago, Chile  <span class="bull"></span> Dec 2015
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C099.iccv15.iqm.svg"><img src="../data/pub/thumb/wide/conf/C099.iccv15.iqm.svg" alt="C99 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Large scale duplicate detection, clustering and mining of documents or images has been conventionally treated with seed detection via hashing, followed by seed growing heuristics using fast search. Principled clustering methods, especially kernelized and spectral ones, have higher complexity and are difficult to scale above millions. Under the assumption of documents or images embedded in Euclidean space, we revisit recent advances in approximate k-means variants, and borrow their best ingredients to introduce a new one, inverted-quantized k-means (IQ-means). Key underlying concepts are quantization of data points and multi-index based inverted search from centroids to cells. Its quantization is a form of hashing and analogous to seed detection, while its updates are analogous to seed growing, yet principled in the sense of distortion minimization. We further design a dynamic variant that is able to determine the number of clusters k in a single run at nearly zero additional cost. Combined with powerful deep learned representations, we achieve clustering of a 100 million image collection on a single machine in less than one hour.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C99,
   title = {Web-scale image clustering revisited},
   author = {Avrithis, Yannis and Kalantidis, Yannis and Anagnostopoulos, Evangelos and Emiris, Ioannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV) (Oral)},
   month = {12},
   address = {Santiago, Chile},
   year = {2015}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C98"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C98" id="tog-C98">
							<i class="left-60 tog far fa-chevron-down"></i>
							Planar Shape Decomposition Made Simple
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Papanelopoulos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://www.bmva.org/bmvc/2015/papers/paper013/" title="Open access">BMVC&nbsp;2015</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C098.bmvc15.cuts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C98.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C098.bmvc15.cuts-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.5244/c.29.13" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/bmvc/PapanelopoulosA15" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:MLfJN-KU85MC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5206456425917967003" title="Citations @ Google Scholar">5</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/4dfd6969cc8c8aeeb02114d5e13745744295b6ba" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/4dfd6969cc8c8aeeb02114d5e13745744295b6ba#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C98">
						<div class="pub-ref">
							In Proc. <em>British Machine Vision Conference</em><br>
							Swansea, UK  <span class="bull"></span> Sep 2015
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C098.bmvc15.cuts.svg"><img src="../data/pub/thumb/wide/conf/C098.bmvc15.cuts.svg" alt="C98 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a very simple computational model for planar shape decomposition that naturally captures most of the rules and salience measures suggested by psychophysical studies, including the minima and short-cut rules, convexity, and symmetry. It is based on a medial axis representation in ways that have not been explored before and sheds more light into the connection between existing rules like minima and convexity. In particular, vertices of the exterior medial axis directly provide the position and extent of negative minima of curvature, while a traversal of the interior medial axis directly provides a small set of candidate endpoints for part-cuts. The final selection follows a simple local convexity rule that can incorporate arbitrary salience measures. Neither global optimization nor differentiation is involved. We provide qualitative and quantitative evaluation and comparisons on ground-truth data from psychophysical experiments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C98,
   title = {Planar shape decomposition made simple},
   author = {Papanelopoulos, Nikos and Avrithis, Yannis},
   booktitle = {Proceedings of British Machine Vision Conference (BMVC)},
   month = {9},
   address = {Swansea, UK},
   year = {2015}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C97"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C97" id="tog-C97">
							<i class="left-60 tog far fa-chevron-down"></i>
							Early Burst Detection for Memory-Efficient Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Shi, Y. Avrithis, H. Jgou</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_cvpr_2015/html/Shi_Early_Burst_Detection_2015_CVPR_paper.html" title="Open access">CVPR&nbsp;2015</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C097.cvpr15.burst.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C97.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C097.cvpr15.burst-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C097.cvpr15.burst-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#ebd" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2015.7298659" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/ShiAJ15" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:gsN89kCJA0AC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13966113278534417702" title="Citations @ Google Scholar">53</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/152baff4f39aa6e080bc4106bfb1aa15d5fa7a5d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/152baff4f39aa6e080bc4106bfb1aa15d5fa7a5d#citing-papers" title="Citations @ Semantic Scholar">43</a>
						</div>

					</div>
					<div class="collapse" id="col-C97">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Boston, MA, US  <span class="bull"></span> Jun 2015
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C097.cvpr15.burst.svg"><img src="../data/pub/thumb/wide/conf/C097.cvpr15.burst.svg" alt="C97 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Recent works show that image comparison based on local descriptors is corrupted by visual bursts, which tend to dominate the image similarity. The existing strategies, like power-law normalization, improve the results by discounting the contribution of visual bursts to the image similarity.
								</p>
								<p>
									In this paper, we propose to explicitly detect the visual bursts in an image at an early stage. We compare several detection strategies jointly taking into account feature similarity and geometrical quantities. The bursty groups are merged into meta-features, which are used as input to state-of-the-art image search systems such as VLAD or the selective match kernel. Then, we show the interest of using this strategy in an asymmetrical manner, with only the database features being aggregated but not those of the query.
								</p>
								<p>
									Extensive experiments performed on public benchmarks for visual retrieval show the benefits of our method, which achieves performance on par with the state of the art but with a significantly reduced complexity, thanks to the lower number of features fed to the indexing system.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C97,
   title = {Early burst detection for memory-efficient image retrieval},
   author = {Shi, Miaojing and Avrithis, Yannis and J\'egou, Herv\'e},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Boston, MA, US},
   year = {2015}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2014"></a>
					2014
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C96"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C96" id="tog-C96">
							<i class="left-60 tog far fa-chevron-down"></i>
							Improving Local Features by Dithering-Based Image Sampling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C. Varytimidis, K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-319-16808-1_40" title="Electronic edition">ACCV&nbsp;2014</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C096.accv14.dither.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C96.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C096.accv14.dither-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/wash/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-319-16808-1_40" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/accv/VarytimidisRAK14" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:g3aElNc5_aQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10619633488311084813" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/1f21821589734f0bd30ac5cec221100941f49d13" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/1f21821589734f0bd30ac5cec221100941f49d13#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C96">
						<div class="pub-ref">
							In Proc. <em>Asian Conference on Computer Vision</em><br>
							Singapore  <span class="bull"></span> Nov 2014
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C096.accv14.dither.svg"><img src="../data/pub/thumb/wide/conf/C096.accv14.dither.svg" alt="C96 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The recent trend of structure-guided feature detectors, as opposed to blob and corner detectors, has led to a family of methods that exploit image edges to accurately capture local shape. Among them, the WaSH detector combines binary edge sampling with gradient strength and computational geometry representations towards distinctive and repeatable local features. In this work, we provide alternative, variable-density sampling schemes on smooth functions of image intensity based on dithering. These methods are parameter-free and more invariant to geometric transformations than uniform sampling. The resulting detectors compare well to the state-of-the-art, while achieving higher performance in a series of matching and retrieval experiments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C96,
   title = {Improving local features by dithering-based image sampling},
   author = {Varytimidis, Christos and Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of Asian Conference on Computer Vision (ACCV)},
   month = {11},
   address = {Singapore},
   year = {2014}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C95"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C95" id="tog-C95">
							<i class="left-60 tog far fa-chevron-down"></i>
							Locally Optimized Product Quantization for Approximate Nearest Neighbor Search
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Kalantidis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_cvpr_2014/html/Kalantidis_Locally_Optimized_Product_2014_CVPR_paper.html" title="Open access">CVPR&nbsp;2014</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C095.cvpr14.lopq.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C95.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C095.cvpr14.lopq-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#lopq" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="image.ntua.gr/iva/research/lopq/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2014.298" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/KalantidisA14" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:EYYDruWGBe4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2227083521271721233" title="Citations @ Google Scholar">318</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/05634cabe22b43fe1f0da1a0b0b2f116bffa3324" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/05634cabe22b43fe1f0da1a0b0b2f116bffa3324#citing-papers" title="Citations @ Semantic Scholar">249</a>
						</div>

					</div>
					<div class="collapse" id="col-C95">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Columbus, OH, US  <span class="bull"></span> Jun 2014
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C095.cvpr14.lopq.png"><img src="../data/pub/thumb/wide/conf/C095.cvpr14.lopq.png" alt="C95 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate nearest neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to provide non-exhaustive search, i.e. inverted lists or a multi-index, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Local optimization is over rotation and space decomposition; interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C95,
   title = {Locally Optimized Product Quantization for Approximate Nearest Neighbor Search},
   author = {Kalantidis, Yannis and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Columbus, OH, US},
   year = {2014}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2013"></a>
					2013
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C94"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C94" id="tog-C94">
							<i class="left-60 tog far fa-chevron-down"></i>
							Quantize and Conquer: a Dimensionality-Recursive Solution to Clustering, Vector Quantization, and Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_iccv_2013/html/Avrithis_Quantize_and_Conquer_2013_ICCV_paper.html" title="Open access">ICCV&nbsp;2013</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C094.iccv13b.qc.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C94.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C094.iccv13b.qc-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C094.iccv13b.qc-slide.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#drvq" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/drvq/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv.2013.376" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/Avrithis13" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:zLWjf1WUPmwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2298703892829669835" title="Citations @ Google Scholar">18</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/01966307000907864dffc25cf0f56f86d198768f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/01966307000907864dffc25cf0f56f86d198768f#citing-papers" title="Citations @ Semantic Scholar">14</a>
						</div>

					</div>
					<div class="collapse" id="col-C94">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Sydney, Australia  <span class="bull"></span> Dec 2013
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C094.iccv13b.qc.svg"><img src="../data/pub/thumb/wide/conf/C094.iccv13b.qc.svg" alt="C94 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Inspired by the close relation between nearest neighbor search and clustering in high-dimensional spaces as well as the success of one helping to solve the other, we introduce a new paradigm where both problems are solved simultaneously. Our solution is recursive, not in the size of input data but in the number of dimensions. One result is a clustering algorithm that is tuned to small codebooks but does not need all data in memory at the same time and is practically constant in the data size. As a by-product, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. A lesser contribution is a new indexing scheme for image retrieval that exploits multiple small codebooks to provide an arbitrarily fine partition of the descriptor space. Large scale experiments on public datasets exhibit state of the art performance and remarkable generalization.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C94,
   title = {Quantize and Conquer: A dimensionality-recursive solution to clustering, vector quantization, and image retrieval},
   author = {Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {12},
   address = {Sydney, Australia},
   year = {2013}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C93"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C93" id="tog-C93">
							<i class="left-60 tog far fa-chevron-down"></i>
							To Aggregate or Not to Aggregate: Selective Match Kernels for Image Search
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, Y. Avrithis, H. Jgou</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://openaccess.thecvf.com/content_iccv_2013/html/Tolias_To_Aggregate_or_2013_ICCV_paper.html" title="Open access">ICCV&nbsp;2013</a>
							<a class="xtra but mr" href="http://techtalks.tv/beta/talks/to-aggregate-or-not-to-aggregate-selective-match-kernels-for-image-search/59407/">Oral</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C093.iccv13a.asmk.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C93.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C093.iccv13a.asmk-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#asmk" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/asmk/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv.2013.177" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/ToliasAJ13" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:EkHepimYqZsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17392159945632611553" title="Citations @ Google Scholar">253</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/85ede368a84521576aa54d479a4f6e556b6a07ce" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/85ede368a84521576aa54d479a4f6e556b6a07ce#citing-papers" title="Citations @ Semantic Scholar">209</a>
						</div>

					</div>
					<div class="collapse" id="col-C93">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Sydney, Australia  <span class="bull"></span> Dec 2013
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C093.iccv13a.asmk.svg"><img src="../data/pub/thumb/wide/conf/C093.iccv13a.asmk.svg" alt="C93 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper considers a family of metrics to compare images based on their local descriptors. It encompasses the VLAD descriptor and matching techniques such as Hamming Embedding. Making the bridge between these approaches leads us to propose a match kernel that takes the best of existing techniques by combining an aggregation procedure with a selective match kernel. Finally, the representation underpinning this kernel is approximated, providing a large scale image search both precise and scalable, as shown by our experiments on several benchmarks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C93,
   title = {To aggregate or not to aggregate: selective match kernels for image search},
   author = {Tolias, Giorgos and Avrithis, Yannis and J\'egou, Herv\'e},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV) (Oral)},
   month = {12},
   address = {Sydney, Australia},
   year = {2013}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2012"></a>
					2012
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C92"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C92" id="tog-C92">
							<i class="left-60 tog far fa-chevron-down"></i>
							W$\alpha$Sh: Weighted $\alpha$-Shapes for Local Feature Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C. Varytimidis, K. Rapantzikos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-642-33709-3_56" title="Electronic edition">ECCV&nbsp;2012</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C092.eccv12b.wash.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C92.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C092.eccv12b.wash-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#wash" title="Binary">
									<i class="fal fa-binary"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/wash/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-642-33709-3_56" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eccv/VarytimidisRA12" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_Re3VWB3Y0AC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2969393318309946425" title="Citations @ Google Scholar">23</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/244f559fa5b0b01025d039e329993875000a03a3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/244f559fa5b0b01025d039e329993875000a03a3#citing-papers" title="Citations @ Semantic Scholar">16</a>
						</div>

					</div>
					<div class="collapse" id="col-C92">
						<div class="pub-ref">
							In Proc. <em>European Conference on Computer Vision</em><br>
							Florence, Italy  <span class="bull"></span> Oct 2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C092.eccv12b.wash.png"><img src="../data/pub/thumb/wide/conf/C092.eccv12b.wash.png" alt="C92 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Depending on the application, local feature detectors should comply to properties that are often contradictory, e.g. distinctiveness vs robustness. Providing a good balance is a standing problem in the field. In this direction, we propose a novel approach for local feature detection starting from sampled edges and based on shape stability measures across the weighted alpha-filtration, a computational geometry construction that captures the shape of a non-uniform set of points. Detected features are blob-like and include non-extremal regions as well as regions determined by cavities of boundary shape. The detector provides distinctive regions, while achieving high robustness in terms of repeatability and matching score, as well as competitive performance in a large scale image retrieval application.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C92,
   title = {W$\alpha$SH: Weighted $\alpha$-Shapes for Local Feature Detection},
   author = {Varytimidis, Christos and Rapantzikos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of European Conference on Computer Vision (ECCV)},
   month = {10},
   address = {Florence, Italy},
   year = {2012}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C91"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C91" id="tog-C91">
							<i class="left-60 tog far fa-chevron-down"></i>
							Approximate Gaussian Mixtures for Large Scale Vocabularies
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Kalantidis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-642-33712-3_2" title="Electronic edition">ECCV&nbsp;2012</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C091.eccv12a.agm.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C91.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C091.eccv12a.agm-supp.tar.gz" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C091.eccv12a.agm-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#agm" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/agm/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-642-33712-3_2" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eccv/AvrithisK12" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Fu2w8maKXqMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11755450633084886749" title="Citations @ Google Scholar">59</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7b2bb59725349a85f3335f3c8f3c7e64325b7a89" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7b2bb59725349a85f3335f3c8f3c7e64325b7a89#citing-papers" title="Citations @ Semantic Scholar">46</a>
						</div>

					</div>
					<div class="collapse" id="col-C91">
						<div class="pub-ref">
							In Proc. <em>European Conference on Computer Vision</em><br>
							Florence, Italy  <span class="bull"></span> Oct 2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C091.eccv12a.agm.png"><img src="../data/pub/thumb/wide/conf/C091.eccv12a.agm.png" alt="C91 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We introduce a clustering method that combines the flexibility of Gaussian mixtures with the scaling properties needed to construct visual vocabularies for image retrieval. It is a variant of expectation-maximization that can converge rapidly while dynamically estimating the number of components. We employ approximate nearest neighbor search to speed-up the E-step and exploit its iterative nature to make search incremental, boosting both speed and precision. We achieve superior performance in large scale retrieval, being as fast as the best known approximate k-means.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C91,
   title = {Approximate {Gaussian} Mixtures for Large Scale Vocabularies},
   author = {Avrithis, Yannis and Kalantidis, Yannis},
   booktitle = {Proceedings of European Conference on Computer Vision (ECCV)},
   month = {10},
   address = {Florence, Italy},
   year = {2012}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C90"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C90" id="tog-C90">
							<i class="left-60 tog far fa-chevron-down"></i>
							SymCity: Feature Selection by Symmetry for Large Scale Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, Y. Kalantidis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/2393347.2393379" title="Electronic edition">ACM-MM&nbsp;2012</a>
							<span class="xtra but mr">Full paper</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C090.acm12a.symcity.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C90.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C090.acm12a.symcity-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C090.acm12a.symcity-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/symcity/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1145/2393347.2393379" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mm/ToliasKA12" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:2KloaMYe4IUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5594098755293418824" title="Citations @ Google Scholar">24</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/952bdc0120fcda3a5bac808177abe2559c0ec9b0" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/952bdc0120fcda3a5bac808177abe2559c0ec9b0#citing-papers" title="Citations @ Semantic Scholar">14</a>
						</div>

					</div>
					<div class="collapse" id="col-C90">
						<div class="pub-ref">
							In Proc. <em>ACM Multimedia Conference</em><br>
							Nara, Japan  <span class="bull"></span> Oct 2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png"><img src="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png" alt="C90 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Many problems, including feature selection, vocabulary learning, location and landmark recognition, structure from motion and 3d reconstruction, rely on a learning process that involves wide-baseline matching on multiple views of the same object or scene. In practical large scale image retrieval applications however, most images depict unique views where this idea does not apply. We exploit self-similarities, symmetries and repeating patterns to select features within a single image. We achieve the same performance compared to the full feature set with only a small fraction of its index size on a dataset of unique views of buildings or urban scenes, in the presence of one million distractors of similar nature. Our best solution is linear in the number of correspondences, with practical running times of just a few milliseconds.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C90,
   title = {{SymCity}: Feature Selection by Symmetry for Large Scale Image Retrieval},
   author = {Tolias, Giorgos and Kalantidis, Yannis and Avrithis, Yannis},
   publisher = {ACM},
   booktitle = {Proceedings of ACM Multimedia Conference (ACM-MM) (Full paper)},
   month = {10},
   address = {Nara, Japan},
   year = {2012}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2011"></a>
					2011
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C89"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C89" id="tog-C89">
							<i class="left-60 tog far fa-chevron-down"></i>
							Speeded-Up, Relaxed Spatial Matching
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/6126427" title="Electronic edition">ICCV&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C089.iccv11a.hpm.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C89.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C089.iccv11a.hpm-supp.pdf" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C089.iccv11a.hpm-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#hpm-int" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
								<a class="lnk mr2" href="../code/#hpm" title="Binary">
									<i class="fal fa-binary"></i>
								</a>
								<a class="lnk mr2" href="../code/#wc2m" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/relaxed_spatial_matching/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv.2011.6126427" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/ToliasA11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:eJXPG6dFmWUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16352077225044433418" title="Citations @ Google Scholar">94</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a4feed266d56bebcb496a17041ee3cf8e2d4866f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a4feed266d56bebcb496a17041ee3cf8e2d4866f#citing-papers" title="Citations @ Semantic Scholar">71</a>
						</div>

					</div>
					<div class="collapse" id="col-C89">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Barcelona, Spain  <span class="bull"></span> Nov 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C089.iccv11a.hpm.png"><img src="../data/pub/thumb/wide/conf/C089.iccv11a.hpm.png" alt="C89 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									A wide range of properties and assumptions determine the most appropriate spatial matching model for an application, e.g. recognition, detection, registration, or large scale visual search. Most notably, these include discriminative power, geometric invariance, rigidity constraints, mapping constraints, assumptions made on the underlying features or descriptors and, of course, computational complexity. Having image retrieval in mind, we present a very simple model inspired by Hough voting in the transformation space, where votes arise from single feature correspondences. A relaxed matching process allows for multiple matching surfaces or non-rigid objects under one-to-one mapping, yet is linear in the number of correspondences. We apply it to geometry re-ranking in a search engine, yielding superior performance with the same space requirements but a dramatic speed-up compared to the state of the art.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C89,
   title = {Speeded-up, Relaxed Spatial Matching},
   author = {Tolias, Giorgos and Avrithis, Yannis},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {11},
   address = {Barcelona, Spain},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C88"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C88" id="tog-C88">
							<i class="left-60 tog far fa-chevron-down"></i>
							The Medial Feature Detector: Stable Regions from Image Boundaries
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, K. Rapantzikos</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/6126436" title="Electronic edition">ICCV&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C088.iccv11b.mfd.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C88.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/supp/conf/C088.iccv11b.mfd-supp.tar.gz" title="Supplementary material">
								<i class="fal fa-folder-plus"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C088.iccv11b.mfd-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#mfd" title="Binary">
									<i class="fal fa-binary"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/medial_features/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/iccv.2011.6126436" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/iccv/AvrithisR11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:eflP2zaiRacC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3928835547646161859" title="Citations @ Google Scholar">27</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a769118110cc96eb2aa2cd7f1390a7022f66a902" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a769118110cc96eb2aa2cd7f1390a7022f66a902#citing-papers" title="Citations @ Semantic Scholar">24</a>
						</div>

					</div>
					<div class="collapse" id="col-C88">
						<div class="pub-ref">
							In Proc. <em>International Conference on Computer Vision</em><br>
							Barcelona, Spain  <span class="bull"></span> Nov 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C088.iccv11b.mfd.svg"><img src="../data/pub/thumb/wide/conf/C088.iccv11b.mfd.svg" alt="C88 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a local feature detector that is able to detect regions of arbitrary scale and shape, without scale space construction. We compute a weighted distance map on image gradient, using our exact linear-time algorithm, a variant of group marching for Euclidean space. We find the weighted medial axis by extending residues, typically used in Voronoi skeletons. We decompose the medial axis into a graph representing image structure in terms of peaks and saddle points. A duality property enables reconstruction of regions using the same marching method. We greedily group regions taking both contrast and shape into account. On the way, we select regions according to our shape fragmentation factor, favoring those well enclosed by boundaries--even incomplete.  We achieve state of the art performance in matching and retrieval experiments with reduced memory and computational requirements.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C88,
   title = {The Medial Feature Detector: Stable Regions from Image Boundaries},
   author = {Avrithis, Yannis and Rapantzikos, Konstantinos},
   booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
   month = {11},
   address = {Barcelona, Spain},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C87"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C87" id="tog-C87">
							<i class="left-60 tog far fa-chevron-down"></i>
							Scalable Triangulation-Based Logo Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Kalantidis, L.G. Pueyo, M. Trevisiol, R. van Zwol, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/1991996.1992016" title="Electronic edition">ICMR&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C087.icmr11.logo.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C87.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C087.icmr11.logo-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
								<a class="lnk mr2" href="../code/#logos27" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1145/1991996.1992016" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mir/KalantidisPTZA11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:sSrBHYA8nusC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11974239448063128777" title="Citations @ Google Scholar">155</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/4d89e4aa54cd116f7393671a9af15a9869ccec5b" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/4d89e4aa54cd116f7393671a9af15a9869ccec5b#citing-papers" title="Citations @ Semantic Scholar">129</a>
						</div>

					</div>
					<div class="collapse" id="col-C87">
						<div class="pub-ref">
							In Proc. <em>ACM International Conference on Multimedia Retrieval</em><br>
							Trento, Italy  <span class="bull"></span> Apr 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C087.icmr11.logo.png"><img src="../data/pub/thumb/wide/conf/C087.icmr11.logo.png" alt="C87 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We propose a scalable logo recognition approach that extends the common bag-of-words model and incorporates local geometry in the indexing process. Given a query image and a large logo database, the goal is to recognize the logo contained in the query, if any. We locally group features in triples using multi-scale Delaunay triangulation and represent triangles by signatures capturing both visual appearance and local geometry. Each class is represented by the union of such signatures over all instances in the class. We see large scale recognition as a sub-linear search problem where signatures of the query image are looked up in an inverted index structure of the class models. We evaluate our approach on a large-scale logo recognition dataset with more than four thousand classes.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C87,
   title = {Scalable Triangulation-based Logo Recognition},
   author = {Kalantidis, Yannis and Pueyo, Lluis Garcia and Trevisiol, Michele and van Zwol, Roelof and Avrithis, Yannis},
   booktitle = {Proceedings of ACM International Conference on Multimedia Retrieval (ICMR)},
   month = {4},
   address = {Trento, Italy},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2010"></a>
					2010
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C86"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C86" id="tog-C86">
							<i class="left-60 tog far fa-chevron-down"></i>
							Retrieving Landmark and Non-Landmark Images from Community Photo Collections
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Kalantidis, G. Tolias, E. Spyrou</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/1873951.1873973" title="Electronic edition">ACM-MM&nbsp;2010</a>
							<span class="xtra but mr">Full paper</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C086.acm-mm10a.scene.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C86.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C086.acm-mm10a.scene-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#viral" title="Application">
									<i class="fal fa-browser"></i>
								</a>
								<a class="lnk mr2" href="../code/#ec1m" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/scene_maps/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1145/1873951.1873973" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mm/AvrithisKTS10" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:mVmsd5A6BfQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18244330247050681542" title="Citations @ Google Scholar">158</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/75f26918aa1de322e2804f9f2492dbe8c3284d77" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/75f26918aa1de322e2804f9f2492dbe8c3284d77#citing-papers" title="Citations @ Semantic Scholar">118</a>
						</div>

					</div>
					<div class="collapse" id="col-C86">
						<div class="pub-ref">
							In Proc. <em>ACM Multimedia Conference</em><br>
							Firenze, Italy  <span class="bull"></span> Oct 2010
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C086.acm-mm10a.scene.svg"><img src="../data/pub/thumb/wide/conf/C086.acm-mm10a.scene.svg" alt="C86 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									State of the art data mining and image retrieval in community photo collections typically focus on popular subsets, e.g. images containing landmarks or associated to Wikipedia articles. We propose an image clustering scheme that, seen as vector quantization, compresses a large corpus of images by grouping visually consistent ones while providing a guaranteed distortion bound. This allows us, for instance, to represent the visual content of all thousands of images depicting the Parthenon in just a few dozens of scene maps and still be able to retrieve any single, isolated, non-landmark image like a house or a graffiti on a wall. Starting from a geo-tagged dataset, we first group images geographically and then visually, where each visual cluster is assumed to depict different views of the the same scene. We align all views to one reference image and construct a 2D scene map by preserving details from all images while discarding repeating visual features. Our indexing, retrieval and spatial matching scheme then operates directly on scene maps. We evaluate the precision of the proposed method on a challenging one-million urban image dataset.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C86,
   title = {Retrieving Landmark and Non-Landmark Images from Community Photo Collections},
   author = {Avrithis, Yannis and Kalantidis, Yannis and Tolias, Giorgos and Spyrou, Evaggelos},
   booktitle = {Proceedings of ACM Multimedia Conference (ACM-MM) (Full paper)},
   month = {10},
   address = {Firenze, Italy},
   year = {2010}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C85"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C85" id="tog-C85">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature Map Hashing: Sub-Linear Indexing of Appearance and Global Geometry
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Tolias, Y. Kalantidis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://dl.acm.org/doi/10.1145/1873951.1873985" title="Electronic edition">ACM-MM&nbsp;2010</a>
							<span class="xtra but mr">Full paper</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C085.acm-mm10b.fmh.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C85.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C085.acm-mm10b.fmh-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
								<a class="lnk mr2" href="../code/#fmh" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
								<a class="lnk mr2" href="../code/#ec50k" title="Data">
									<i class="fal fa-database"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/feature_map_hashing" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1145/1873951.1873985" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mm/AvrithisTK10" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:RHpTSmoSYBkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18340408019659022322" title="Citations @ Google Scholar">37</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/f03548dbaee0e2d484c14be811698e695fcdcb9c" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/f03548dbaee0e2d484c14be811698e695fcdcb9c#citing-papers" title="Citations @ Semantic Scholar">28</a>
						</div>

					</div>
					<div class="collapse" id="col-C85">
						<div class="pub-ref">
							In Proc. <em>ACM Multimedia Conference</em><br>
							Firenze, Italy  <span class="bull"></span> Oct 2010
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg"><img src="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg" alt="C85 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a new approach to image indexing and retrieval, which integrates appearance with global image geometry in the indexing process, while enjoying robustness against viewpoint change, photometric variations, occlusion, and background clutter. We exploit shape parameters of local features to estimate image alignment via a single correspondence. Then, for each feature, we construct a sparse spatial map of all remaining features, encoding their normalized position and appearance, typically vector quantized to visual word. An image is represented by a collection of such feature maps and RANSAC-like matching is reduced to a number of set intersections. Because the induced dissimilarity is still not a metric, we extend min-wise independent permutations to collections of sets and derive a similarity measure for feature map collections. We then exploit sparseness to build an inverted file whereby the retrieval process is sub-linear in the total number of images, ideally linear in the number of relevant ones. We achieve excellent performance on 10^4 images, with a query time in the order of milliseconds.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C85,
   title = {Feature Map Hashing: Sub-linear Indexing of Appearance and Global Geometry},
   author = {Avrithis, Yannis and Tolias, Giorgos and Kalantidis, Yannis},
   booktitle = {Proceedings of ACM Multimedia Conference (ACM-MM) (Full paper)},
   month = {10},
   address = {Firenze, Italy},
   year = {2010}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C84"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C84" id="tog-C84">
							<i class="left-60 tog far fa-chevron-down"></i>
							Detecting Regions from Single Scale Edges
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-642-35749-7_23" title="Electronic edition">SGA/ECCV&nbsp;2010</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C084.eccv-sga10.edge.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C84.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C084.eccv-sga10.edge-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/edge_based_feature_detection" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-642-35749-7_23" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eccv/RapantzikosAK10" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:B3FOqHPlNUQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14861089716667085579" title="Citations @ Google Scholar">15</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5bb2fcfa52a6f1f036507ccc03abf36a3256389c" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/5bb2fcfa52a6f1f036507ccc03abf36a3256389c#citing-papers" title="Citations @ Semantic Scholar">11</a>
						</div>

					</div>
					<div class="collapse" id="col-C84">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Sign, Gesture and Activity</em><br>
							part of <em>European Conference on Computer Vision</em><br>
							Hersonissos, Crete, Greece  <span class="bull"></span> Sep 2010
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C084.eccv-sga10.edge.png"><img src="../data/pub/thumb/wide/conf/C084.eccv-sga10.edge.png" alt="C84 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We believe that the potential of edges in local feature detection has not been fully exploited and therefore propose a detector that starts from single scale edges and produces reliable and interpretable blob-like regions and groups of regions of arbitrary shape. The detector is based on merging local maxima of the distance transform guided by the gradient strength of the surrounding edges. Repeatability and matching score are evaluated and compared to state-of-the-art detectors on standard benchmarks. Furthermore, we demonstrate the potential application of our method to wide-baseline matching and feature detection in sequences involving human activity.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C84,
   title = {Detecting Regions from Single Scale Edges},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of International Workshop on Sign, Gesture and Activity (SGA), part of European Conference on Computer Vision (ECCV)},
   month = {9},
   address = {Hersonissos, Crete, Greece},
   year = {2010}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2009"></a>
					2009
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C83"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C83" id="tog-C83">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dense Saliency-Based Spatiotemporal Feature Points for Action Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/5206525" title="Electronic edition">CVPR&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C083.cvpr09.dense.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C83.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/poster/conf/C083.cvpr09.dense-poster.pdf" title="Poster">
								<i class="fal fa-person-chalkboard"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/spatiotemporal_feature_detection" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cvpr.2009.5206525" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cvpr/RapantzikosAK09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Y0pCki6q_DkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2142399280353189537" title="Citations @ Google Scholar">214</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/78b59620e591872487610fb73b2555acd8f47b83" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/78b59620e591872487610fb73b2555acd8f47b83#citing-papers" title="Citations @ Semantic Scholar">166</a>
						</div>

					</div>
					<div class="collapse" id="col-C83">
						<div class="pub-ref">
							In Proc. <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Miami, FL, US  <span class="bull"></span> Jun 2009
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C083.cvpr09.dense.svg"><img src="../data/pub/thumb/wide/conf/C083.cvpr09.dense.svg" alt="C83 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Several spatiotemporal feature point detectors have been recently used in video analysis for action recognition. Feature points are detected using a number of measures, namely saliency, cornerness, periodicity, motion activity etc. Each of these measures is usually intensity-based and provides a different trade-off between density and informativeness. In this paper, we use saliency for feature point detection in videos and incorporate color and motion apart from intensity. Our method uses a multi-scale volumetric representation of the video and involves spatiotemporal operations at the voxel level. Saliency is computed by a global minimization process constrained by pure volumetric constraints, each of them being related to an informative visual aspect, namely spatial proximity, scale and feature similarity (intensity, color, motion). Points are selected as the extrema of the saliency response and prove to balance well between density and informativeness. We provide an intuitive view of the detected points and visual comparisons against state-of-the-art space-time detectors. Our detector outperforms them on the KTH dataset using Nearest-Neighbor classifiers and ranks among the top using different classification frameworks. Statistics and comparisons are also performed on the more difficult Hollywood Human Actions (HOHA) dataset increasing the performance compared to current published results.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C83,
   title = {Dense saliency-based spatiotemporal feature points for action recognition},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   address = {Miami, FL, US},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C82"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C82" id="tog-C82">
							<i class="left-60 tog far fa-chevron-down"></i>
							Video Event Detection and Summarization Using Audio, Visual and Text Saliency
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Evangelopoulos, A. Zlatintsi, G. Skoumas, K. Rapantzikos, A. Potamianos, P. Maragos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/4960393" title="Electronic edition">ICASSP&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C082.icassp09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C82.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/visual_saliency" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/icassp.2009.4960393" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icassp/EvangelopoulosZSRPMA09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:7PzlFSSx8tAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12733105102188997742" title="Citations @ Google Scholar">95</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/ec0ad3190dc94ca216b7e76b199d421d66305a06" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/ec0ad3190dc94ca216b7e76b199d421d66305a06#citing-papers" title="Citations @ Semantic Scholar">68</a>
						</div>

					</div>
					<div class="collapse" id="col-C82">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em><br>
							Taipei, Taiwan  <span class="bull"></span> Apr 2009
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C082.icassp09.svg"><img src="../data/pub/thumb/wide/conf/C082.icassp09.svg" alt="C82 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Detection of perceptually important video events is formulated here on the basis of saliency models for the audio, visual and textual information conveyed in a video stream. Audio saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color and motion. Text saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The various modality curves are integrated in a single attention curve, where the presence of an event may be signified in one or multiple domains. This multimodal saliency curve is the basis of a bottom-up video summarization algorithm, that refines results from unimodal or audiovisual-based skimming. The algorithm performs favorably for video summarization in terms of informativeness and enjoyability.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C82,
   title = {Video event detection and summarization using audio, visual and text saliency},
   author = {Evangelopoulos, Georgios and Zlatintsi, Athanasia and Skoumas, Georgios and Rapantzikos, Konstantinos and Potamianos, Alexandros and Maragos, Petros and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   month = {4},
   address = {Taipei, Taiwan},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C81"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C81" id="tog-C81">
							<i class="left-60 tog far fa-chevron-down"></i>
							Integrating Image Segmentation and Classification for Fuzzy Knowledge-Based Multimedia Indexing
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, N. Simou, G. Papadopoulos, R. Benmokhtar, K. Chandramouli, V. Tzouvaras, V. Mezaris, M. Phinikettos, Y. Avrithis, Y. Kompatsiaris, B. Huet, E. Izquierdo</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-540-92892-8_29" title="Electronic edition">MMM&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C081.mmm09a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C81.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-540-92892-8_29" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmm/AthanasiadisSPBCTMPAKHI09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ldfaerwXgEUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16630232959151055221" title="Citations @ Google Scholar">19</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a56237deecd0b5af74f3d4aae3b43c6d6afc6050" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a56237deecd0b5af74f3d4aae3b43c6d6afc6050#citing-papers" title="Citations @ Semantic Scholar">16</a>
						</div>

					</div>
					<div class="collapse" id="col-C81">
						<div class="pub-ref">
							In Proc. <em>15th International Multimedia Modeling Conference</em><br>
							Sophia Antipolis, France  <span class="bull"></span> Jan 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we propose a methodology for semantic indexing of images, based on techniques of image segmentation, classification and fuzzy reasoning. The proposed knowledge-assisted analysis architecture integrates algorithms applied on three overlapping levels of semantic information: i) no semantics, i.e. segmentation based on low-level features such as color and shape, ii) mid-level semantics, such as concurrent image segmentation and object detection, region-based classification and, iii) rich semantics, i.e. fuzzy reasoning for extraction of implicit knowledge. In that way, we extract semantic description of raw multimedia content and use it for indexing and retrieval purposes, backed up by a fuzzy knowledge repository. We conducted several experiments to evaluate each technique, as well as the whole methodology in overall and, results show the potential of our approach.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C81,
   title = {Integrating Image Segmentation and Classification for Fuzzy Knowledge-based Multimedia Indexing},
   author = {Athanasiadis, Thanos and Simou, Nikolaos and Papadopoulos, Georgios and Benmokhtar, Rachid and Chandramouli, Krishna and Tzouvaras, Vassilis and Mezaris, Vasileios and Phinikettos, Marios and Avrithis, Yannis and Kompatsiaris, Yiannis and Huet, Benoit and Izquierdo, Ebroul},
   booktitle = {Proceedings of 15th International Multimedia Modeling Conference (MMM)},
   month = {1},
   pages = {263--274},
   address = {Sophia Antipolis, France},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C80"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C80" id="tog-C80">
							<i class="left-60 tog far fa-chevron-down"></i>
							Large Scale Concept Detection in Video Using a Region Thesaurus
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Tolias, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-3-540-92892-8_20" title="Electronic edition">MMM&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C080.mmm09b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C80.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-540-92892-8_20" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmm/SpyrouTA09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:WbkHhVStYXYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7865643915725172231" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/960690098e1fb85f5f926cfe59146adbd712c11a" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/960690098e1fb85f5f926cfe59146adbd712c11a#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C80">
						<div class="pub-ref">
							In Proc. <em>15th International Multimedia Modeling Conference</em><br>
							Sophia Antipolis, France  <span class="bull"></span> Jan 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents an approach on high-level feature detection within video documents, using a Region Thesaurus. A video shot is represented by a single keyframe and MPEG-7 features are extracted locally, from coarse segmented regions. Then a clustering algorithm is applied on those extracted regions and a region thesaurus is constructed to facilitate the description of each keyframe at a higher level than the low-level descriptors but at a lower than the high-level concepts. A model vector representation is formed and several high-level concept detectors are appropriately trained using a global keyframe annotation. The proposed approach is thoroughly evaluated on the TRECVID 2007 development data for the detection of nine high level concepts, demonstrating sufficient performance on large data sets.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C80,
   title = {Large Scale Concept Detection in Video Using a Region Thesaurus},
   author = {Spyrou, Evaggelos and Tolias, Giorgos and Avrithis, Yannis},
   booktitle = {Proceedings of 15th International Multimedia Modeling Conference (MMM)},
   month = {1},
   address = {Sophia Antipolis, France},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C79"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C79" id="tog-C79">
							<i class="left-60 tog far fa-chevron-down"></i>
							Compound Document Analysis by Fusing Evidence Across Media
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Nikolopoulos, C. Lakka, I. Kompatsiaris, C. Varytimidis, K. Rapantzikos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/5137837" title="Electronic edition">CBMI&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C079.cbmi09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C79.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/cbmi.2009.35" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cbmi/NikolopoulosLKVRA09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:a0OBvERweLwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11219911720916021290" title="Citations @ Google Scholar">4</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6c5787ec5c0f72288f772e03e9cc20e5faeeaac8" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6c5787ec5c0f72288f772e03e9cc20e5faeeaac8#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-C79">
						<div class="pub-ref">
							In Proc. <em>7th International Workshop on Content-Based Multimedia Indexing</em><br>
							Chania, Greece  <span class="bull"></span> Jun 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A cross media analysis scheme for the semantic interpretation of compound documents is presented. The proposed scheme is essentially a late-fusion mechanism that operates on top of single-media extractors output. Evidence extracted from heterogeneous sources are used to trigger probabilistic inference on a Bayesian network that encodes domain knowledge and quantifies causality. Experiments performed on a set of 54 compound documents showed that the proposed scheme is able to exploit the existing cross media relations and achieve performance improvements.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C79,
   title = {Compound document analysis by fusing evidence across media},
   author = {Nikolopoulos, Spiros and Lakka, Christina and Kompatsiaris, Ioannis and Varytimidis, Christos and Rapantzikos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of 7th International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {6},
   address = {Chania, Greece},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C78"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C78" id="tog-C78">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Kalantidis, G. Tolias, E. Spyrou, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CBMI&nbsp;2009</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C078.cbmi09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C78.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d9a8079b3b349532b3d94d35cc1f97ac3df4ba9f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d9a8079b3b349532b3d94d35cc1f97ac3df4ba9f#citing-papers" title="Citations @ Semantic Scholar">10</a>
						</div>

					</div>
					<div class="collapse" id="col-C78">
						<div class="pub-ref">
							In Proc. <em>7th International Workshop on Content-Based Multimedia Indexing</em><br>
							Chania, Greece  <span class="bull"></span> Jun 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The popularity of social networks and web-based personal image collections has resulted to a continuously growing volume of publicly available photos and videos. Users are uploading, describing, tagging and annotating their personal photos. Moreover, a recent trend is to also "geotag" them, that is to mark the location they were taken onto a web-based map. Consequently, this growth of image collections has created the need for fast, robust and efficient systems, able to analyze large-scale diverse and heterogeneous visual content. This growing need for automatic metadata generation, concept detection, search and retrieval has boosted research efforts towards these directions. The work presented herein is a web-based system that aims not only to the retrieval of visually similar images, but also to determine the location they were taken by exploiting the available socially created metadata. This system makes use of a visual vocabulary and a bag-of words approach, in order to describe the visual properties of an image. Moreover, geometric constraints are applied, in order to extend the bag-of-words model towards more accurate results. We begin by describing some related work in the field of image retrieval, in order to present both the relation and the novelties of the presented system in comparison with the existing techniques.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C78,
   title = {Visual Image Retrieval and Localization},
   author = {Kalantidis, Yannis and Tolias, Giorgos and Spyrou, Evaggelos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 7th International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {6},
   address = {Chania, Greece},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2008"></a>
					2008
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C77"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C77" id="tog-C77">
							<i class="left-60 tog far fa-chevron-down"></i>
							Movie Summarization Based on Audiovisual Saliency Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Evangelopoulos, K. Rapantzikos, A. Potamianos, P. Maragos, A. Zlatintsi, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip.2008.4712308" title="DOI">ICIP&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C077.icip08.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C77.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/visual_saliency" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icip/EvangelopoulosRPMZA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_kc_bZDykSQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=4736478776377429315" title="Citations @ Google Scholar">77</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/52d39bb39d88f6024c8acb9968cec66dc98da6a3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/52d39bb39d88f6024c8acb9968cec66dc98da6a3#citing-papers" title="Citations @ Semantic Scholar">54</a>
						</div>

					</div>
					<div class="collapse" id="col-C77">
						<div class="pub-ref">
							In Proc. <em>15th International Conference on Image Processing</em><br>
							San Diego, CA, US  <span class="bull"></span> Oct 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Based on perceptual and computational attention modeling studies, we formulate measures of saliency for an audiovisual stream. Audio saliency is captured by signal modulations and related multifrequency band features, extracted through nonlinear operators and energy tracking. Visual saliency is measured by means of a spatiotemporal attention model driven by various feature cues (intensity, color, motion). Audio and video curves are integrated in a single attention curve, where events may be enhanced, suppressed or vanished. The presence of salient events is signified on this audiovisual curve by geometrical features such as local extrema, sharp transition points and level sets. An audiovisual saliency-based movie summarization algorithm is proposed and evaluated. The algorithm is shown to perform very well in terms of summary informativeness and enjoyability for movie clips of various genres.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C77,
   title = {Movie Summarization Based on Audiovisual Saliency Detection},
   author = {Evangelopoulos, Georgios and Rapantzikos, Konstantinos and Potamianos, Alexandros and Maragos, Petros and Zlatintsi, Athanasia and Avrithis, Yannis},
   booktitle = {Proceedings of 15th International Conference on Image Processing (ICIP)},
   month = {10},
   address = {San Diego, CA, US},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C76"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C76" id="tog-C76">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Region Semantics and Visual Context for Scene Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip.2008.4711689" title="DOI">MIR/ICIP&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C076.icip.kspace08.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C76.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icip/SpyrouMA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:k_IJM867U9cC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=433948086384497695" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/918daebd5ab37d8c96ce2e5ebb5ce7e263c64951" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/918daebd5ab37d8c96ce2e5ebb5ce7e263c64951#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C76">
						<div class="pub-ref">
							In Proc. <em>1st Workshop on Multimedia Information Retrieval: New Trends and Challenges</em><br>
							part of <em>International Conference on Image Processing</em><br>
							San Diego, CA, US  <span class="bull"></span> Oct 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we focus on scene classification and detection of high-level concepts within multimedia documents, by introducing an intermediate contextual approach as a means of exploiting the visual context of images. More specifically, we introduce and model a novel relational knowledge representation, founded on topological and semantic relations between the concepts of an image. We further develop an algorithm
								</p>
								<p>
									to address computationally efficient handling of visual context and extraction of mid-level region characteristics. Based on the proposed knowledge model, we combine the notion of visual context with region semantics, in order to exploit their efficacy in dealing with scene classification problems. Finally, initial experimental results are presented, in order to demonstrate possible applications of the proposed methodology.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C76,
   title = {Using Region Semantics And Visual Context For Scene Classification},
   author = {Spyrou, Evaggelos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 1st Workshop on Multimedia Information Retrieval: New Trends and Challenges (MIR), part of International Conference on Image Processing (ICIP)},
   month = {10},
   address = {San Diego, CA, US},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C75"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C75" id="tog-C75">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal Semantic Video Segmentation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Galmar, Th. Athanasiadis, B. Huet, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/mmsp.2008.4665143" title="DOI">MMSP&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C075.mmsp08a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C75.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/semantic_image_and_video_segmentation" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmsp/GalmarAHA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:HoB7MX3m0LUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5088491150296810362" title="Citations @ Google Scholar">13</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/68afe4d966bc183e46f805e0355917a84d5c7d91" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/68afe4d966bc183e46f805e0355917a84d5c7d91#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C75">
						<div class="pub-ref">
							In Proc. <em>10th International Workshop on Multimedia Signal Processing</em><br>
							Cairns, Australia  <span class="bull"></span> Oct 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, we propose a framework to extend semantic labeling of images to video shot sequences and achieve efficient and semantic-aware spatiotemporal video segmentation. This task faces two major challenges, namely the temporal variations within a video sequence which affect image segmentation and labeling, and the computational cost of region labeling. Guided by these limitations, we design a method where spatiotemporal segmentation and object labeling are coupled to achieve semantic annotation of video shots. An internal graph structure that describes both visual and semantic properties of image and video regions is adopted. The process of spatiotemporal semantic segmentation is subdivided in two stages: Firstly, the video shot is split into small block of frames. Spatiotemporal regions (volumes) are extracted and labeled individually within each block. Then, we iteratively merge consecutive blocks by a matching procedure which considers both semantic and visual properties. Results on real video sequences show the potential of our approach.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C75,
   title = {Spatiotemporal Semantic Video Segmentation},
   author = {Galmar, Eric and Athanasiadis, Thanos and Huet, Benoit and Avrithis, Yannis},
   publisher = {IEEE},
   booktitle = {Proceedings of 10th International Workshop on Multimedia Signal Processing (MMSP)},
   month = {10},
   address = {Cairns, Australia},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C74"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C74" id="tog-C74">
							<i class="left-60 tog far fa-chevron-down"></i>
							Regions of Interest for Accurate Object Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Kapsalas, K. Rapantzikos, A. Sofou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/cbmi.2008.4564940" title="DOI">CBMI&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C074.cbmi08a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C74.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/object_detection_based_local_region_representation" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cbmi/KapsalasRSA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:M3NEmzRMIkIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3570081393876977548" title="Citations @ Google Scholar">37</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/b892d81fda105ff8109229536b89e65e323685a0" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/b892d81fda105ff8109229536b89e65e323685a0#citing-papers" title="Citations @ Semantic Scholar">28</a>
						</div>

					</div>
					<div class="collapse" id="col-C74">
						<div class="pub-ref">
							In Proc. <em>6th International Workshop on Content-Based Multimedia Indexing</em><br>
							London, UK  <span class="bull"></span> Jun 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in such a way that out of context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C74,
   title = {Regions Of Interest for Accurate Object Detection},
   author = {Kapsalas, Petros and Rapantzikos, Konstantinos and Sofou, Anastasia and Avrithis, Yannis},
   booktitle = {Proceedings of 6th International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {6},
   address = {London, UK},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C73"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C73" id="tog-C73">
							<i class="left-60 tog far fa-chevron-down"></i>
							Affine Invariant Curve Matching Using Normalization and Curvature Scale-Space
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">V. Giannekou, P. Tzouveli, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/cbmi.2008.4564948" title="DOI">CBMI&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C073.cbmi08b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C73.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/cbmi/GiannekouTAK08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:bFI3QPDXJZMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16005876378013636212" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/c182616f500010133726054e1d0053bf71a71a40" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/c182616f500010133726054e1d0053bf71a71a40#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-C73">
						<div class="pub-ref">
							In Proc. <em>6th International Workshop on Content-Based Multimedia Indexing</em><br>
							London, UK  <span class="bull"></span> Jun 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an affine invariant curve matching method using curvature scale-space and normalization is proposed. Prior to curve matching, curve normalization with respect to affine transformations is applied, allowing a lossless affine invariant curve representation. The maxima points of the curvature scale-space (CSS) image are then used to represent the normalized curve, while retaining the local properties of the curve. The matching algorithm that follows, matches the maxima sets of CSS images and the resulting matching cost provides a measure of similarity. The method's performance and robustness is evaluated through a variety of curves and affine transformations, obtaining precise shape similarity and retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C73,
   title = {Affine invariant curve matching using normalization and curvature scale-space},
   author = {Giannekou, Vicky and Tzouveli, Paraskevi and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 6th International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {6},
   address = {London, UK},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C72"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C72" id="tog-C72">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Visual Context Ontology for Multimedia High-Level Concept Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">MRC&nbsp;2008</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C072.mrc08.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C72.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:tS2w5q8j5-wC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10868700492044353987" title="Citations @ Google Scholar">1</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/85aaf51eeca01a6490049d732bf39d6d5b3f737f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/85aaf51eeca01a6490049d732bf39d6d5b3f737f#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C72">
						<div class="pub-ref">
							In Proc. <em>5th International Workshop in Modeling and Reasoning in Context</em><br>
							Delft, The Netherlands  <span class="bull"></span> Jun 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The notion of context plays a significant role in multimedia content search and retrieval systems. In this paper we focus our research efforts on a visual context knowledge representation, to be utilized for multimedia high-level concept detection. We propose and describe in detail types of contextual relations evident within the multimedia content, model them and provide a clear methodology on how to extract them. A visual context ontology is introduced, containing relations among different types of content entities, such as images, regions, region types and high-level concepts. In this manner, we facilitate traditional object detection approaches towards semantical interpretation. The application of the proposed knowledge structure provides encouraging initial results, improving the efficacy of related multimedia analysis techniques.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C72,
   title = {A Visual Context Ontology for Multimedia High-Level Concept Detection},
   author = {Spyrou, Evaggelos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 5th International Workshop in Modeling and Reasoning in Context (MRC)},
   month = {6},
   address = {Delft, The Netherlands},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C71"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C71" id="tog-C71">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Semantic Multimedia Analysis Approach Utilizing a Region Thesaurus and LSA
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Tolias, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/wiamis.2008.49" title="DOI">WIAMIS&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C071.wiamis08.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C71.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wiamis/SpyrouTMA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:4JMBOYKVnBMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5099555260260891291" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7747a23e8378b75de5659d25dc60d2a33a6a53dd" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7747a23e8378b75de5659d25dc60d2a33a6a53dd#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C71">
						<div class="pub-ref">
							In Proc. <em>9th International Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Klagenfurt, Austria  <span class="bull"></span> May 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents an approach on high-level feature detection within video documents, using a Region Thesaurus and Latent Semantic Analysis. A video shot is represented by a single keyframe. MPEG-7 features are extracted from coarse regions of it. A clustering algorithm is applied on all extracted regions and a region thesaurus is constructed. Its use is to assist to the mapping of low- to high-level features by a model vector representation. Latent Semantic Analysis is then applied on the model vectors to exploit the latent relations among regions types aiming to improve detection performance. The proposed approach is thoroughly examined using TRECVID 2007 development data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C71,
   title = {A Semantic Multimedia Analysis Approach Utilizing a Region Thesaurus and {LSA}},
   author = {Spyrou, Evaggelos and Tolias, Giorgos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 9th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {5},
   address = {Klagenfurt, Austria},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2007"></a>
					2007
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C70"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C70" id="tog-C70">
							<i class="left-60 tog far fa-chevron-down"></i>
							High-Level Concept Detection Based on Mid-Level Semantic Information and Contextual Adaptation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/smap.2007.38" title="DOI">SMAP&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C070.smap07a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C70.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/smap/MylonasSA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:NaGl4SEjCO4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=4769449773740600680" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a8f761b3091f5bea56fd5e1d3fc516168c578b43" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a8f761b3091f5bea56fd5e1d3fc516168c578b43#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C70">
						<div class="pub-ref">
							In Proc. <em>2nd International Workshop on Semantic Media Adaptation and Personalization</em><br>
							London, UK  <span class="bull"></span> Dec 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we propose the use of enhanced mid-level information, such as information obtained from the application of supervised or unsupervised learning methodologies on low-level characteristics, in order to improve semantic multimedia analysis. High-level, a priori contextual knowledge about the semantic meaning of objects and their low-level visual descriptions are combined in an integrated approach that handles in a uniform way the gap between semantics and low-level features. Prior work on low-level feature extraction is extended and a region thesaurus containing all mid-level features is constructed using a hierarchical clustering method. A model vector that contains the distances from each mid-level element is formed and a Neural Network-based detector is trained for each semantic concept. Contextual adaptation improves the quality of the produced results, by utilizing fuzzy algebra, fuzzy sets and relations. The novelty of the presented work is the context-driven mid-level manipulation of region types, utilizing a domain-independent ontology infrastructure to handle the knowledge. Early experimental results are presented using data derived from the beach domain.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C70,
   title = {High-Level Concept Detection based on Mid-level Semantic Information and Contextual Adaptation},
   author = {Mylonas, Phivos and Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 2nd International Workshop on Semantic Media Adaptation and Personalization (SMAP)},
   month = {12},
   address = {London, UK},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C69"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C69" id="tog-C69">
							<i class="left-60 tog far fa-chevron-down"></i>
							Keyframe Extraction Using Local Visual Semantics in the Form of a Region Thesaurus
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/smap.2007.39" title="DOI">SMAP&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C069.smap07b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C69.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/smap/SpyrouA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:J_g5lzvAfSwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7318965545294062533" title="Citations @ Google Scholar">12</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7386e89b4a7e66e7d45b5e1737c3d93f57ac136b" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7386e89b4a7e66e7d45b5e1737c3d93f57ac136b#citing-papers" title="Citations @ Semantic Scholar">13</a>
						</div>

					</div>
					<div class="collapse" id="col-C69">
						<div class="pub-ref">
							In Proc. <em>2nd International Workshop on Semantic Media Adaptation and Personalization</em><br>
							London, UK  <span class="bull"></span> Dec 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents an approach for efficient keyframe extraction, using local semantics in form of a region thesaurus. More specifically, certain MPEG-7 color and texture features are locally extracted from keyframe regions. Then, using a hierarchical clustering approach a local region thesaurus is constructed to facilitate the description of each frame in terms of higher semantic features. The feature is consisted by the most common region types that are encountered within the video shot, along with their synonyms. These region types carry semantic information. Each keyframe is represented by a vector consisting of the degrees of confidence of the existence of all region types within this shot. Using this keyframe representation, the most representative keyframe is then selected for each shot. Where a single keyframe is not adequate, using the same algorithm and exploiting the coverage of the visual thesaurus, more keyframes are extracted.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C69,
   title = {Keyframe Extraction using Local Visual Semantics in the form of a Region Thesaurus},
   author = {Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 2nd International Workshop on Semantic Media Adaptation and Personalization (SMAP)},
   month = {12},
   address = {London, UK},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C68"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C68" id="tog-C68">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Region Thesaurus Approach for High-Level Concept Detection in the Natural Disaster Domain
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-3-540-77051-0_7" title="DOI">SAMT&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C068.samt07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C68.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/SpyrouA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:M3ejUd6NZC8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=8667651852576709695" title="Citations @ Google Scholar">20</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/45c08763f733ff1a1c5f2474684cdd7bc5ded50d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/45c08763f733ff1a1c5f2474684cdd7bc5ded50d#citing-papers" title="Citations @ Semantic Scholar">14</a>
						</div>

					</div>
					<div class="collapse" id="col-C68">
						<div class="pub-ref">
							In Proc. <em>2nd International Conference on Semantics and Digital Media Technologies</em><br>
							Genova, Italy  <span class="bull"></span> Dec 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents an approach on high-level feature detection using a region thesaurus. MPEG-7 features are locally extracted from segmented regions and for a large set of images. A hierarchical clustering approach is applied and a relatively small number of region types is selected. This set of region types defines the region thesaurus. Using this thesaurus, low-level features are mapped to high-level concepts as model vectors. This representation is then used to train support vector machine-based feature detectors. As a next step, latent semantic analysis is applied on the model vectors, to further improve the analysis performance. High-level concepts detected derive from the natural disaster domain.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C68,
   title = {A Region Thesaurus Approach for High-Level Concept Detection in the Natural Disaster Domain},
   author = {Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 2nd International Conference on Semantics and Digital Media Technologies (SAMT)},
   month = {12},
   address = {Genova, Italy},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C67"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C67" id="tog-C67">
							<i class="left-60 tog far fa-chevron-down"></i>
							Enriching a Context Ontology with Mid-Level Features for Semantic Multimedia Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">MARESO/SAMT&nbsp;2007</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C067.samt.mareso07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C67.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:isC4tDSrTZIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18329075245981425946" title="Citations @ Google Scholar">8</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/672d2d910074ba9f4eaf3af63963d90844743eec" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/672d2d910074ba9f4eaf3af63963d90844743eec#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-C67">
						<div class="pub-ref">
							In Proc. <em>1st Workshop on Multimedia Annotation and Retrieval enabled by Shared Ontologies</em><br>
							part of <em>International Conference on Semantics And Digital Media Technologies</em><br>
							Genova, Italy  <span class="bull"></span> Dec 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we focus on a contextual domain ontology representation aiding in the process of knowledge-assisted multimedia analysis. Previous work on the detection of high-level concepts within multimedia documents is extended by introducing a "mid-level" ontology as a means of exploiting the visual context of images, in terms of high-level concepts and mid-level region types they consist of. More specifically, we introduce a context ontology, define its components, its relations and integrate it in our knowledge modelling approach. In previous works we have developed algorithms to address computationally efficient handling of visual context and extraction of mid-level characteristics and now we expect these diverse algorithms and methodologies to be combined in order to exploit the proposed knowledge model. The ultimate goal remains that of efficient semantic multimedia analysis. Finally, a use case scenario derived from the beach domain is also presented, in order to demonstrate a possible application of the proposed knowledge representation.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C67,
   title = {Enriching a context ontology with mid-level features for semantic multimedia analysis},
   author = {Mylonas, Phivos and Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 1st Workshop on Multimedia Annotation and Retrieval enabled by Shared Ontologies (MARESO), part of International Conference on Semantics And Digital Media Technologies (SAMT)},
   month = {12},
   address = {Genova, Italy},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C66"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C66" id="tog-C66">
							<i class="left-60 tog far fa-chevron-down"></i>
							Towards Semantic Multimedia Indexing by Classification and Reasoning on Textual Metadata
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, N. Simou, V. Tzouvaras, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-253/paper03.pdf" title="Electronic edition">KAMC/SAMT&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C066.samt.kamc07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C66.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/MylonasSTA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:JV2RwH3_ST0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=4652825456660928567" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/3000c0d3ce7ffe132e2f3c52276bef87a5148a78" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/3000c0d3ce7ffe132e2f3c52276bef87a5148a78#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-C66">
						<div class="pub-ref">
							In Proc. <em>Knowledge Acquisition from Multimedia Content Workshop</em><br>
							part of <em>International Conference on Semantics And Digital Media Technologies</em><br>
							Genova, Italy  <span class="bull"></span> Dec 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The task of multimedia document categorization forms a well-known problem in information retrieval. The task is to assign a multimedia document to one or more categories, based on its contents. In this case, effective management and thematic categorization requires the extraction of the underlying semantics. The proposed approach utilizes as input, analyzes and exploits the textual annotation that accompanies a multimedia document, in order to extract its underlying semantics, construct a semantic index and finally classify the documents to thematic categories. This process is based on a unified knowledge and semantics representation model introduced, as well as basic principles of fuzzy relational algebra. On top of that the fuzzy extension of expressive description logic language SHIN, f-SHIN and its reasoning services are used to further refine and optimize the initial categorization results. The proposed approach was tested on a set of real-life multimedia documents, derived from the Internet, as well as personal databases and shows rather promising results.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C66,
   title = {Towards Semantic Multimedia Indexing by Classification and Reasoning on Textual Metadata},
   author = {Mylonas, Phivos and Simou, Nikolaos and Tzouvaras, Vassilis and Avrithis, Yannis},
   booktitle = {Proceedings of Knowledge Acquisition from Multimedia Content Workshop (KAMC), part of International Conference on Semantics And Digital Media Technologies (SAMT)},
   month = {12},
   address = {Genova, Italy},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C65"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C65" id="tog-C65">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Audio-Visual Saliency Model for Movie Summarization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, G. Evangelopoulos, P. Maragos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/mmsp.2007.4412882" title="DOI">MMSP&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C065.mmsp07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C65.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmsp/RapantzikosEMA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:YFjsv_pBGBYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11875037912784181155" title="Citations @ Google Scholar">27</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/fe05f7d27d24ed60591e2db8600ac1906ccb5983" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/fe05f7d27d24ed60591e2db8600ac1906ccb5983#citing-papers" title="Citations @ Semantic Scholar">19</a>
						</div>

					</div>
					<div class="collapse" id="col-C65">
						<div class="pub-ref">
							In Proc. <em>IEEE International Workshop on Multimedia Signal Processing</em><br>
							Crete, Greece  <span class="bull"></span> Oct 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A saliency-based method for generating video summaries is presented, which exploits coupled audiovisual information from both media streams. Efficient and advanced speech and image processing algorithms to detect key frames that are acoustically and visually salient are used. Promising results are shown from experiments on a movie database.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C65,
   title = {An Audio-Visual Saliency Model for Movie Summarization},
   author = {Rapantzikos, Konstantinos and Evangelopoulos, Georgios and Maragos, Petros and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE International Workshop on Multimedia Signal Processing (MMSP)},
   month = {10},
   address = {Crete, Greece},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C64"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C64" id="tog-C64">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Multimedia Analysis Based on Region Types and Visual Context
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-0-387-74161-1_42" title="DOI">AIAI&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C064.aiai07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C64.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/ifip12/SpyrouMA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:EUQCXRtRnyEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18175681250253074649" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/8f28f9f39966a51e6a41b8637e4fde4c3682e607" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/8f28f9f39966a51e6a41b8637e4fde4c3682e607#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C64">
						<div class="pub-ref">
							In Proc. <em>4th IFIP Conference on Artificial Intelligence Applications and Innovations</em><br>
							Athens, Greece  <span class="bull"></span> Sep 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper previous work on the detection of high-level concepts within multimedia documents is extended by introducing a mid-level ontology as a means of exploiting the visual context of images in terms of the regions they consist of. More specifically, we construct a mid-level ontology, define its relations and integrate it in our knowledge modelling approach. In the past we have developed algorithms to address computationally efficient handling of visual context and extraction of mid-level characteristics and now we explain how these diverse algorithms and methodologies can be combined in order to approach a greater goal, that of semantic multimedia analysis. Early experimental results are presented using data derived from the beach domain.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C64,
   title = {Semantic Multimedia Analysis based on Region Types and Visual Context},
   author = {Spyrou, Evaggelos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 4th IFIP Conference on Artificial Intelligence Applications and Innovations (AIAI)},
   month = {9},
   address = {Athens, Greece},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C63"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C63" id="tog-C63">
							<i class="left-60 tog far fa-chevron-down"></i>
							Salienshrink: Saliency-Based Wavelet Shrinkage
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip.2007.4378952" title="DOI">ICIP&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C063.icip07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C63.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icip/RapantzikosAK07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:bnK-pcrLprsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/1a14aad22ae8435603349d7e5a24b3a4fc017678" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C63">
						<div class="pub-ref">
							In Proc. <em>14th International Conference on Image Processing</em><br>
							San Antonio, TX, US  <span class="bull"></span> Sep 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper describes salienShrink, a method to denoise images based on computing a map of salient coefficients in the wavelet domain and use it to improve common denoising algorithms. By salient, we refer to those coefficients that correspond mostly to pure signal and should therefore be preserved throughout the denoising procedure. We use a computationally efficient model to detect salient regions in the bands of the multiresolution wavelet transform. These regions are used to obtain a more accurate estimate of the noise level, improving the performance of existing well known shrinkage methods. Extensive experimental results on the BiShrink method show that the proposed method effectively enhances PSNR and improves the visual quality of the denoised images.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C63,
   title = {SALIENShrink: Saliency-Based Wavelet Shrinkage},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 14th International Conference on Image Processing (ICIP)},
   volume = {3},
   month = {9},
   pages = {333--336},
   address = {San Antonio, TX, US},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C62"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C62" id="tog-C62">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal Saliency for Event Detection and Representation in the 3D Wavelet Domain: Potential in Human Action Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1145/1282280.1282326" title="DOI">CIVR&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C062.civr07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C62.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/civr/RapantzikosAK07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:hMod-77fHWUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7459348755046647968" title="Citations @ Google Scholar">47</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/917f92401834a511026b3de0369424b80d20fdc1" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/917f92401834a511026b3de0369424b80d20fdc1#citing-papers" title="Citations @ Semantic Scholar">35</a>
						</div>

					</div>
					<div class="collapse" id="col-C62">
						<div class="pub-ref">
							In Proc. <em>ACM International Conference on Image and Video Retrieval</em><br>
							Amsterdam, The Netherlands  <span class="bull"></span> Jul 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Event detection and recognition is still one of the most active fields in computer vision, since the complexity of the dynamic events and the need for computational efficient solutions pose several difficulties. This paper addresses detection and representation of spatiotemporal salient regions using the 3D Discrete Wavelet Transform (DWT). We propose a framework to measure saliency based on the orientation selective bands of the 3D DWT and represent events using simple features of salient regions. We apply this method to human action recognition, test it on a large public video database consisting of six human actions and compare the results against an established method in the literature. Qualitative and quantitative evaluation indicates the potential of the proposed method to localize and represent human actions.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C62,
   title = {Spatiotemporal saliency for event detection and representation in the 3D Wavelet Domain: Potential in human action recognition},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of ACM International Conference on Image and Video Retrieval (CIVR)},
   month = {7},
   pages = {294--301},
   address = {Amsterdam, The Netherlands},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C61"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C61" id="tog-C61">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Multiple Domain Visual Context in Image Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/wiamis.2007.86" title="DOI">WIAMIS&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C061.wiamis07.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C61.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/wiamis/MylonasA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:35N4QoGY0k4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14057321522306538297" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/68416c0cd54b34ec095ece79519319c1ef10235b" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/68416c0cd54b34ec095ece79519319c1ef10235b#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C61">
						<div class="pub-ref">
							In Proc. <em>8th International Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Santorini, Greece  <span class="bull"></span> Jun 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we propose an algorithm to improve the results of knowledge-assisted image analysis, based on contextual information. In order to achieve this, we utilize fuzzy algebra, fuzzy sets and relations, towards efficient manipulation of image region concepts. We provide a novel context modelling, based on the OWL language, using RDF reification. Initial image analysis results are enhanced by the utilization of domain-independent, semantic knowledge in terms of concepts and relations between them. The novelty of the presented work is the context-driven re-adjustment of the degrees of confidence of the detected concepts produced by any image analysis technique, utilizing a domain-independent ontology infrastructure to handle the knowledge, as well as multiple application domains.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C61,
   title = {Using Multiple Domain Visual Context in Image Analysis},
   author = {Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 8th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {6},
   address = {Santorini, Greece},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2006"></a>
					2006
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C60"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C60" id="tog-C60">
							<i class="left-60 tog far fa-chevron-down"></i>
							Rule-Based Reasoning for Semantic Image Segmentation and Interpretation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Berka, Th. Athanasiadis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-233/p39.pdf" title="Electronic edition">SAMT&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C060.samt06d.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C60.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/BerkaAA06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ye4kPcJQO24C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11579982539643374556" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/8b8ece3321acccc06b7f1c4dfdddaf45b3b39f9c" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/8b8ece3321acccc06b7f1c4dfdddaf45b3b39f9c#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C60">
						<div class="pub-ref">
							In Proc. <em>1st International Conference on Semantics And digital Media Technology</em><br>
							Athens, Greece  <span class="bull"></span> Dec 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, we propose the application of rule-based reasoning for knowledge assisted image segmentation and object detection. A region merging approach is proposed based on fuzzy labeling and not on visual descriptors, while reasoning is used in evaluation of dissimilarity between adjacent regions according to rules applied on local information.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C60,
   title = {Rule-based Reasoning for Semantic Image Segmentation and Interpretation},
   author = {Berka, Petr and Athanasiadis, Thanos and Avrithis, Yannis},
   publisher = {CEUR-WS},
   booktitle = {Poster \& Demo Proceedings of 1st International Conference on Semantics And digital Media Technology (SAMT)},
   month = {12},
   pages = {39--40},
   address = {Athens, Greece},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C59"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C59" id="tog-C59">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Context-Based Region Labeling Approach for Semantic Image Segmentation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/11930334_17" title="DOI">SAMT&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C059.samt06b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C59.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/AthanasiadisMA06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:NMxIlDl6LWMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10930437777471468032" title="Citations @ Google Scholar">12</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/8d45144e39f4b58269cd4e18dab81b631698c028" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/8d45144e39f4b58269cd4e18dab81b631698c028#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C59">
						<div class="pub-ref">
							In Proc. <em>1st International Conference on Semantics And digital Media Technology</em><br>
							Athens, Greece  <span class="bull"></span> Dec 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we present a framework for simultaneous image segmentation and region labeling leading to automatic image annotation. The proposed framework operates at semantic level using possible semantic labels to make decisions on handling image regions instead of visual features used traditionally. In order to stress its independence of a specific image segmentation approach we applied our idea on two region growing algorithms, i.e. watershed and recursive shortest spanning tree. Additionally we exploit the notion of visual context by employing fuzzy algebra and ontological taxonomic knowledge representation, incorporating in this way global information and improving region interpretation. In this process, semantic region growing labeling results are being re-adjusted appropriately, utilizing contextual knowledge in the form of domain-specific semantic concepts and relations. The performance of the overall methodology is demonstrated on a real-life still image dataset from the popular domains of beach holidays and motorsports.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C59,
   title = {A Context-based Region Labeling Approach for Semantic Image Segmentation},
   author = {Athanasiadis, Thanos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 1st International Conference on Semantics And digital Media Technology (SAMT)},
   month = {12},
   pages = {212--225},
   address = {Athens, Greece},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C58"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C58" id="tog-C58">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Context and a Genetic Algorithm for Knowledge-Assisted Image Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Dasiopoulou, G. Papadopoulos, Ph. Mylonas, Y. Avrithis, I. Kompatsiaris</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-233/p21.pdf" title="Electronic edition">SAMT&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C058.samt06a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C58.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/DasiopoulouPMAK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:LjlpjdlvIbIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/27f83f8e6866df50a65e7d9a2de46a9580d1f1bb" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C58">
						<div class="pub-ref">
							In Proc. <em>1st International Conference on Semantics And Digital Media Technology</em><br>
							Athens, Greece  <span class="bull"></span> Dec 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this poster, we present an approach to contextualized semantic image annotation as an optimization problem. Ontologies are used to capture general and contextual knowledge of the domain considered, and a genetic algorithm is applied to realize the final annotation. Experiments with images from the beach vacation domain demonstrate the performance of the proposed approach and illustrate the added value of utilizing contextual information.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C58,
   title = {Using Context and a Genetic Algorithm for Knowledge-Assisted Image Analysis},
   author = {Dasiopoulou, Stamatia and Papadopoulos, Georgios and Mylonas, Phivos and Avrithis, Yannis and Kompatsiaris, Ioannis},
   booktitle = {Proceedings of 1st International Conference on Semantics And Digital Media Technology (SAMT)},
   month = {12},
   address = {Athens, Greece},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C57"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C57" id="tog-C57">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Local Region Semantics for Concept Detection in Video
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Koumoulos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-233/p31.pdf" title="Electronic edition">SAMT&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C057.samt06c.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C57.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/SpyrouKAK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:zA6iFVUQeVQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17767892724262661128" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/530a0ad3dd0f47bb28c3758fbf26e6af106d6a2f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/530a0ad3dd0f47bb28c3758fbf26e6af106d6a2f#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C57">
						<div class="pub-ref">
							In Proc. <em>1st International Conference on Semantics And digital Media Technology</em><br>
							Athens, Greece  <span class="bull"></span> Dec 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents a framework for the detection of semantic features in video sequences. Low-level feature extraction is performed on the keyframes of the shots and a "feature vector" including color and texture features is formed. A region "thesaurus" that contains all the high-level features is constructed using a subtractive clustering method.Then, a "model vector" that contains the distances from each region type is formed and a SVM detector is trained for each semantic concept. Experiments were performed using TRECVID 2005 development data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C57,
   title = {Using Local Region Semantics for Concept Detection in Video},
   author = {Spyrou, Evaggelos and Koumoulos, George and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 1st International Conference on Semantics And digital Media Technology (SAMT)},
   month = {12},
   address = {Athens, Greece},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C56"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C56" id="tog-C56">
							<i class="left-60 tog far fa-chevron-down"></i>
							Image Analysis Using Domain Knowledge and Visual Context
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, Th. Athanasiadis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">IWSSIP&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C056.iwssip06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C56.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:rO6llkc54NcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15166657121407296970" title="Citations @ Google Scholar">4</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/98ad30f98c32be8f4d53e34750f3b1534c20f815" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/98ad30f98c32be8f4d53e34750f3b1534c20f815#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C56">
						<div class="pub-ref">
							In Proc. <em>13th International Conference on Systems, Signals and Image Processing</em><br>
							Budapest, Hungary  <span class="bull"></span> Sep 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Tackling the problems of automatic object recognition and/or scene classification with generic algorithms is not producing efficient and reliable results in the field of image analysis. Restricting the problem to a specific domain is a common approach to cope with this, still unresolved, issue. In this paper we propose a methodology to improve the results of image analysis, based on available contextual information derived from the popular sports domain. Our research efforts include application of a knowledge-assisted image analysis algorithm that utilizes an ontology infrastructure to handle knowledge and MPEG-7 visual descriptors for region labeling. A novel ontological representation for context is introduced, combining fuzziness with Semantic Web characteristics, such as RDF. Initial region labeling analysis results are then being re-adjusted appropriately according to a confidence value readjustment algorithm, by means of fine-tuning the degrees of confidence of each detected region label. In this process contextual knowledge in the form of domain-specific semantic concepts and relations is utilized. Performance of the overall methodology is demonstrated through its application on a real-life still image dataset derived from the tennis sub-domain.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C56,
   title = {Image Analysis Using Domain Knowledge and Visual Context},
   author = {Mylonas, Phivos and Athanasiadis, Thanos and Avrithis, Yannis},
   booktitle = {Proceedings of 13th International Conference on Systems, Signals and Image Processing (IWSSIP)},
   month = {9},
   address = {Budapest, Hungary},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C55"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C55" id="tog-C55">
							<i class="left-60 tog far fa-chevron-down"></i>
							Priority Coding for Video-Telephony Applications Based on Visual Attention
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Tsapatsoulis, K. Rapantzikos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1145/1374296.1374329" title="DOI">MobiMedia&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C055.mobimedia06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C55.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mobimedia/TsapatsoulisRA06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:cFHS6HbyZ2cC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10230809086850606399" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/f6a217416c010b603d2f54da418e48efe7fe6894" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/f6a217416c010b603d2f54da418e48efe7fe6894#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-C55">
						<div class="pub-ref">
							In Proc. <em>2nd International Mobile Multimedia Communications Conference</em><br>
							Alghero, Italy  <span class="bull"></span> Sep 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we investigate the utilization of visual saliency maps for ROI-based video coding of video-telephony applications. Visually salient areas indicated in the saliency map are considered as ROIs. These areas are automatically detected using an algorithm for visual attention (VA) which builds on the bottom-up approach proposed by Itti et al. A top-down channel emulating the visual search for human faces performed by humans has been added, while orientation, intensity and color conspicuity maps are computed within a unified multi-resolution framework based on wavelet subband analysis. Priority encoding, for experimentation purposes, is utilized in a simple manner: Frame areas outside the priority regions are blurred using a smoothing filter and then passed to the video encoder. This leads to better compression of both Intra-coded (I) frames (more DCT coefficients are zeroed in the DCT-quantization step) and Inter coded (P,B) frames (lower prediction error). In more sophisticated approaches, priority encoding could be incorporated by varying the quality factor of the DCT quantization table. Extended experiments concerning both static images as well as low-quality video show the compression efficiency of the proposed method. The comparisons are made against standard JPEG and MPEG-1 encoding respectively.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C55,
   title = {Priority Coding for Video-telephony Applications based on Visual Attention},
   author = {Tsapatsoulis, Nicolas and Rapantzikos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of 2nd International Mobile Multimedia Communications Conference (MobiMedia)},
   month = {9},
   address = {Alghero, Italy},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C54"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C54" id="tog-C54">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Contextual Personalization Approach Based on Ontological Knowledge
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Vallet, M. Fernndez, P. Castells, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-210/paper8.pdf" title="Electronic edition">CO/ECAI&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C054.ecai.co06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C54.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/ecai/ValletFCMA06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:hFOr9nPyWt4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14698910065463845369,8218062686443519024" title="Citations @ Google Scholar">25</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/69baaa84c5ffcfd77f2219331412045cb71703f0" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/69baaa84c5ffcfd77f2219331412045cb71703f0#citing-papers" title="Citations @ Semantic Scholar">15</a>
						</div>

					</div>
					<div class="collapse" id="col-C54">
						<div class="pub-ref">
							In Proc. <em>Contexts and Ontologies: Theory, Practice and Applications Workshop</em><br>
							part of <em>17th European Conference on Artificial Intelligence</em><br>
							Riva del Garda, Italy  <span class="bull"></span> Aug 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Combining traditional personalization techniques with novel knowledge representation paradigms, such as the ontology-based approach proposed in the Semantic Web field, is a challenging task. Personalization is a difficult problem when dealing with multimedia content and information retrieval, where context is increasingly acknowledged to be a key notion in order to make proper sense of user needs. This work focuses on contextualization within personalization in a multimedia environment. Towards that scope, we propose a novel contextual knowledge modeling scheme, and an approach for the dynamic, contextual activation of semantic user preferences to better represent user interests in coherence with ongoing user activities, e.g. in an interactive retrieval process. The application of this methodology is demonstrated using two user scenarios, and the performance results of a preliminary experiment are shown.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C54,
   title = {A contextual personalization approach based on ontological knowledge},
   author = {Vallet, David and Fern\'andez, Miriam and Castells, Pablo and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of Contexts and Ontologies: Theory, Practice and Applications Workshop (CO), part of 17th European Conference on Artificial Intelligence (ECAI)},
   month = {8},
   address = {Riva del Garda, Italy},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C53"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C53" id="tog-C53">
							<i class="left-60 tog far fa-chevron-down"></i>
							Personalized Information Retrieval in Context
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Vallet, M. Fernndez, P. Castells, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">MRC/AAAI&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C053.aaai.mrc06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C53.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:0EnyYjriUFMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15541278535652665898" title="Citations @ Google Scholar">44</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/4bcdfebf4ed40bd552b29bb1d0c98ddf9314d183" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/4bcdfebf4ed40bd552b29bb1d0c98ddf9314d183#citing-papers" title="Citations @ Semantic Scholar">33</a>
						</div>

					</div>
					<div class="collapse" id="col-C53">
						<div class="pub-ref">
							In Proc. <em>3rd International Workshop on Modeling and Retrieval of Context</em><br>
							part of <em>21st National Conference on Artificial Intelligence</em><br>
							Boston, MA, US  <span class="bull"></span> Jul 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in such a way that out of context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C53,
   title = {Personalized Information Retrieval in Context},
   author = {Vallet, David and Fern\'andez, Miriam and Castells, Pablo and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 3rd International Workshop on Modeling and Retrieval of Context (MRC), part of 21st National Conference on Artificial Intelligence (AAAI)},
   month = {7},
   address = {Boston, MA, US},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C52"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C52" id="tog-C52">
							<i class="left-60 tog far fa-chevron-down"></i>
							Ontology-Based Personalization for Multimedia Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, D. Vallet, M. Fernndez, P. Castells, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SWP/ESWC&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C052.eswc.swp06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C52.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:lSLTfruPkqcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9388261505124609543" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/aa592c9214138896470bc2bcfab552ed39d4e8fd" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/aa592c9214138896470bc2bcfab552ed39d4e8fd#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-C52">
						<div class="pub-ref">
							In Proc. <em>Semantic Web Personalization Workshop</em><br>
							part of <em>3rd European Semantic Web Conference</em><br>
							Budva, Montenegro  <span class="bull"></span> Jun 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Personalization is a difficult problem related to fields and applications ranging from information retrieval to multimedia content manipulation. Challenge is greater, when trying to combine traditional personalization techniques with novel knowledge representations like ontologies. This paper proposes a novel contextual knowledge modeling, based on ontologies and fuzzy relations and exploits it in user profiling representation, extraction and use. The personalized results of the application of this methodology are then ranked accordingly. The performance of the proposed techniques is demonstrated through preliminary experimental results derived from a real-life data set.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C52,
   title = {Ontology-based Personalization for Multimedia Content},
   author = {Mylonas, Phivos and Vallet, David and Fern\'andez, Miriam and Castells, Pablo and Avrithis, Yannis},
   booktitle = {Proceedings of Semantic Web Personalization Workshop (SWP), part of 3rd European Semantic Web Conference (ESWC)},
   month = {6},
   address = {Budva, Montenegro},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C51"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C51" id="tog-C51">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fast Video Object Tracking Using Affine Invariant Normalization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Tzouveli, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/0-387-34224-9_64" title="DOI">AIAI&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C051.aiai06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C51.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/ifip12/TzouveliAK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:q3oQSFYPqjQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16391322664544684634" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/08ab0e0e858762d6b679800ea3af1e65afc6fd6a" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/08ab0e0e858762d6b679800ea3af1e65afc6fd6a#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C51">
						<div class="pub-ref">
							In Proc. <em>3rd IFIP Conference on Artificial Intelligence Applications & Innovations</em><br>
							Athens, Greece  <span class="bull"></span> Jun 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									One of the most common problems in computer vision and image processing applications is the localization of object boundaries in a video frame and its tracking in the next frames. In this paper, a fully automatic method for fast tracking of video objects in a video sequence using affine invariant normalization is proposed. Initially, the detection of a video object is achieved using a GVF snake. Next, a vector of the affine parameters of each contour of the extracted video object in two successive frames is computed using affine-invariant normalization. Under the hypothesis that these contours are similar, the affine transformation between the two contours is computed in a very fast way. Using this transformation to predict the position of the contour in the next frame allows initialization of the GVF snake very close to the real position. Applying this technique to the following frames, a very fast tracking technique is achieved. Moreover, this technique can be applied on sequences with very fast moving objects where traditional trackers usually fail. Results on synthetic sequences are presented which illustrate the theoretical developments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C51,
   title = {Fast Video Object Tracking using Affine Invariant Normalization},
   author = {Tzouveli, Paraskevi and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 3rd IFIP Conference on Artificial Intelligence Applications \& Innovations (AIAI)},
   month = {6},
   address = {Athens, Greece},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C50"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C50" id="tog-C50">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Semantic Region Growing Approach in Image Segmentation and Annotation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SWAMM/WWW&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C050.swamm06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C50.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:BqipwSGYUEgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12056231658487879919" title="Citations @ Google Scholar">8</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5545b46205cffcc57d9f22b8c46e708275ee07dd" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/5545b46205cffcc57d9f22b8c46e708275ee07dd#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-C50">
						<div class="pub-ref">
							In Proc. <em>1st International Workshop on Semantic Web Annotations for Multimedia</em><br>
							part of <em>15th World Wide Web Conference</em><br>
							Edinburgh, UK  <span class="bull"></span> May 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this position paper we examine the limitation of region growing segmentation techniques to extract semantically meaningful objects from an image. We propose a region growing algorithm that performs on a semantic level, driven by the knowledge of what each region represents at every iteration step of the merging process. This approach utilizes simultaneous segmentation and labeling of regions leading to automatic image annotation.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C50,
   title = {A Semantic Region Growing Approach in Image Segmentation and Annotation},
   author = {Athanasiadis, Thanos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 1st International Workshop on Semantic Web Annotations for Multimedia (SWAMM), part of 15th World Wide Web Conference (WWW)},
   month = {5},
   address = {Edinburgh, UK},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C49"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C49" id="tog-C49">
							<i class="left-60 tog far fa-chevron-down"></i>
							Improving Image Analysis Using a Contextual Approach
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, Th. Athanasiadis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C049.wiamis06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C49.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:aqlVkmm33-oC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=8568015211156817391" title="Citations @ Google Scholar">14</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/285e1c30a048b50a60f280941a69a0b43047d250" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/285e1c30a048b50a60f280941a69a0b43047d250#citing-papers" title="Citations @ Semantic Scholar">13</a>
						</div>

					</div>
					<div class="collapse" id="col-C49">
						<div class="pub-ref">
							In Proc. <em>7th International Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Seoul, Korea  <span class="bull"></span> Apr 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Generic algorithms for automatic object recognition and/or scene classification are unfortunately not producing reliable and robust results. A common approach to cope with this, still unresolved, issue is to restrict the problem at hand to a specific domain. In this paper we propose an algorithm to improve the results of image analysis, based on the contextual information we have, which relates the detected concepts to any given domain. Initial results produced by the image analysis module are domain-specific semantic concepts and are being re-adjusted appropriately by the suggested algorithm, by means of fine-tuning the degrees of confidence of each detected concept. The novelty of the presented work is twofold: i) the knowledge-assisted image analysis algorithm, that utilizes an ontology infrastructure to handle the knowledge and MPEG-7 visual descriptors for the region labeling and ii) the context-driven re-adjustment of the degrees of confidence of the detected labels.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C49,
   title = {Improving Image Analysis using a Contextual Approach},
   author = {Mylonas, Phivos and Athanasiadis, Thanos and Avrithis, Yannis},
   booktitle = {Proceedings of 7th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {4},
   address = {Seoul, Korea},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2005"></a>
					2005
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C48"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C48" id="tog-C48">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fuzzy Support Vector Machines for Image Classification Fusing MPEG-7 Visual Descriptors
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Stamou, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">EWIMT&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C048.ewimt05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C48.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:e5wmG9Sq2KIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13068800229288311970" title="Citations @ Google Scholar">15</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/97d164d2037644772bc2f353a4f6d0824507423e" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/97d164d2037644772bc2f353a4f6d0824507423e#citing-papers" title="Citations @ Semantic Scholar">18</a>
						</div>

					</div>
					<div class="collapse" id="col-C48">
						<div class="pub-ref">
							In Proc. <em>2nd European Workshop on the Integration of Knowledge, Semantic, and Digital Media Techniques</em><br>
							London, UK  <span class="bull"></span> Nov 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper proposes a new type of a support vector machine which uses a kernel constituted from fuzzy basis functions. The proposed network combines the characteristics both of a support vector machine and a fuzzy system: high generalization performance, even when the dimension of the input space is very high, structured and numerical representation of knowledge and ability to extract linguistic fuzzy rules, in order to bridge the "semantic gap" between the low-level descriptors and the high-level semantics of an image. The Fuzzy SVM network was evaluated using images from the aceMedia Repository and more specifically in a beach/urban scenes classification problem.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C48,
   title = {Fuzzy Support Vector Machines for Image Classification fusing {MPEG-7} Visual Descriptors},
   author = {Spyrou, Evaggelos and Stamou, Giorgos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 2nd European Workshop on the Integration of Knowledge, Semantic, and Digital Media Techniques (EWIMT)},
   month = {11},
   address = {London, UK},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C47"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C47" id="tog-C47">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using a Multimedia Ontology Infrastructure for Semantic Annotation of Multimedia Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, V. Tzouvaras, K. Petridis, F. Precioso, Y. Avrithis, Y. Kompatsiaris</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ceur-ws.org/Vol-185/semAnnot05-06.pdf" title="Electronic edition">SemAnnot/ISWC&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C047.semannot05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C47.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/semweb/AthanasiadisTPP05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:UeHWp8X0CEIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11563375653766181190" title="Citations @ Google Scholar">81</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/924ead47181df09ea75207b1c6f8f47c637898bc" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/924ead47181df09ea75207b1c6f8f47c637898bc#citing-papers" title="Citations @ Semantic Scholar">62</a>
						</div>

					</div>
					<div class="collapse" id="col-C47">
						<div class="pub-ref">
							In Proc. <em>5th International Workshop on Knowledge Markup and Semantic Annotation,</em><br>
							part of <em>4th International Semantic Web Conference</em><br>
							Galway, Ireland  <span class="bull"></span> Nov 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we discuss the use of knowledge for the automatic extraction of semantic metadata from multimedia content. For the representation of knowledge we extended and enriched current general-purpose ontologies to include low-level visual features. More specifically, we implemented a tool that links MPEG-7 visual descriptors to high-level, domain-specific concepts. For the exploitation of this knowledge infrastructure we developed an experimentation platform, that allows us to analyze multimedia content and automatically create the associated semantic metadata, as well as to test, validate and refine the ontologies built. We pursued a tight and functional integration of the knowledge base and the analysis modules putting them in a loop of constant interaction instead of being the one just a pre- or post-processing step of the other.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C47,
   title = {Using a Multimedia Ontology Infrastructure for Semantic Annotation of Multimedia Content},
   author = {Athanasiadis, Thanos and Tzouvaras, Vassilis and Petridis, Kosmas and Precioso, Frederic and Avrithis, Yannis and Kompatsiaris, Yiannis},
   publisher = {CEUR-WS},
   booktitle = {Proceedings of 5th International Workshop on Knowledge Markup and Semantic Annotation, (SemAnnot), part of 4th International Semantic Web Conference (ISWC)},
   month = {11},
   pages = {59--68},
   address = {Galway, Ireland},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C46"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C46" id="tog-C46">
							<i class="left-60 tog far fa-chevron-down"></i>
							Combined Domain Specific and Multimedia Ontologies for Image Understanding
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Petridis, F. Precioso, Th. Athanasiadis, Y. Avrithis, Y. Kompatsiaris</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">KI&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C046.ki05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C46.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:HDshCWvjkbEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3957374948167041229" title="Citations @ Google Scholar">12</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d821b1b08cea3f843e557d2bc9da6dd66cb53323" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d821b1b08cea3f843e557d2bc9da6dd66cb53323#citing-papers" title="Citations @ Semantic Scholar">7</a>
						</div>

					</div>
					<div class="collapse" id="col-C46">
						<div class="pub-ref">
							In Proc. <em>28th German Conference on Artificial Intelligence</em><br>
							Koblenz, Germany  <span class="bull"></span> Sep 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Knowledge representation and annotation of multimedia documents typically have been pursued in two different directions. Previous approaches have focused either on low level descriptors, such as dominant color, or on the content dimension and corresponding manual annotations, such as person or vehicle. In this paper, we present a knowledge infrastructure to bridge the gap between the two directions. Ontologies are being extended and enriched to include low-level audiovisual features and descriptors. Additionally, a tool for linking low-level MPEG-7 visual descriptions to ontologies and annotations has been developed. In this way, we construct ontologies that include prototypical instances of domain concepts together with a formal specification of the corresponding visual descriptors. Thus, we combine high-level domain concepts and low-level multimedia descriptions, enabling for new media content analysis.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C46,
   title = {Combined Domain Specific and Multimedia Ontologies for Image Understanding},
   author = {Petridis, Kosmas and Precioso, Frederic and Athanasiadis, Thanos and Avrithis, Yannis and Kompatsiaris, Yiannis},
   booktitle = {Proceedings of 28th German Conference on Artificial Intelligence (KI)},
   month = {9},
   address = {Koblenz, Germany},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C45"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C45" id="tog-C45">
							<i class="left-60 tog far fa-chevron-down"></i>
							Self-Tuning Personalized Information Retrieval in an Ontology-Based Framework
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Castells, M. Fernndez, D. Vallet, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/11575863_119" title="DOI">SWWS&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C045.swws05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C45.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/otm/CastellsFVMA05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:1qzjygNMrQYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15439214368097800078,17175524455443048559" title="Citations @ Google Scholar">99</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/9dc975eb6dd3739f6fb02fcb67df997583e408ea" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/9dc975eb6dd3739f6fb02fcb67df997583e408ea#citing-papers" title="Citations @ Semantic Scholar">79</a>
						</div>

					</div>
					<div class="collapse" id="col-C45">
						<div class="pub-ref">
							In Proc. <em>First IFIP WG 2.12 & WG 12.4 International Workshop on Web Semantics</em><br>
							Agia Napa, Cyprus  <span class="bull"></span> Nov 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Reliability is a well-known concern in the field of personalization technologies. We propose the extension of an ontology-based retrieval system with semantic-based personalization techniques, upon which automatic mechanisms are devised that dynamically gauge the degree of personalization, so as to benefit from adaptivity but yet reduce the risk of obtrusiveness and loss of user control. On the basis of a common domain ontology KB, the personalization framework represents, captures and exploits user preferences to bias search results towards personal user interests. Upon this, the intensity of personalization is automatically increased or decreased according to an assessment of the imprecision contained in user requests and system responses before personalization is applied.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C45,
   title = {Self-Tuning Personalized Information Retrieval in an Ontology-Based Framework},
   author = {Castells, Pablo and Fern\'andez, Miriam and Vallet, David and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of First IFIP WG 2.12 \& WG 12.4 International Workshop on Web Semantics (SWWS)},
   month = {11},
   address = {Agia Napa, Cyprus},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C44"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C44" id="tog-C44">
							<i class="left-60 tog far fa-chevron-down"></i>
							On the Use of Spatiotemporal Visual Attention for Video Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">VLBV&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C044.vlbv05a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C44.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:-f6ydRqryjwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10196747548737310493" title="Citations @ Google Scholar">13</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/12c3b6745e29a72d18e394b5874c5242867408bb" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/12c3b6745e29a72d18e394b5874c5242867408bb#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C44">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Very Low Bitrate Video Coding</em><br>
							Sardinia, Italy  <span class="bull"></span> Sep 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									It is common sense among experts that visual attention plays an important role in perception, being necessary for obtaining salient information about the surroundings. It may be the "glue" that binds simple visual features into an object [1]. Having proposed a spatiotemporal model for visual attention in the past, we elaborate on this work and use it for video classification. Our claim is that simple visual features bound to spatiotemporal salient regions will better represent the video content. Hence, we expect that feature vectors extracted from these regions will enhance the performance of the classifier. We present statistics on sports sequences of five different categories that verify our claims.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C44,
   title = {On the use of spatiotemporal visual attention for video classification},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of International Workshop on Very Low Bitrate Video Coding (VLBV)},
   month = {9},
   address = {Sardinia, Italy},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C43"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C43" id="tog-C43">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Ontology Infrastructure for Multimedia Reasoning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Simou, C. Saathoff, S. Dasiopoulou, E. Spyrou, N. Voisine, V. Tzouvaras, I. Kompatsiaris, Y. Avrithis, S. Staab</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/11738695_8" title="DOI">VLBV&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C043.vlbv05b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C43.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/vlbv/SimouSDSVTKAS05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:WqliGbK-hY8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14948305626482786147" title="Citations @ Google Scholar">41</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/3184f86f0e5fd8701954da81fc9d30b8a14c0d84" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/3184f86f0e5fd8701954da81fc9d30b8a14c0d84#citing-papers" title="Citations @ Semantic Scholar">26</a>
						</div>

					</div>
					<div class="collapse" id="col-C43">
						<div class="pub-ref">
							In Proc. <em>International Workshop Very Low Bitrate Video Coding</em><br>
							Sardinia, Italy  <span class="bull"></span> Sep 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an ontology infrastucture for multimedia reasoning is presented, making it possible to combine low-level visual descriptors with domain specific knowledge and subsequently analyze multimedia content with a generic algorithm that makes use of this knowledge. More specifically, the ontology infrastructure consists of a domain-specific ontology, a visual descriptor ontology (VDO) and an upper ontology. In order to interpret a scene, a set of atom regions is generated by an initial segmentation and their descriptors are extracted. Considering all descriptors in association with the related prototype instances and relations, a genetic algorithm labels the atom regions. Finally, a constraint reasoning engine enables the final region merging and labelling into meaningful objects.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C43,
   title = {An Ontology Infrastructure for Multimedia Reasoning},
   author = {Simou, Nikolaos and Saathoff, Carsten and Dasiopoulou, Stamatia and Spyrou, Evaggelos and Voisine, Nicolas and Tzouvaras, Vassilis and Kompatsiaris, Ioannis and Avrithis, Yannis and Staab, Steffen},
   booktitle = {Proceedings of International Workshop Very Low Bitrate Video Coding (VLBV)},
   month = {9},
   address = {Sardinia, Italy},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C42"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C42" id="tog-C42">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Semantically-Enhanced Personalization Framework for Knowledge-Driven Media Services 
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Vallet, Ph. Mylonas, M.A. Corella, J.M. Fuentes, P. Castells, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">ICWI&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C042.icwi05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C42.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:r_AWSJRzSzQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1234865969125757117,9006256687547490933" title="Citations @ Google Scholar">28</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/222deca412eeae03f00f8f744d35b1b81cba56e6" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/222deca412eeae03f00f8f744d35b1b81cba56e6#citing-papers" title="Citations @ Semantic Scholar">21</a>
						</div>

					</div>
					<div class="collapse" id="col-C42">
						<div class="pub-ref">
							In Proc. <em>IADIS International Conference on WWW / Internet</em><br>
							Lisbon, Portugal  <span class="bull"></span> Oct 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper describes a comprehensive framework giving support to a wide range of personalization facilities in a multi-media content management environment. The framework builds upon a rich, ontology-based representation of the domain of discourse, whereby content semantics are linked to a rich representation of user preferences. The expressive power of ontologies is used to develop automatic learning capabilities, in order to update user profiles as users interact with the system. The resulting descriptions of user interests in terms of ontologies are exploited, along with available content metadata, to provide users with personalized content search, browsing, ranking, and retrieval. On a wider perspective, the framework is built as an open platform that provides for further user and device adaptive capability extensions.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C42,
   title = {A Semantically-Enhanced Personalization Framework for Knowledge-Driven Media Services },
   author = {Vallet, David and Mylonas, Phivos and Corella, Miguel A. and Fuentes, Jos\'e M. and Castells, Pablo and Avrithis, Yannis},
   booktitle = {Proceedings of IADIS International Conference on WWW / Internet (ICWI)},
   month = {10},
   address = {Lisbon, Portugal},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C41"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C41" id="tog-C41">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fusing MPEG-7 Visual Descriptors for Image Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, H. Le Borgne, Th. Mailis, E. Cooke, Y. Avrithis, N. O'Connor</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/11550907_134" title="DOI">ICANN&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C041.icann05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C41.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icann/SpyrouBMCAO05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:9yKSN-GCB0IC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13239548278585919522" title="Citations @ Google Scholar">99</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/dec95b0b83e54ffa88205420438c2f6e4fbb00a7" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/dec95b0b83e54ffa88205420438c2f6e4fbb00a7#citing-papers" title="Citations @ Semantic Scholar">73</a>
						</div>

					</div>
					<div class="collapse" id="col-C41">
						<div class="pub-ref">
							In Proc. <em>International Conference on Artificial Neural Networks</em><br>
							Warsaw, Poland  <span class="bull"></span> Sep 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper proposes a number of content-based image classification techniques based on fusing various low-level MPEG-7 visual descriptors. The goal is to fuse several descriptors in order to improve the performance of several machine-learning classifiers. Fusion is necessary as descriptors would be otherwise incompatible and inappropriate to directly include e.g. in a Euclidean distance. Three approaches are described: A merging fusion combined with an SVM classifier, a back-propagation fusion combined with a K-Nearest Neighbor classifier and a Fuzzy-ART neurofuzzy network. In the latter case, fuzzy rules can be extracted in an effort to bridge the semantic gap between the low-level descriptors and the high-level semantics of an image. All networks were evaluated using content from the aceMedia Repository and more specifically in a beach/urban scenes classification problem.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C41,
   title = {Fusing {MPEG-7} visual descriptors for image classification},
   author = {Spyrou, Evaggelos and Le Borgne, Herv\'e and Mailis, Theofilos and Cooke, Eddie and Avrithis, Yannis and O'Connor, Noel},
   booktitle = {Proceedings of International Conference on Artificial Neural Networks (ICANN)},
   month = {9},
   address = {Warsaw, Poland},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C40"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C40" id="tog-C40">
							<i class="left-60 tog far fa-chevron-down"></i>
							Context Modelling for Multimedia Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CONTEXT&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C040.context05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C40.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:5nxA0vEk-isC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1269285214440993348" title="Citations @ Google Scholar">29</a>
						</div>

					</div>
					<div class="collapse" id="col-C40">
						<div class="pub-ref">
							In Proc. <em>5th International and Interdisciplinary Conference on Modeling and Using Context</em><br>
							Paris, France  <span class="bull"></span> Jul 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Context is of great importance in a wide range of computing applications and has become a major topic in multimedia content search and retrieval systems. In this paper we focus our research efforts on visual context, a part of context suitable for multimedia analysis and usage. We introduce our efforts towards the scope of clarifying context in the fields of object detection and scene classification during multimedia analysis. We also present a method for visual context modelling, based on spatial object and region-based relations, to use in content-based multimedia search and retrieval systems.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C40,
   title = {Context modelling for multimedia analysis},
   author = {Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 5th International and Interdisciplinary Conference on Modeling and Using Context (CONTEXT)},
   month = {7},
   address = {Paris, France},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C39"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C39" id="tog-C39">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Enhanced Spatiotemporal Visual Attention Model for Sports Video Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CBMI&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C039.cbmi05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C39.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Wp0gIr-vW9MC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9024218050234293939" title="Citations @ Google Scholar">15</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/676913f4d13d3545271095c3dd401c71738806f8" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/676913f4d13d3545271095c3dd401c71738806f8#citing-papers" title="Citations @ Semantic Scholar">12</a>
						</div>

					</div>
					<div class="collapse" id="col-C39">
						<div class="pub-ref">
							In Proc. <em>4th International Workshop on Content-Based Multimedia Indexing</em><br>
							Riga, Latvia  <span class="bull"></span> Jun 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Inspired by the human visual system, visual attention (VA) models seem to provide solutions to problems of semantic image understanding by selecting only a small but representative fraction of visual input to process. Having proposed a spatiotemporal VA model for video processing in the past, we propose considerable enhancements in this paper, including the use of steerable filters for 3D orientation estimation, and of PCA for fusion of features for the construction of saliency volumes. We further employ segmentation and feature extraction on salient regions to provide video classification using an SVM classifier. Finally, we provide results on sports video classification and comment on the usefulness of spatiotemporal VA for such purposes.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C39,
   title = {An enhanced spatiotemporal visual attention model for sports video analysis},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis},
   booktitle = {Proceedings of 4th International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {6},
   address = {Riga, Latvia},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C38"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C38" id="tog-C38">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Annotation of Images and Videos for Multimedia Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Bloehdorn, K. Petridis, C. Saathoff, N. Simou, V. Tzouvaras, Y. Avrithis, S. Handschuh, Y. Kompatsiaris, S. Staab, M.G. Strintzis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/11431053_40" title="DOI">ESWC&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C038.eswc05a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C38.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/esws/BloehdornPSSTAHKSS05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:u5HHmVD_uO8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=6022421964289866238" title="Citations @ Google Scholar">294</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/ce1ad32207a8b7b368968670a0ad4b792101b1ec" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/ce1ad32207a8b7b368968670a0ad4b792101b1ec#citing-papers" title="Citations @ Semantic Scholar">241</a>
						</div>

					</div>
					<div class="collapse" id="col-C38">
						<div class="pub-ref">
							In Proc. <em>2nd European Semantic Web Conference</em><br>
							Heraklion, Greece  <span class="bull"></span> May 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Annotations of multimedia documents typically have been pursued in two different directions. Either previous approaches have focused on low level descriptors, such as dominant color, or they have focused on the content dimension and corresponding annotations, such as person or vehicle. In this paper, we present a software environment to bridge between the two directions. M-OntoMat-Annotizer allows for linking low level MPEG-7 visual descriptions to conventional Semantic Web ontologies and annotations. We use M-OntoMat-Annotizer in order to construct ontologies that include prototypical instances of high-level domain concepts together with a formal specification of corresponding visual descriptors. Thus, we formalize the interrelationship of high- and low-level multimedia concept descriptions allowing for new kinds of multimedia content analysis and reasoning.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C38,
   title = {Semantic Annotation of Images and Videos for Multimedia Analysis},
   author = {Bloehdorn, Stephan and Petridis, Kosmas and Saathoff, Carsten and Simou, Nikolaos and Tzouvaras, Vassilis and Avrithis, Yannis and Handschuh, Siegfried and Kompatsiaris, Yiannis and Staab, Steffen and Strintzis, Michael G.},
   booktitle = {Proceedings of 2nd European Semantic Web Conference (ESWC)},
   month = {5},
   address = {Heraklion, Greece},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C37"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C37" id="tog-C37">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Visual Descriptor Ontology for Multimedia Reasoning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Simou, V. Tzouvaras, Y. Avrithis, G. Stamou, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C037.wiamis05b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C37.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:eQOLeE2rZwMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=108773721205153834" title="Citations @ Google Scholar">63</a>
						</div>

					</div>
					<div class="collapse" id="col-C37">
						<div class="pub-ref">
							In Proc. <em>6th International Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Montreux, Switzerland  <span class="bull"></span> Apr 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we present the construction of an ontology that represents the structure of the MPEG-7 visual part. The goal of this ontology is to enable machines to generate and understand visual descriptions which can be used for multimedia reasoning. Within the specification, MPEG-7 definitions (description schemes and descriptors) are expressed in XML Schema. Although XML Schema provides the syntactic, structural, cardinality and datatyping constraints required by MPEG-7, it does not provide the semantic interoperability required to make MPEG-7 visual descriptors accessible by other domains. The knowledge representation provided by the ontology can be used to develop tools which perform knowledge-based reasoning. For the construction of the ontology we use the RDFS ontology language. We present the problems that occurred, mainly, due to the RDFS modelling limitations. Finally, we propose a way to apply reasoning using the VD ontology.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C37,
   title = {A Visual Descriptor Ontology for Multimedia Reasoning},
   author = {Simou, Nikolaos and Tzouvaras, Vassilis and Avrithis, Yannis and Stamou, Giorgos and Kollias, Stefanos},
   booktitle = {Proceedings of 6th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {4},
   address = {Montreux, Switzerland},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C36"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C36" id="tog-C36">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge-Assisted Video Analysis Using a Genetic Algorithm
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Voisine, S. Dasiopoulou, V. Mezaris, E. Spyrou, Th. Athanasiadis, I. Kompatsiaris, Y. Avrithis, M.G. Strintzis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;2005</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C036.wiamis05a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C36.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:hqOjcs7Dif8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3566692977224693111" title="Citations @ Google Scholar">26</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5013e25c5074225161db5b577517e62ad9b7efbc" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/5013e25c5074225161db5b577517e62ad9b7efbc#citing-papers" title="Citations @ Semantic Scholar">20</a>
						</div>

					</div>
					<div class="collapse" id="col-C36">
						<div class="pub-ref">
							In Proc. <em>6th International Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Montreux, Switzerland  <span class="bull"></span> Apr 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Efficient video content management and exploitation requires extraction of the underlying semantics, which is a non-trivial task involving the association of low-level features with high-level concepts. In this paper, a knowledge-assisted approach for extracting semantic information of domain-specific video content is presented. Domain knowledge considers both low-level visual features (color, motion, shape) and spatial information (topological and directional relations). An initial segmentation algorithm generates a set of over-segmented atom-regions and a neural network is used to estimate the similarity distance between the extracted atom-region descriptors and the ones of the object models included in the domain ontology. A genetic algorithm is applied then in order to find the optimal interpretation according to the domain conceptualization. The proposed approach was tested on the Tennis and Formula One domains with promising results.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C36,
   title = {Knowledge-Assisted Video Analysis Using A Genetic Algorithm},
   author = {Voisine, Nicolas and Dasiopoulou, Stamatia and Mezaris, Vasileios and Spyrou, Evaggelos and Athanasiadis, Thanos and Kompatsiaris, Ioannis and Avrithis, Yannis and Strintzis, Michael G.},
   booktitle = {Proceedings of 6th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {4},
   address = {Montreux, Switzerland},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C35"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C35" id="tog-C35">
							<i class="left-60 tog far fa-chevron-down"></i>
							Handling Uncertainty in Video Analysis with Spatiotemporal Visual Attention
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/fuzzy.2005.1452395" title="DOI">FUZZ-IEEE&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C035.fuzz-ieee05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C35.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/fuzzIEEE/RapantzikosAK05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:R3hNpaxXUhUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9499063641168671838" title="Citations @ Google Scholar">13</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/9db410172f5d8dcff81c3a2f9691bb6f17f1c0dc" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/9db410172f5d8dcff81c3a2f9691bb6f17f1c0dc#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C35">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Fuzzy Systems</em><br>
							Reno, Nevada  <span class="bull"></span> May 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In natural vision, we center our fixation on the most informative points in a scene in order to reduce our overall uncertainty about the scene and help interpret it. Even if we are looking for a specific stimulus around us, we face a great amount of uncertainty since that stimulus could be in any spatial location. Visual attention (VA) schemes have been proposed by researchers to account for the ability of the human eye to quickly fixate on informative regions. Recently, VA in images, and especially saliency-based VA, became an active research topic of the computer vision community. The proposed work provides an extension towards VA in video sequences by integrating spatiotemporal information. The potential applications include video classification, scene understanding, surveillance and segmentation.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C35,
   title = {Handling Uncertainty in Video Analysis With Spatiotemporal Visual Attention},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
   month = {5},
   pages = {213--217},
   address = {Reno, Nevada},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2004"></a>
					2004
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C34"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C34" id="tog-C34">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Mediator System for Hetero-Lingual Audiovisual Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Th. Athanasiadis, Y. Avrithis, G. Stamou, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">MPEP&nbsp;2004</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C034.mpep04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C34.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:SeFeTyx0c_EC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14019674532282177471" title="Citations @ Google Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C34">
						<div class="pub-ref">
							In Proc. <em>International Conference on Multi-platform e-Publishing</em><br>
							Athens, Greece  <span class="bull"></span> Nov 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of hetero-lingual digital audiovisual (a/v) archives. This innovative system exploits the advances in handling a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" and "personalized response". The proposed system is targeting the intelligent extraction of semantic information from a/v and text related data taking into account the nature of the queries that users my issue, and the context determined by user profiles.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C34,
   title = {A mediator system for hetero-lingual audiovisual content},
   author = {Wallace, Manolis and Athanasiadis, Thanos and Avrithis, Yannis and Stamou, Giorgos and Kollias, Stefanos},
   booktitle = {Proceedings of International Conference on Multi-platform e-Publishing (MPEP)},
   month = {11},
   address = {Athens, Greece},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C33"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C33" id="tog-C33">
							<i class="left-60 tog far fa-chevron-down"></i>
							Achieving Integration of Knowledge and Content Technologies: the aceMedia Project
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Hobson, T. May, J. Tromp, Y. Kompatsiaris, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">EWIMT&nbsp;2004</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C033.ewimt04b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C33.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/ewimt/KompatsiarisAHMT04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:blknAaTinKkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=4792450819747353116" title="Citations @ Google Scholar">5</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/e31026bbd8e209d002735bd200e32136023ffe67" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/e31026bbd8e209d002735bd200e32136023ffe67#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C33">
						<div class="pub-ref">
							In Proc. <em>European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology</em><br>
							London, U.K.  <span class="bull"></span> Nov 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Integration of knowledge and multimedia content technologies is important for the future of European industry and commerce. aceMedia is an IST FP6 project which aims to unite these two established disciplines to achieve significant advances by the combination of the two domains. This paper describes research in content processing and knowledge assisted multimedia analysis within the aceMedia project, and provides a scenario of use which illustrates the benefits of this combined approach.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C33,
   title = {Achieving Integration of Knowledge and Content Technologies: the {aceMedia} Project},
   author = {Hobson, Paola and May, Tony and Tromp, Jolanda and Kompatsiaris, Yiannis and Avrithis, Yannis},
   booktitle = {Proceedings of European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology (EWIMT)},
   month = {11},
   address = {London, U.K.},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C32"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C32" id="tog-C32">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge Representation for Semantic Multimedia Content Analysis and Reasoning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Petridis, I. Kompatsiaris, M.G. Strintzis, S. Bloehdorn, S. Handschuh, S. Staab, N. Simou, V. Tzouvaras, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">EWIMT&nbsp;2004</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C032.ewimt04a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C32.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/ewimt/PetridisKSBHSSTA04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:YsMSGLbcyi4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13978746502675603022,15013653208905772189,14949729076076153285" title="Citations @ Google Scholar">95</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/53e1cbbaa8fe5641b8e594697d6a400e6e042af5" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/53e1cbbaa8fe5641b8e594697d6a400e6e042af5#citing-papers" title="Citations @ Semantic Scholar">82</a>
						</div>

					</div>
					<div class="collapse" id="col-C32">
						<div class="pub-ref">
							In Proc. <em>European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology</em><br>
							London, U.K.  <span class="bull"></span> Nov 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, a knowledge representation infrastructure for semantic multimedia content analysis and reasoning is presented. This is one of the major objectives of the aceMedia Integrated Project where ontologies are being extended and enriched to include low-level audiovisual features, descriptors and behavioural models in order to support automatic content annotation. More specifically, the developed infrastructure consists of the core ontology based on extensions of the DOLCE core ontology and the multimedia-specific infrastructure components. These are, the Visual Descriptors Ontology, which is based on an RDFS representation of the MPEG-7 Visual Descriptors and the Multimedia Structure Ontology, based on the MPEG-7 MDS. Furthermore, the developed Visual Descriptor Extraction tool is presented, which will support the initialization of domain ontologies with multimedia features.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C32,
   title = {Knowledge Representation for Semantic Multimedia Content Analysis and Reasoning},
   author = {Petridis, Kosmas and Kompatsiaris, Ioannis and Strintzis, Michael G. and Bloehdorn, Stephan and Handschuh, Siegfried and Staab, Steffen and Simou, Nikolaos and Tzouvaras, Vassilis and Avrithis, Yannis},
   booktitle = {Proceedings of European Workshop on the Integration of Knowledge, Semantics and Digital Media Technology (EWIMT)},
   month = {11},
   address = {London, U.K.},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C31"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C31" id="tog-C31">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal Visual Attention Architecture for Video Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, N. Tsapatsoulis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/mmsp.2004.1436423" title="DOI">MMSP&nbsp;2004</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C031.mmsp04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C31.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmsp/RapantzikosTA04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:QIV2ME_5wuYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9322107401447909520" title="Citations @ Google Scholar">18</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/55af59de254750fcf82b6ac6c6a1825a6217061e" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/55af59de254750fcf82b6ac6c6a1825a6217061e#citing-papers" title="Citations @ Semantic Scholar">19</a>
						</div>

					</div>
					<div class="collapse" id="col-C31">
						<div class="pub-ref">
							In Proc. <em>IEEE International Workshop On Multimedia Signal Processing</em><br>
							Siena, Italy  <span class="bull"></span> Sep 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Several visual attention (VA) schemes have been proposed with the saliency-based ones being the most popular. The proposed work provides an extension towards VA video sequences by treating it as volumetric data. The architecture is presented in detail and potential applications are investigated. We expect that the extended VA scheme will reveal interesting events across the sequence like occlusions and short occurrences of objects, providing a basis for video surveillance (e.g. intruder detection) and summarization applications.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C31,
   title = {Spatiotemporal Visual Attention Architecture for Video Analysis},
   author = {Rapantzikos, Konstantinos and Tsapatsoulis, Nicolas and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE International Workshop On Multimedia Signal Processing (MMSP)},
   month = {9},
   pages = {83--86},
   address = {Siena, Italy},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C30"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C30" id="tog-C30">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fuzzy Relational Knowledge Representation and Context in the Service of Semantic Information Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/fuzzy.2004.1375376" title="DOI">FUZZ-IEEE&nbsp;2004</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C030.fuzz-ieee04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C30.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/fuzzIEEE/WallaceA04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:4TOpqqG69KYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1739764774597300772" title="Citations @ Google Scholar">19</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/b1f53f7c3dd942414cf317e93990f68d4ed9d200" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/b1f53f7c3dd942414cf317e93990f68d4ed9d200#citing-papers" title="Citations @ Semantic Scholar">15</a>
						</div>

					</div>
					<div class="collapse" id="col-C30">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Fuzzy Systems</em><br>
							Budapest, Hungary  <span class="bull"></span> Jul 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we follow a fuzzy relational approach to knowledge representation. With the use of semantic fuzzy relations we define and extract the semantic context out of a set of semantic entities. Based on this, we then proceed to the case of information retrieval and explain how the three participating contexts, namely the context of the query, the context of the document and the context of the user, can be estimated and utilized towards the achievement of more intuitive information services.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C30,
   title = {Fuzzy Relational Knowledge Representation and Context in the Service of Semantic Information Retrieval},
   author = {Wallace, Manolis and Avrithis, Yannis},
   booktitle = {Proceedings of IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)},
   month = {7},
   address = {Budapest, Hungary},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C29"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C29" id="tog-C29">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adding Semantics to Audiovisual Content: the FAETHON Project
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-3-540-27814-6_77" title="DOI">CIVR&nbsp;2004</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C029.civr04b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C29.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/civr/AthanasiadisA04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:bEWYMUwI8FkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11038775430145166884" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/feb0876774c3f7c8f8a0c7958a0e58dbf2b34b61" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/feb0876774c3f7c8f8a0c7958a0e58dbf2b34b61#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C29">
						<div class="pub-ref">
							In Proc. <em>3rd International Conference for Image and Video Retrieval</em><br>
							Dublin, Ireland  <span class="bull"></span> Jul 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents FAETHON, a distributed information system that offers enhanced search and retrieval capabilities to users interacting with digital audiovisual (a/v) archives. Its novelty primarily originates in the unified intelligent access to heterogeneous a/v content. The paper emphasizes on the features that provide enhanced search and retrieval capabilities to users, as well as intelligent management of the a/v content by content creators / distributors. It describes the system's main components, the intelligent metadata creation package, the a/v search engine & portal, and the MPEG-7 compliant a/v archive interfaces. Finally, it provides ideas on the positioning of FAETHON in the market of a/v archives and video indexing and retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C29,
   title = {Adding Semantics to Audiovisual Content: The {FAETHON} Project},
   author = {Athanasiadis, Thanos and Avrithis, Yannis},
   booktitle = {Proceedings of 3rd International Conference for Image and Video Retrieval (CIVR)},
   month = {7},
   pages = {665--673},
   address = {Dublin, Ireland},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C28"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C28" id="tog-C28">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge Assisted Analysis and Categorization for Semantic Video Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Th. Athanasiadis, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-3-540-27814-6_65" title="DOI">CIVR&nbsp;2004</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C028.civr04a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C28.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/civr/WallaceAA04" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:5Ul4iDaHHb8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13318516418225914597" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/24f2d75c9b8261217cae2a57021ce283c972f69e" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/24f2d75c9b8261217cae2a57021ce283c972f69e#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-C28">
						<div class="pub-ref">
							In Proc. <em>3rd International Conference for Image and Video Retrieval</em><br>
							Dublin, Ireland  <span class="bull"></span> Jul 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we discuss the use of knowledge for the analysis and semantic retrieval of video. We follow a fuzzy relational approach to knowledge representation, based on which we define and extract the context of either a multimedia document or a user query. During indexing, the context of the document is utilized for the detection of objects and for automatic thematic categorization. During retrieval, the context of the query is used to clarify the exact meaning of the query terms and to meaningfully guide the process of query expansion and index matching. Indexing and retrieval tools have been implemented to demonstrate the proposed techniques and results are presented using video from audiovisual archives.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C28,
   title = {Knowledge Assisted Analysis and Categorization for Semantic Video Retrieval},
   author = {Wallace, Manolis and Athanasiadis, Thanos and Avrithis, Yannis},
   booktitle = {Proceedings of 3rd International Conference for Image and Video Retrieval (CIVR)},
   month = {7},
   pages = {555--563},
   address = {Dublin, Ireland},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C27" id="tog-C27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Integrating Knowledge, Semantics and Content for User-Centred Intelligent Media Services: the aceMedia Project
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">I. Kompatsiaris, Y. Avrithis, P. Hobson, M.G. Strintzis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;2004</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C027.wiamis04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C27.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:WF5omc3nYNoC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13709081856647936111" title="Citations @ Google Scholar">51</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/53ce27064c33d2a5024341557feb1713d2f5b50d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/53ce27064c33d2a5024341557feb1713d2f5b50d#citing-papers" title="Citations @ Semantic Scholar">34</a>
						</div>

					</div>
					<div class="collapse" id="col-C27">
						<div class="pub-ref">
							In Proc. <em>Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Lisboa, Portugal  <span class="bull"></span> Apr 2004
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an approach for knowledge and context-assisted content analysis and reasoning based on a multimedia ontology infrastructure is presented. This is one of the major objectives of the aceMedia Integrated Project. In aceMedia, ontologies will be extended and enriched to include lowlevel audiovisual features, descriptors and behavioural models in order to support automatic content annotation. This approach is part of an integrated framework consisting of: user-oriented design, knowledge-driven content processing and distributed system architecture. The overall objective of aceMedia is the implementation of a novel concept for unified media representation: the Autonomous Content Entity (ACE), which has three layers: content, its associated metadata, and an intelligence layer. The ACE concept will be verified by two user focused application prototypes, enabled for both home network and mobile communication environments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C27,
   title = {Integrating Knowledge, Semantics and Content for User-Centred Intelligent Media Services: the {aceMedia} Project},
   author = {Kompatsiaris, Ioannis and Avrithis, Yannis and Hobson, Paola and Strintzis, Michael G.},
   booktitle = {Proceedings of Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {4},
   address = {Lisboa, Portugal},
   year = {2004}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2003"></a>
					2003
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C26"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C26" id="tog-C26">
							<i class="left-60 tog far fa-chevron-down"></i>
							Intelligent Visual Descriptor Extraction from Video Sequences
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">P. Tzouveli, G. Andreou, G. Tsechpenakis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-3-540-25981-7_9" title="DOI">AMR&nbsp;2003</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C026.amr03.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C26.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/amr/TzouveliATAK03" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:9vf0nzSNQJEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10556193743090924045" title="Citations @ Google Scholar">13</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/37ebbb7f65613d26ef16650d3c114316e4aef179" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/37ebbb7f65613d26ef16650d3c114316e4aef179#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C26">
						<div class="pub-ref">
							In Proc. <em>1st International Workshop on Adaptive Multimedia Retrieval</em><br>
							Hamburg, Germany  <span class="bull"></span> Sep 2003
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Extraction of visual descriptor is a crucial problem for state-of-the-art visual information analysis. In this paper, we present a knowledge-based approach for detection of visual objects in video sequences. The propose approach models objects through their visual descriptors defined in MPEG7. It first extracts moving regions using an efficient active contours technique. It then computes visual descriptions of the moving regions including color features, shape features which are invariant to affine transformations, as well as motion features. The extracted features are matched to a-priori knowledge about the objects' descriptions,using appropriately defined matching functions. Results are presented which illustrate the theoretical developments
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C26,
   title = {Intelligent Visual Descriptor Extraction from Video Sequences},
   author = {Tzouveli, Paraskevi and Andreou, Georgios and Tsechpenakis, Gabriel and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 1st International Workshop on Adaptive Multimedia Retrieval (AMR)},
   month = {9},
   address = {Hamburg, Germany},
   year = {2003}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C25"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C25" id="tog-C25">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Context and Fuzzy Relations to Interpret Multimedia Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, G. Akrivas, Ph. Mylonas, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CBMI&nbsp;2003</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C025.cbmi03.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C25.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:8k81kl-MbHgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=6673359448650489335" title="Citations @ Google Scholar">24</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/70d81e0da7e81b9cb4ff69bbdbec0b41b135546f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/70d81e0da7e81b9cb4ff69bbdbec0b41b135546f#citing-papers" title="Citations @ Semantic Scholar">23</a>
						</div>

					</div>
					<div class="collapse" id="col-C25">
						<div class="pub-ref">
							In Proc. <em>3rd International Workshop on Content-Based Multimedia Indexing</em><br>
							Rennes, France  <span class="bull"></span> Sep 2003
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Object detection techniques are coming closer to the automatic detection and identification of objects in multimedia documents. Still, this is not sufficient for the understanding of multimedia content, mainly because a simple object may be related to multiple topics, few of which are indeed related to a given document. In this paper we determine the thematic categories that are related to a document based on the objects that have been automatically detected in it. Our approach relies on stored knowledge and a fuzzy hierarchical clustering algorithm; this algorithm uses a similarity measure that is based on the notion of context. The context is extracted using fuzzy ontological relations.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C25,
   title = {Using context and fuzzy relations to interpret multimedia content},
   author = {Wallace, Manolis and Akrivas, Giorgos and Mylonas, Phivos and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 3rd International Workshop on Content-Based Multimedia Indexing (CBMI)},
   month = {9},
   address = {Rennes, France},
   year = {2003}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C24"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C24" id="tog-C24">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unified Access to Heterogeneous Audiovisual Archives
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Stamou, M. Wallace, F. Marques, Ph. Salembier, X. Giro, W. Haas, H. Vallant, M. Zufferey</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">IKNOW&nbsp;2003</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C024.iknow03.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C24.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:4DMP91E08xMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9022365166745380021" title="Citations @ Google Scholar">11</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/56db283d974e087246b0707ffa145f6397818eb5" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/56db283d974e087246b0707ffa145f6397818eb5#citing-papers" title="Citations @ Semantic Scholar">8</a>
						</div>

					</div>
					<div class="collapse" id="col-C24">
						<div class="pub-ref">
							In Proc. <em>3rd International Conference on Knowledge Management</em><br>
							Graz, Austria  <span class="bull"></span> Jul 2003
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual (a/v) archives. This innovative system exploits the advances in handlings a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" and "personalized response". The proposed system is targeting the intelligent extraction of semantic information from a/v and text related data taking into account the nature of the queries that users my issue, and the context determined by user profiles. It also provides a personalisation process of the response in order to provide end-users with desired information. From a technical point of view, the FAETHON system plays the role of an intermediate access server residing between the end users and multiple heterogeneous audiovisual archives organized according to the new MPEG standards.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C24,
   title = {Unified Access to Heterogeneous Audiovisual Archives},
   author = {Avrithis, Yannis and Stamou, Giorgos and Wallace, Manolis and Marques, Ferran and Salembier, Philippe and Giro, Xavier and Haas, Werner and Vallant, Heribert and Zufferey, Michael},
   booktitle = {Proceedings of 3rd International Conference on Knowledge Management (IKNOW)},
   month = {7},
   address = {Graz, Austria},
   year = {2003}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C23"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C23" id="tog-C23">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Unification of Heterogenous Multimedia Archives
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Stamou, Y. Avrithis, S. Kollias, F. Marques, Ph. Salembier</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;2003</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C023.wiamis03.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C23.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:TFP_iSt0sucC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18277010694387662501" title="Citations @ Google Scholar">5</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/e91e0d7997cd98808bc9a3230f19718685a110c0" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/e91e0d7997cd98808bc9a3230f19718685a110c0#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-C23">
						<div class="pub-ref">
							In Proc. <em>4th European Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							London, UK  <span class="bull"></span> Apr 2003
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Multimedia Content is described via textual, semantic and structural Descriptors and Description Schemes, as introduced in MPEG-7. The semantic part of the description is closer to what the user expects from a multimedia search engine, however it poses difficulties, because of the potential incompatibility of the semantic entities among different archives. In this paper, we present FAETHON, a system that unifies the semantic description of heterogeneous archives, through the use of a semantic encyclopaedia.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C23,
   title = {Semantic Unification of Heterogenous Multimedia Archives},
   author = {Stamou, Giorgos and Avrithis, Yannis and Kollias, Stefanos and Marques, Ferran and Salembier, Philippe},
   booktitle = {Proceedings of 4th European Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {4},
   address = {London, UK},
   year = {2003}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2002"></a>
					2002
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C22"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C22" id="tog-C22">
							<i class="left-60 tog far fa-chevron-down"></i>
							Intelligent Semantic Access to Audiovisual Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Stamou, A. Delopoulos, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/3-540-46014-4_20" title="DOI">SETN&nbsp;2002</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C022.setn02.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C22.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/setn/AvrithisSDK02" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:eq2jaN3J8jMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15046130410039240038" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/babc4d848cea1f6c68cbb7e53af003cd4b879ed1" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/babc4d848cea1f6c68cbb7e53af003cd4b879ed1#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-C22">
						<div class="pub-ref">
							In Proc. <em>2nd Hellenic Conference on Artificial Intelligence</em><br>
							Thessaloniki, Greece  <span class="bull"></span> Apr 2002
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual (a/v) archives. This novel system exploits the advances in handling a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced access services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" and "personalized response". The proposed system is targeting the intelligent extraction of semantic information from a/v and text related data taking into account the nature of useful queries that users may issue, and the context determined by user profiles. From a technical point of view, it will play the role of an intermediate access server residing between the end users and multiple heterogeneous audiovisual archives organized according to new MPEG standards.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C22,
   title = {Intelligent Semantic Access to Audiovisual Content},
   author = {Avrithis, Yannis and Stamou, Giorgos and Delopoulos, Anastasios and Kollias, Stefanos},
   booktitle = {Proceedings of 2nd Hellenic Conference on Artificial Intelligence (SETN)},
   month = {4},
   address = {Thessaloniki, Greece},
   year = {2002}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2001"></a>
					2001
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C21"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C21" id="tog-C21">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Intelligent System for Retrieval and Mining of Audiovisual Material Based on the MPEG-7 Description Schemes
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Akrivas, S. Ioannou, E. Karakoulakis, K. Karpouzis, Y. Avrithis, A. Delopoulos, S. Kollias, I. Varlamis, M. Vaziriannis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">EUNITE&nbsp;2001</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C021.eunite01.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C21.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_Qo2XoVZTnwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15412897341591193457" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/4e74599a6de1de4754242ec18b18db26773f01f3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/4e74599a6de1de4754242ec18b18db26773f01f3#citing-papers" title="Citations @ Semantic Scholar">8</a>
						</div>

					</div>
					<div class="collapse" id="col-C21">
						<div class="pub-ref">
							In Proc. <em>European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive Systems</em><br>
							Tenerife, Spain  <span class="bull"></span> Dec 2001
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A system for digitization, storage and retrieval of audiovisual information and its associated data (meta-info) is presented. The principles of the evolving MPEG-7 standard have been adopted for the creation of the data model used by the system, permitting efficient separation of database design, content description, business logic and presentation of query results. XML Schema is used in defining the data model, and XML in describing audiovisual content. Issues regarding problems that emerged during system design and their solutions are discussed, such as customization, deviations from the standard MPEG-7 DSs or even the design of entirely custom DSs. Although the system includes modules for digitization, annotation, archiving and intelligent data mining, the paper mainly focuses on the use of MPEG-7 as the information model.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C21,
   title = {An Intelligent System for Retrieval and Mining of Audiovisual Material Based on the {MPEG-7} Description Schemes},
   author = {Akrivas, Giorgos and Ioannou, Spyros and Karakoulakis, Elias and Karpouzis, Kostas and Avrithis, Yannis and Delopoulos, Anastasios and Kollias, Stefanos and Varlamis, Iraklis and Vaziriannis, Michalis},
   booktitle = {Proceedings of European Symposium on Intelligent Technologies, Hybrid Systems and their implementation on Smart Adaptive Systems (EUNITE)},
   month = {12},
   address = {Tenerife, Spain},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C20"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C20" id="tog-C20">
							<i class="left-60 tog far fa-chevron-down"></i>
							FAETHON: Unified Intelligent Access to Heterogeneous Audiovisual Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Stamou</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">VLBV&nbsp;2001</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C020.vlbv01.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C20.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:9ZlFYXVOiuMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3894560493737867901" title="Citations @ Google Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-C20">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Very Low Bitrate Video Coding</em><br>
							Athens, Greece  <span class="bull"></span> Oct 2001
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, the FAETHON project is presented, whose overall objective is to develop an integrated information system that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual (a/v) archives. This novel system will exploit the advances in handling a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced access services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" and "personalized response".
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C20,
   title = {{FAETHON}: Unified Intelligent Access to Heterogeneous Audiovisual Content},
   author = {Avrithis, Yannis and Stamou, Giorgos},
   booktitle = {Proceedings of International Workshop on Very Low Bitrate Video Coding (VLBV)},
   month = {10},
   address = {Athens, Greece},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C19"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C19" id="tog-C19">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unified Intelligent Access to Heterogeneous Audiovisual Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Delopoulos, S. Kollias, Y. Avrithis, W. Haas, K. Majcen</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CBMI&nbsp;2001</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C019.cbmi01.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C19.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:5awf1xo2G04C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2827330486524744586" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/cab22f0f5f42e447919e27b0842eb0959219dad3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/cab22f0f5f42e447919e27b0842eb0959219dad3#citing-papers" title="Citations @ Semantic Scholar">12</a>
						</div>

					</div>
					<div class="collapse" id="col-C19">
						<div class="pub-ref">
							In Proc. <em>2nd International Workshop in Content-Based Multimedia Indexing</em><br>
							Brescia, Italy  <span class="bull"></span> Sep 2001
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Content-based audiovisual data retrieval utilizing new emerging related standards such as MPEG-7 will yield ineffective results, unless major focus is given to the semantic information level. Mapping of low level, sub-symbolic descriptors of a/v archives to high level symbolic ones is in general difficult, even impossible with the current state of technology. It can, however, be tackled when dealing with specific application domains. It seems that the extraction of semantic information from a/v and text related data is tractable taking into account the nature of useful queries that users may issue and the context determined by user profile. The IST project FAETHON is developing a novel platform to implement these ideas for user friendly and highly informative access to distributed audiovisual archives.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C19,
   title = {Unified Intelligent Access to Heterogeneous Audiovisual Content},
   author = {Delopoulos, Anastasios and Kollias, Stefanos and Avrithis, Yannis and Haas, Werner and Majcen, Kurt},
   booktitle = {Proceedings of 2nd International Workshop in Content-Based Multimedia Indexing (CBMI)},
   month = {9},
   address = {Brescia, Italy},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-2000"></a>
					2000
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C18"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C18" id="tog-C18">
							<i class="left-60 tog far fa-chevron-down"></i>
							Affine-Invariant Curve Normalization for Shape-Based Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Xirouhakis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icpr.2000.905643" title="DOI">ICPR&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C018.icpr00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C18.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C018.icpr00-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icpr/AvrithisXK00" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:mB3voiENLucC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15283481625122113246" title="Citations @ Google Scholar">14</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/66372748b1d2989f02f228cff9a421a07c222b54" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/66372748b1d2989f02f228cff9a421a07c222b54#citing-papers" title="Citations @ Semantic Scholar">10</a>
						</div>

					</div>
					<div class="collapse" id="col-C18">
						<div class="pub-ref">
							In Proc. <em>15th International Conference on Pattern Recognition</em><br>
							Barcelona, Spain  <span class="bull"></span> Sep 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A novel method for two-dimensional curve normalization with respect to affine transformations is presented in this paper, allowing an affine-invariant curve representation to be obtained without any actual loss of information on the original curve. It can be applied as a pre-processing step to any shape representation, classification, recognition or retrieval technique, since it effectively decouples the problem of affine-invariant description from feature extraction and pattern matching. Curves estimated from object contours are first modeled by cubic B-splines and then normalized in several steps in order to eliminate translation, scaling, skew, starting point, rotation and reflection transformations, based on a combination of curve features including moments and Fourier descriptors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C18,
   title = {Affine-Invariant Curve Normalization for Shape-Based Retrieval},
   author = {Avrithis, Yannis and Xirouhakis, Yiannis and Kollias, Stefanos},
   booktitle = {Proceedings of 15th International Conference on Pattern Recognition (ICPR)},
   month = {9},
   pages = {1015--1018},
   address = {Barcelona, Spain},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C17"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C17" id="tog-C17">
							<i class="left-60 tog far fa-chevron-down"></i>
							Efficient Face Detection for Multimedia Applications
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Tsapatsoulis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip.2000.899289" title="DOI">ICIP&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C017.icip00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C17.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C017.icip00-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icip/TsapatsoulisAK00" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_FxGoFyzp5QC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13376388663715902521" title="Citations @ Google Scholar">98</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7ec5fd6862a3d73ac7c03d227f1d6b0b64e1c933" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7ec5fd6862a3d73ac7c03d227f1d6b0b64e1c933#citing-papers" title="Citations @ Semantic Scholar">29</a>
						</div>

					</div>
					<div class="collapse" id="col-C17">
						<div class="pub-ref">
							In Proc. <em>the International Conference on Image Processing</em><br>
							Vancouver, BC, Canada  <span class="bull"></span> Sep 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Face detection is becoming an important tool in the framework of many multimedia applications. Several face detection algorithms based on skin color characteristics have recently appeared in the literature. Most of them face generalization problems due to the skin color model they use. In this work we present a study which attempts to minimize the generalization problem by combining the M-RSST color segmentation algorithm with a Gaussian model of the skin color distribution and global shape features. Moreover by associating the resultant segments with a face probability we can index and retrieve facial images from multimedia databases.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C17,
   title = {Efficient Face Detection for Multimedia Applications},
   author = {Tsapatsoulis, Nicolas and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of the International Conference on Image Processing (ICIP)},
   month = {9},
   address = {Vancouver, BC, Canada},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C16"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C16" id="tog-C16">
							<i class="left-60 tog far fa-chevron-down"></i>
							Color-Based Retrieval of Facial Images
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, N. Tsapatsoulis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/7075724/" title="Electronic edition">EUSIPCO&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C016.eusipco00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C16.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C016.eusipco00-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eusipco/AvrithisTK00" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:dhFuZR0502QC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11284495408371145183" title="Citations @ Google Scholar">58</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/60b67aa82fcab34dd21f1569d7227159a322bacc" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/60b67aa82fcab34dd21f1569d7227159a322bacc#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C16">
						<div class="pub-ref">
							In Proc. <em>10th European Signal Processing Conference</em><br>
							Tampere, Finland  <span class="bull"></span> Sep 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Content-based retrieval from image databases attracts increasing interest the last few years. On the other hand several recent works on face detection based on the chrominance components of the color space have been presented in the literature showing promising results. In this work we combine color segmentation techniques and color based face detection in an efficient way for the purpose of facial image retrieving. In particular, images stored in a multimedia database are analyzed using the M-RSST segmentation algorithm and segment features including average color components, size, location, shape and texture are extracted for several image resolutions. An adaptive two-dimensional Gaussian density function is then employed for modeling skin-tone chrominance color component distribution and detecting image segments that probably correspond to human faces. This information is combined with object shape characteristics so that robust face detection is achieved. Based on the above, a query by example framework is proposed, supporting a highly interactive, configurable and flexible content-based retrieval system for human faces. Experimental results have shown that the proposed implementation combines efficiency, robustness and speed, and could be extended to generic visual information retrieval or video databases.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C16,
   title = {Color-Based Retrieval of Facial Images},
   author = {Avrithis, Yannis and Tsapatsoulis, Nicolas and Kollias, Stefanos},
   booktitle = {Proceedings of 10th European Signal Processing Conference (EUSIPCO)},
   month = {9},
   pages = {1397--1400},
   address = {Tampere, Finland},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C15"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C15" id="tog-C15">
							<i class="left-60 tog far fa-chevron-down"></i>
							Broadcast News Parsing Using Visual Cues: a Robust Face Detection Approach
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, N. Tsapatsoulis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icme.2000.871044" title="DOI">ICME&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C015.icme00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C15.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C015.icme00-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icmcs/AvrithisTK00" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:roLk4NBRz8UC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12324074418883580292,2147503823487702965" title="Citations @ Google Scholar">64</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/e1fb22dbbf5fcb50945b8037924537b6535bc76a" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/e1fb22dbbf5fcb50945b8037924537b6535bc76a#citing-papers" title="Citations @ Semantic Scholar">42</a>
						</div>

					</div>
					<div class="collapse" id="col-C15">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Multimedia and Expo</em><br>
							New York City, NY, US  <span class="bull"></span> Jul 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Automatic content-based analysis and indexing of broadcast news recordings or digitized news archives is becoming an important tool in the framework of many multimedia interactive services such as news summarization, browsing, retrieval and news-on-demand (NoD) applications. Existing approaches have achieved high performance in such applications but heavily rely on textual cues such as closed caption tokens and teletext transcripts. In this work we present an efficient technique for temporal segmentation and parsing of news recordings based on visual cues that can either be employed as stand-alone application for non-closed captioned broadcasts or integrated with audio and textual cues of existing systems. The technique involves robust face detection by means of color segmentation, skin color matching and shape processing, and is able to identify typical news instances like anchorpersons, reports and outdoor shots.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C15,
   title = {Broadcast News Parsing Using Visual Cues: A Robust Face Detection Approach},
   author = {Avrithis, Yannis and Tsapatsoulis, Nicolas and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE International Conference on Multimedia and Expo (ICME)},
   month = {7},
   pages = {1469--1472},
   address = {New York City, NY, US},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-1999"></a>
					1999
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C14"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C14" id="tog-C14">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Stochastic Framework for Optimal Key Frame Extraction from MPEG Video Databases
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Doulamis, A. Doulamis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/mmsp.1999.793811" title="DOI">MMSP&nbsp;1999</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C014.mmsp99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C14.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmsp/DoulamisDAK99" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:N5tVd3kTz84C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5077865837034791527" title="Citations @ Google Scholar">137</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/45cefc106eee8bd424edb9fb565c6f13f9cbfcf8" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/45cefc106eee8bd424edb9fb565c6f13f9cbfcf8#citing-papers" title="Citations @ Semantic Scholar">127</a>
						</div>

					</div>
					<div class="collapse" id="col-C14">
						<div class="pub-ref">
							In Proc. <em>IEEE International Workshop on Multimedia Signal Processing</em><br>
							Copenhagen, Denmark  <span class="bull"></span> Sep 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A video content representation framework is proposed in this paper for extracting limited but meaningful information of video data directly from MPEG compressed domain. A hierarchical color and motion segmentation scheme is applied to each video shot, transforming the conventional frame-based representation to a feature-based one. Then, all features are gathered together using a fuzzy formulation and extraction of several key frames is performed for each shot in a content-based rate sampling framework. In particular, our approach is based on minimization of a cross-correlation criterion among video frames of a given shot, so as to be located a set of minimally correlated feature vectors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C14,
   title = {A stochastic framework for optimal key frame extraction from {MPEG} video databases},
   author = {Doulamis, Nikolaos and Doulamis, Anastasios and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE International Workshop on Multimedia Signal Processing (MMSP)},
   month = {9},
   pages = {141--146},
   address = {Copenhagen, Denmark},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C13"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C13" id="tog-C13">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Optimal Framework for Summarization of Stereoscopic Video Sequences
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Doulamis, A. Doulamis, Y. Avrithis, K. Ntalianis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">IWSNHC3DI&nbsp;1999</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C013.iwsnhc3di99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C13.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C013.iwsnhc3di99-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:3s1wT3WcHBgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15131885027724319678" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0f5ce77018f62b4b427f7c21285b6d321eee16f4" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0f5ce77018f62b4b427f7c21285b6d321eee16f4#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-C13">
						<div class="pub-ref">
							In Proc. <em>International Workshop on Synthetic-Natural Hybrid Coding and Three Dimensional Imaging</em><br>
							Santorini, Greece  <span class="bull"></span> Sep 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In the context of this paper a generalized framework for non-linear representation of 3-D video sequences is proposed, regardless of the scene complexity. In particular, depth information is exploited to provide a more reliable video content segmentation. In this paper this is accomplished by merging color segments which belong to similar depth since a video object is usually located on the same depth plane while color segments give very accurate contours of the objects. To accelerate the segmentation algorithm a multiresolution implementation of the Recursive Shortest Spanning Tree (RSST) algorithm is presented both for color and depth segmentation. All features extracted by the video sequence analysis module are gathered together using a fuzzy feature vector formulation to increase the robustness of the proposed summarization scheme. Finally, key frames within each shot are extracted by minimizing a cross correlation criterion by means of a genetic algorithm.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C13,
   title = {An Optimal Framework for Summarization of Stereoscopic Video Sequences},
   author = {Doulamis, Nikolaos and Doulamis, Anastasios and Avrithis, Yannis and Ntalianis, Klimis and Kollias, Stefanos},
   booktitle = {Proceedings of International Workshop on Synthetic-Natural Hybrid Coding and Three Dimensional Imaging (IWSNHC3DI)},
   month = {9},
   address = {Santorini, Greece},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C12"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C12" id="tog-C12">
							<i class="left-60 tog far fa-chevron-down"></i>
							On the Use of Radon Transform for Facial Expression Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Tsapatsoulis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">ISAS&nbsp;1999</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C012.isas99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C12.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C012.isas99-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ns9cj8rnVeAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=884822067482815095" title="Citations @ Google Scholar">11</a>
						</div>

					</div>
					<div class="collapse" id="col-C12">
						<div class="pub-ref">
							In Proc. <em>5th International Conference on Information Systems Analysis and Synthesis</em><br>
							Orlando, FL, US  <span class="bull"></span> Aug 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A facial expression recognition scheme is presented in this paper, based on features derived from the optical flow between two instances of a face in the same emotional state. A pre-processing step of isolating the human face from the background is first employed by means of face detection and registration. A spatio-temporal description of the expression is then obtained by evaluating the Radon transform of the motion vectors between the face in its neutral condition and at the 'apex' of the expression. A linear curve normalization scheme is proposed, achieving a translation, scaling and resolution invariant representation of the Radon curves. Finally, experimental results are presented, illustrating the performance of the proposed algorithm for expression classification using a correlation criterion and a neural network classifier.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C12,
   title = {On the use of Radon Transform for Facial Expression Recognition},
   author = {Tsapatsoulis, Nicolas and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 5th International Conference on Information Systems Analysis and Synthesis (ISAS)},
   month = {8},
   address = {Orlando, FL, US},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C11"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C11" id="tog-C11">
							<i class="left-60 tog far fa-chevron-down"></i>
							Affine Invariant Representation and Classification of Object Contours for Image and Video Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Xirouhakis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CSCC&nbsp;1999</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C011.cscc99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C11.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C011.cscc99-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:2P1L_qKh6hAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14899856537899910841" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/c38775a0b5f65883d04eacc4356b8a258bbe032c" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/c38775a0b5f65883d04eacc4356b8a258bbe032c#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-C11">
						<div class="pub-ref">
							In Proc. <em>3rd IEEE/IMACS World Multiconference on Circuits, Systems, Communications and Computers</em><br>
							Athens, Greece  <span class="bull"></span> Jul 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Recent literature comprises a large number of papers on the query and retrieval of visual information based on its content. At the same time, a number of prototype systems have been implemented enabling searching through on-line image databases and still image retrieval. However, it has been often pointed out that meaningful/semantic information should be extracted from visual information in order to improve the efficiency and functionality of a content-based retrieval tool. In this context, present work focuses on the extraction of objects from images and video clips and modeling of the resulting object contours using B-splines. Affine-invariant curve representation is obtained through Normalized Fourier descriptors (NFD), curve moments, as well as a novel curve normalization algorithm that leads to major preservation of object shape information. A neural network approach is employed for supervised classification of video objects into prototype object classes. Experiments on several real-life and simulated video sequences are included to evaluate the classification results for all affine-invariant representations used.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C11,
   title = {Affine Invariant Representation and Classification of Object Contours for Image and Video Retrieval},
   author = {Avrithis, Yannis and Xirouhakis, Yiannis and Kollias, Stefanos},
   booktitle = {Proceedings of 3rd IEEE/IMACS World Multiconference on Circuits, Systems, Communications and Computers (CSCC)},
   month = {7},
   address = {Athens, Greece},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C10"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C10" id="tog-C10">
							<i class="left-60 tog far fa-chevron-down"></i>
							Interactive Content-Based Retrieval in Video Databases Using Fuzzy Classification and Relevance Feedback
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Doulamis, Y. Avrithis, N. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/mmcs.1999.778618" title="DOI">ICMSC&nbsp;1999</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C010.icmcs99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C10.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icmcs/DoulamisADK99" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:W7OEmFMy1HYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9698945086892883467" title="Citations @ Google Scholar">56</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/9464789bedfa771064acbe49d77620739afee095" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/9464789bedfa771064acbe49d77620739afee095#citing-papers" title="Citations @ Semantic Scholar">51</a>
						</div>

					</div>
					<div class="collapse" id="col-C10">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Multimedia Computing and Systems</em><br>
							Florence, Italy  <span class="bull"></span> Jun 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents an integrated framework for interactive content-based retrieval in video databases by means of visual queries. The proposed system incorporates algorithms for video shot detection, key frame and shot selection, automated video object segmentation and tracking, and construction of multidimensional feature vectors using fuzzy classification of color, motion or texture segment properties. Retrieval is then performed in an interactive way by employing a parametric distance between feature vectors and updating distance parameters according to user requirements using relevance feedback. Experimental results demonstrate increased performance and flexibility according to user information needs.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C10,
   title = {Interactive Content-Based Retrieval in Video Databases Using Fuzzy Classification and Relevance Feedback},
   author = {Doulamis, Anastasios and Avrithis, Yannis and Doulamis, Nikolaos and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE International Conference on Multimedia Computing and Systems (ICMSC)},
   month = {6},
   pages = {954--958},
   address = {Florence, Italy},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-1998"></a>
					1998
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C9"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C9" id="tog-C9">
							<i class="left-60 tog far fa-chevron-down"></i>
							Image Retrieval and Classification Using Affine Invariant B-Spline Representation and Neural Networks
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Xirouhakis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">ICNNM&nbsp;1998</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C009.iee98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C9.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C009.iee98-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:hC7cP41nSMkC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1075409581871312530,17102485299088037981" title="Citations @ Google Scholar">14</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a003959d6e50706bf71399fe4b0f0e6a6686aaa6" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a003959d6e50706bf71399fe4b0f0e6a6686aaa6#citing-papers" title="Citations @ Semantic Scholar">13</a>
						</div>

					</div>
					<div class="collapse" id="col-C9">
						<div class="pub-ref">
							In Proc. <em>IEE Colloquium on Neural Nets and Multimedia</em><br>
							London, UK  <span class="bull"></span> Oct 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, a system for content-based image retrieval from video databases is introduced, using B-splines for affine invariant object representation. A small number of key-frames is extracted from each video sequence, which provide sufficient information about the video content. Color and motion segmentation and tracking is then employed for automatic extraction of video objects. A B-spline representation of the object contours is then obtained, which possesses important properties, such as smoothness, continuity and invariance under affine transformation. A neural network approach is used for supervised classification of video objects into prototype object classes. Finally, higher level classes can be constructed combining primary classes, providing the ability to obtain a high level of abstraction in the representation of each video sequence.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C9,
   title = {Image Retrieval and Classification Using Affine Invariant B-Spline Representation and Neural Networks},
   author = {Xirouhakis, Yiannis and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of IEE Colloquium on Neural Nets and Multimedia (ICNNM)},
   month = {10},
   pages = {4/1--4/4},
   address = {London, UK},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C8"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C8" id="tog-C8">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Adaptive Approach to Video Indexing and Retrieval Using Fuzzy Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, A. Doulamis, N. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">VLBV&nbsp;1998</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C008.vlbv98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C8.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:TQgYirikUcIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5374327875285607614" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6b04e5ffab1179faeebb7ae71efb5af4e3017911" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6b04e5ffab1179faeebb7ae71efb5af4e3017911#citing-papers" title="Citations @ Semantic Scholar">13</a>
						</div>

					</div>
					<div class="collapse" id="col-C8">
						<div class="pub-ref">
							In Proc. <em>International Conference on Very Low Bitrate Video Coding</em><br>
							Urbana, IL, US  <span class="bull"></span> Oct 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									An integrated framework for content-based indexing and retrieval in video databases is presented in this paper, which has the capability of adapting its performance according to user requirements. Video sequences are represented by extracting a small number of key frames or scenes and constructing multidimensional feature vectors using fuzzy classification of color, motion or texture segment properties. Queries are then performed by employing a parametric distance between feature vectors, and adaptation is achieved by estimating distance parameters according to user requirements, resulting in a content based retrieval system of increased performance and flexibility.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C8,
   title = {An Adaptive Approach to Video Indexing and Retrieval Using Fuzzy Classification},
   author = {Avrithis, Yannis and Doulamis, Anastasios and Doulamis, Nikolaos and Kollias, Stefanos},
   booktitle = {Proceedings of International Conference on Very Low Bitrate Video Coding (VLBV)},
   month = {10},
   address = {Urbana, IL, US},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C7"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C7" id="tog-C7">
							<i class="left-60 tog far fa-chevron-down"></i>
							Video Content Representation Using Optimal Extraction of Frames and Scenes
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Doulamis, A. Doulamis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1109/icip.1998.723660" title="DOI">ICIP&nbsp;1998</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C007.icip98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C7.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C007.icip98-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/icip/DoulamisDAK98" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:IjCSPb-OGe4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12386017633710331008" title="Citations @ Google Scholar">87</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/dff581989541530a1bfc2c68d1cbdd6e7251c796" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/dff581989541530a1bfc2c68d1cbdd6e7251c796#citing-papers" title="Citations @ Semantic Scholar">65</a>
						</div>

					</div>
					<div class="collapse" id="col-C7">
						<div class="pub-ref">
							In Proc. <em>IEEE International Conference on Image Processing</em><br>
							Chicago, IL, US  <span class="bull"></span> Oct 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an efficient video content representation is proposed using optimal extraction of characteristic frames and scenes. This representation, apart from providing browsing capabilities to digital video databases, also allows more efficient content-based queries and indexing. For performing the frame/scene extraction, a feature vector formulation of the images is proposed based on color and motion segmentation. Then, the scene selection is accomplished by clustering similar scenes based on a distortion criterion. Frame selection is performed using an optimization method for locating a set of minimally correlated feature vectors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C7,
   title = {Video Content Representation Using Optimal Extraction of Frames and Scenes},
   author = {Doulamis, Nikolaos and Doulamis, Anastasios and Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE International Conference on Image Processing (ICIP)},
   month = {10},
   pages = {875--879},
   address = {Chicago, IL, US},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C6"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C6" id="tog-C6">
							<i class="left-60 tog far fa-chevron-down"></i>
							Ultrasonic Array Imaging Using CDMA Techniques
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, A. Delopoulos, G. Papageorgiou</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/7090099/" title="Electronic edition">EUSIPCO&nbsp;1998</a>
							<a class="lnk mr" href="../data/pub/pdf/conf/C006.eusipco98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C6.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C006.eusipco98-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/eusipco/AvrithisDP98" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:J-pR_7NvFogC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17287670842430970971" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/496e3a821b4d5ecea1b5e1d99493614bf6d6fa83" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-C6">
						<div class="pub-ref">
							In Proc. <em>IX European Signal Processing Conference</em><br>
							Rhodes, Greece  <span class="bull"></span> Sep 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A new method for designing ultrasonic imaging systems is presented in this paper. The method is based on the use of transducer arrays whose elements transmit wideband signals generated by pseudo-random codes, similarly to code division multiple access (CDMA) systems in communications. The use of code sequences instead of pulses, which are typically used in conventional phased arrays, combined with transmit and receive beamforming for steering different codes at each direction, permits parallel acquisition of a large number of measurements corresponding to different directions.  Significantly higher image acquisition rate as well as lateral and contrast resolution are thus obtained, while axial resolution remains close to that of phased arrays operating in pulse-echo mode. Time and frequency division techniques are also studied and a unified theoretical model is derived, which is validated by experimental results.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C6,
   title = {Ultrasonic Array Imaging Using {CDMA} Techniques},
   author = {Avrithis, Yannis and Delopoulos, Anastasios and Papageorgiou, Grigorios},
   booktitle = {Proceedings of IX European Signal Processing Conference (EUSIPCO)},
   month = {9},
   address = {Rhodes, Greece},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C5"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C5" id="tog-C5">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Genetic Algorithm for Efficient Video Content Representation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Doulamis, Y. Avrithis, N. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SOFTCOM&nbsp;1998</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C005.softcom98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C5.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C005.softcom98-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:dfsIfKJdRG4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=4819585974270919459" title="Citations @ Google Scholar">7</a>
						</div>

					</div>
					<div class="collapse" id="col-C5">
						<div class="pub-ref">
							In Proc. <em>IMACS/IFAC International Symposium on Soft Computing in Engineering Applications</em><br>
							Athens, Greece  <span class="bull"></span> Jun 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The first stage of the proposed algorithm includes a scene cut detection mechanism.  Then, video processing and image analysis techniques are applied to each video frame for extracting color, motion and texture information. Color information is extracted by applying a hierarchical color segmentation algorithm to each video frame.  Consequently, apart from the color histogram of each frame additional features are collected concerning the number of color segments, and their location, size and shape.  Motion information is also extracted in a similar way by using a motion estimation and segmentation algorithm.  All the above features are gathered in order to form a multidimensional feature vector for each video frame. The representation of each frame by a feature vector, apart from reducing storage requirements, transforms the image domain to another domain, more efficient for key frame selection. Since similar frames can be characterized by different color or motion segments, due to imperfections of the segmentation algorithms, a fuzzy representation of feature vectors is adopted in order to provide more robust searching capabilities. In particular, we classify color as well as motion and texture segments into pre-determined classes forming a multidimensional histogram and a degree of membership is allocated to each category so that the possibility of erroneous comparisons is eliminated.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C5,
   title = {A Genetic Algorithm for Efficient Video Content Representation},
   author = {Doulamis, Anastasios and Avrithis, Yannis and Doulamis, Nikolaos and Kollias, Stefanos},
   booktitle = {Proceedings of IMACS/IFAC International Symposium on Soft Computing in Engineering Applications (SOFTCOM)},
   month = {6},
   address = {Athens, Greece},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C4"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C4" id="tog-C4">
							<i class="left-60 tog far fa-chevron-down"></i>
							Efficient Content Representation in MPEG Video Databases
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, N. Doulamis, A. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CBAIVL/CVPR&nbsp;1998</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C004.cvpr98.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C4.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C004.cvpr98-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:kNdYIx-mwKoC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17849218711876200610" title="Citations @ Google Scholar">26</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/8cceea1ee2e813a60c9a1f3fc514dbb0bdf9be35" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/8cceea1ee2e813a60c9a1f3fc514dbb0bdf9be35#citing-papers" title="Citations @ Semantic Scholar">24</a>
						</div>

					</div>
					<div class="collapse" id="col-C4">
						<div class="pub-ref">
							In Proc. <em>IEEE Workshop on Content-Based Access of Image and Video Libraries</em><br>
							part of <em>IEEE Conference on Computer Vision and Pattern Recognition</em><br>
							Santa Barbara, CA, US  <span class="bull"></span> Jun 1998
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an efficient video content representation system is presented which permits automatic extraction of a limited number of characteristic frames or scenes that provide sufficient information about the content of an MPEG video sequence. This can be used for reduction of the amount of stored information that is necessary in order to provide search capabilities in a multimedia database, resulting in faster and more efficient video queries. Moreover, the proposed system can be used for automatic generation of low resolution video clip previews (trailers), giving the ability to browse databases on web pages. Finally, direct content-based retrieval with image queries is possible using the feature vector representation incorporated in our system.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C4,
   title = {Efficient Content Representation in {MPEG} Video Databases},
   author = {Avrithis, Yannis and Doulamis, Nikolaos and Doulamis, Anastasios and Kollias, Stefanos},
   booktitle = {Proceedings of IEEE Workshop on Content-Based Access of Image and Video Libraries (CBAIVL), part of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
   month = {6},
   pages = {91--95},
   address = {Santa Barbara, CA, US},
   year = {1998}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-1997"></a>
					1997
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C3" id="tog-C3">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fuzzy Image Classification Using Multiresolution Neural Networks with Applications to Remote Sensing
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">DSP&nbsp;1997</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C003.dsp97.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C003.dsp97-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:O3NaXMp0MMsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=15424746899310857754" title="Citations @ Google Scholar">10</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/551ea453d65bc46a06b1a7bc03aa7b76d4a56d94" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/551ea453d65bc46a06b1a7bc03aa7b76d4a56d94#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-C3">
						<div class="pub-ref">
							In Proc. <em>13th International Conference on Digital Signal Processing</em><br>
							Santorini, Greece  <span class="bull"></span> Jul 1997
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Recent progress in supervised image classification research, has demonstrated the potential usefulness of incorporating fuzziness in the training, allocation and testing stages of several classification techniques. In this paper a multiresolution neural network approach to supervised classification is presented, exploiting the inherent fuzziness of such techniques in order to perform classification at different resolution levels and gain in computational complexity. In particular, multiresolution image analysis is carried out and hierarchical neural networks are used as an efficient architecture for classification of the derived multiresolution image representations. A new scheme is then introduced for transferring classification results to higher resolutions based on the fuzziness of the results of lower resolutions, resulting in faster implementation. Experimental results on land cover mapping applications from remotely sensed data illustrate significant improvements in classification speed without deterioration of representation accuracy.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C3,
   title = {Fuzzy Image Classification Using Multiresolution Neural Networks with Applications to Remote Sensing},
   author = {Avrithis, Yannis and Kollias, Stefanos},
   booktitle = {Proceedings of 13th International Conference on Digital Signal Processing (DSP)},
   month = {7},
   pages = {261--264},
   address = {Santorini, Greece},
   year = {1997}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C2" id="tog-C2">
							<i class="left-60 tog far fa-chevron-down"></i>
							Indexing and Retrieval of the Most Characteristic Frames / Scenes in Video Databases
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Doulamis, Y. Avrithis, N. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">WIAMIS&nbsp;1997</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C002.wiamis97.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C002.wiamis97-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:3fE2CSJIrl8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7143828805240626999" title="Citations @ Google Scholar">23</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/94940e34254d6c9331b947a0ab56af9d21279cc3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/94940e34254d6c9331b947a0ab56af9d21279cc3#citing-papers" title="Citations @ Semantic Scholar">10</a>
						</div>

					</div>
					<div class="collapse" id="col-C2">
						<div class="pub-ref">
							In Proc. <em>Workshop on Image Analysis for Multimedia Interactive Services</em><br>
							Louvain-la-Neuve, Belgium  <span class="bull"></span> Jun 1997
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									An integrated framework for automatic extraction of the most characteristic frames or scenes of a video sequence is presented in this paper. This is accomplished by extracting a collection of a small number of frames or scenes that provide sufficient information about the video sequence. The scene/frame selection mechanism is based on a transformation from the image to a feature domain, which is more suitable for image comparisons, queries and retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C2,
   title = {Indexing and Retrieval of the Most Characteristic Frames / Scenes in Video Databases},
   author = {Doulamis, Anastasios and Avrithis, Yannis and Doulamis, Nikolaos and Kollias, Stefanos},
   booktitle = {Proceedings of Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)},
   month = {6},
   pages = {105--110},
   address = {Louvain-la-Neuve, Belgium},
   year = {1997}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="conf-1993"></a>
					1993
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="C1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-C1" id="tog-C1">
							<i class="left-60 tog far fa-chevron-down"></i>
							An Efficient Scheme for Invariant Optical Character Recognition Using Triple Correlations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, A. Delopoulos, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">DSP&nbsp;1993</span>
							<a class="lnk mr" href="../data/pub/pdf/conf/C001.dsp93.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/conf/C1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/conf/C001.dsp93-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:abG-DnoFyZgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=6451099321101217141" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6ec86551e797fa83b80899f5e9af7ac6f26bdbc2" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6ec86551e797fa83b80899f5e9af7ac6f26bdbc2#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-C1">
						<div class="pub-ref">
							In Proc. <em>International Conference on Digital Signal Processing</em><br>
							Nicosia, Cyprus  <span class="bull"></span> Jul 1993
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C001.dsp93.png"><img src="../data/pub/thumb/wide/conf/C001.dsp93.png" alt="C1 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The implementation of an efficient scheme for translation, rotation and scale invariant optical character recognition is presented in this paper. An image representation is used, which is based on appropriate clustering and transformation of the image triple-correlation domain. This representation is one-to-one related to the class of all shifted-rotated-scaled versions of the original image, as well as robust to a wide variety of additive noises. Special attention is given to binary images, which are used for Optical Character Recognition, and simulation results illustrate the performance of the proposed implementation.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@conference{C1,
   title = {An Efficient Scheme for Invariant Optical Character Recognition Using Triple Correlations},
   author = {Avrithis, Yannis and Delopoulos, Anastasios and Kollias, Stefanos},
   booktitle = {Proceedings of International Conference on Digital Signal Processing (DSP)},
   month = {7},
   address = {Nicosia, Cyprus},
   year = {1993}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="report"></a>
					<span class="mr">Preprints / technical reports</span>
					<i class="fal fa-memo"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="report-2024"></a>
					2024
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R51"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R51" id="tog-R51">
							<i class="left-60 tog far fa-chevron-down"></i>
							Multi-Target Unsupervised Domain Adaptation for Semantic Segmentation without External Data
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Xu, P. Ghamisi, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/2405.06502" title="Open access">arXiv,&nbsp;2024</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R051.2405.06502v1.uda.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R51.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R051.2405.06502v1.uda.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
								<a class="lnk mr2" href="../code/#ut-kd" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R51">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2405.06502</em>  <span class="bull"></span> May 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R051.2405.06502.uda.svg"><img src="../data/pub/thumb/wide/report/R051.2405.06502.uda.svg" alt="R51 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Multi-target unsupervised domain adaptation (UDA) aims to learn a unified model to address the domain shift between multiple target domains. Due to the difficulty of obtaining annotations for dense predictions, it has recently been introduced into cross-domain semantic segmentation. However, most existing solutions require labeled data from the source domain and unlabeled data from multiple target domains concurrently during training. Collectively, we refer to this data as "external". When faced with new unlabeled data from an unseen target domain, these solutions either do not generalize well or require retraining from scratch on all data. To address these challenges, we introduce a new strategy called "multi-target UDA without external data" for semantic segmentation. Specifically, the segmentation model is initially trained on the external data. Then, it is adapted to a new unseen target domain without accessing any external data. This approach is thus more scalable than existing solutions and remains applicable when external data is inaccessible. We demonstrate this strategy using a simple method that incorporates self-distillation and adversarial learning, where knowledge acquired from the external data is preserved during adaptation through "one-way" adversarial learning. Extensive experiments in several synthetic-to-real and real-to-real adaptation settings on four benchmark urban driving datasets show that our method significantly outperforms current state-of-the-art solutions, even in the absence of external data. Our source code is available online <a href="https://github.com/YonghaoXu/UT-KD">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R51,
   title = {Multi-Target Unsupervised Domain Adaptation for Semantic Segmentation without External Data},
   author = {Xu, Yonghao and Ghamisi, Pedram and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2405.06502},
   month = {5},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R50"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R50" id="tog-R50">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Learning Paradigm for Interpretable Gradients
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Torres Figueroa, H. Zhang, R. Sicre, Y. Avrithis, S. Ayache</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/2404.15024" title="Open access">arXiv,&nbsp;2024</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R050.2404.15024v1.gpb.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R50.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R050.2404.15024v1.gpb.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R50">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2404.15024</em>  <span class="bull"></span> Apr 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R050.2404.15024.gpb.svg"><img src="../data/pub/thumb/wide/report/R050.2404.15024.gpb.svg" alt="R50 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper studies interpretability of convolutional networks by means of saliency maps. Most approaches based on Class Activation Maps (CAM) combine information from fully connected layers and gradient through variants of backpropagation. However, it is well understood that gradients are noisy and alternatives like guided backpropagation have been proposed to obtain better visualization at inference. In this work, we present a novel training approach to improve the quality of gradients for interpretability. In particular, we introduce a regularization loss such that the gradient with respect to the input image obtained by standard backpropagation is similar to the gradient obtained by guided backpropagation. We find that the resulting gradient is qualitatively less noisy and improves quantitatively the interpretability properties of different networks, using several interpretability methods.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R50,
   title = {A Learning Paradigm for Interpretable Gradients},
   author = {Torres Figueroa, Felipe and Zhang, Hanwei and Sicre, Ronan and Avrithis, Yannis and Ayache, Stephane},
   journal = {arXiv preprint arXiv:2404.15024},
   month = {4},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R49"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R49" id="tog-R49">
							<i class="left-60 tog far fa-chevron-down"></i>
							CA-Stream: Attention-Based Pooling for Interpretable Image Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Torres Figueroa, H. Zhang, R. Sicre, S. Ayache, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2404.14996" title="Open access">arXiv,&nbsp;2024</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R049.2404.14996v1.ca-stream.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R49.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R049.2404.14996v1.ca-stream.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R49">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2404.14996</em>  <span class="bull"></span> Apr 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R049.2404.14996.ca-stream.svg"><img src="../data/pub/thumb/wide/report/R049.2404.14996.ca-stream.svg" alt="R49 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Explanations obtained from transformer-based architectures in the form of raw attention, can be seen as a class-agnostic saliency map. Additionally, attention-based pooling serves as a form of masking the in feature space. Motivated by this observation, we design an attention-based pooling mechanism intended to replace Global Average Pooling (GAP) at inference. This mechanism, called Cross-Attention Stream (CA-Stream), comprises a stream of cross attention blocks interacting with features at different network depths. CA-Stream enhances interpretability in models, while preserving recognition performance.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R49,
   title = {{CA}-Stream: Attention-based pooling for interpretable image recognition},
   author = {Torres Figueroa, Felipe and Zhang, Hanwei and Sicre, Ronan and Ayache, Stephane and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2404.14996},
   month = {4},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R48"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R48" id="tog-R48">
							<i class="left-60 tog far fa-chevron-down"></i>
							On Train-Test Class Overlap and Detection for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, J. Yoon, T. Hwang, S. Choi, Y.H. Gu, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2404.01524" title="Open access">arXiv,&nbsp;2024</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R048.2404.01524v1.gld-clean.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R48.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R048.2404.01524v1.gld-clean.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R48">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2404.01524</em>  <span class="bull"></span> Apr 2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R048.2404.01524.gld-clean.svg"><img src="../data/pub/thumb/wide/report/R048.2404.01524.gld-clean.svg" alt="R48 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris [34], the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.What does it take to focus on objects or interest and ignore background clutter when indexing? Do we need to train an object detector and the representation separately? Do we need location supervision? We introduce Single-stage Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect objects of interest and extract a global image representation. We outperform previous state-of-the-art on both existing training sets and the new RGLDv2-clean. Our dataset is available at <a href="https://github.com/dealicious-inc/RGLDv2-clean">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R48,
   title = {On Train-Test Class Overlap and Detection for Image Retrieval},
   author = {Song, Chull Hwan and Yoon, Jooyoung and Hwang, Taebaek and Choi, Shunghyun and Gu, Yeong Hyeon and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2404.01524},
   month = {4},
   year = {2024}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2023"></a>
					2023
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R47"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R47" id="tog-R47">
							<i class="left-60 tog far fa-chevron-down"></i>
							PowMix: a Versatile Regularizer for Multimodal Sentiment Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Georgiou, Y. Avrithis, A. Potamianos</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2312.12334" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R047.2312.12334v1.powmix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R47.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R047.2312.12334v1.powmix.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2312.12334" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2312-12334" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:XUvXOeBm_78C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/62ea7ad505c36850b6a91a82fd8d411e625da49e" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R47">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2312.12334</em>  <span class="bull"></span> Dec 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R047.2312.12334.powmix.png"><img src="../data/pub/thumb/wide/report/R047.2312.12334.powmix.png" alt="R47 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Multimodal sentiment analysis (MSA) leverages heterogeneous data sources to interpret the complex nature of human sentiments. Despite significant progress in multimodal architecture design, the field lacks comprehensive regularization methods. This paper introduces PowMix, a versatile embedding space regularizer that builds upon the strengths of unimodal mixing-based regularization approaches and introduces novel algorithmic components that are specifically tailored to multimodal tasks. PowMix is integrated before the fusion stage of multimodal architectures and facilitates intra-modal mixing, such as mixing text with text, to act as a regularizer. PowMix consists of five components: 1) a varying number of generated mixed examples, 2) mixing factor reweighting, 3) anisotropic mixing, 4) dynamic mixing, and 5) cross-modal label mixing. Extensive experimentation across benchmark MSA datasets and a broad spectrum of diverse architectural designs demonstrate the efficacy of PowMix, as evidenced by consistent performance improvements over baselines and existing mixing methods. An in-depth ablation study highlights the critical contribution of each PowMix component and how they synergistically enhance performance. Furthermore, algorithmic analysis demonstrates how PowMix behaves in different scenarios, particularly comparing early versus late fusion architectures. Notably, PowMix enhances overall performance without sacrificing model robustness or magnifying text dominance. It also retains its strong performance in situations of limited data. Our findings position PowMix as a promising versatile regularization strategy for MSA. Code will be made available.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R47,
   title = {{PowMix}: A Versatile Regularizer for Multimodal Sentiment Analysis},
   author = {Georgiou, Efthymios and Avrithis, Yannis and Potamianos, Alexandros},
   journal = {arXiv preprint arXiv:2312.12334},
   month = {12},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R46"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R46" id="tog-R46">
							<i class="left-60 tog far fa-chevron-down"></i>
							Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, E. Kijak, L. Amsaleg, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2311.05538" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R046.2311.05538v1.multimix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R46.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R046.2311.05538v1.multimix.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2311.05538" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2311-05538" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R46">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2311.05538</em>  <span class="bull"></span> Nov 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R046.2311.05538.multimix.png"><img src="../data/pub/thumb/wide/report/R046.2311.05538.multimix.png" alt="R46 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Mixup refers to interpolation-based data augmentation, originally motivated as a way to go beyond empirical risk minimization (ERM). Its extensions mostly focus on the definition of interpolation and the space (input or feature) where it takes place, while the augmentation process itself is less studied. In most methods, the number of generated examples is limited to the mini-batch size and the number of examples being interpolated is limited to two (pairs), in the input space.
								</p>
								<p>
									We make progress in this direction by introducing MultiMix, which generates an arbitrarily large number of interpolated examples beyond the mini-batch size and interpolates the entire mini-batch in the embedding space. Effectively, we sample on the entire convex hull of the mini-batch rather than along linear segments between pairs of examples.
								</p>
								<p>
									On sequence data, we further extend to Dense MultiMix. We densely interpolate features and target labels at each spatial location and also apply the loss densely. To mitigate the lack of dense labels, we inherit labels from examples and weight interpolation factors by attention as a measure of confidence.
								</p>
								<p>
									Overall, we increase the number of loss terms per mini-batch by orders of magnitude at little additional cost. This is only possible because of interpolating in the embedding space. We empirically show that our solutions yield significant improvement over state-of-the-art mixup methods on four different benchmarks, despite interpolation being only linear. By analyzing the embedding space, we show that the classes are more tightly clustered and uniformly spread over the embedding space, thereby explaining the improved behavior.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R46,
   title = {Embedding Space Interpolation Beyond Mini-Batch, Beyond Pairs and Beyond Examples},
   author = {Venkataramanan, Shashanka and Kijak, Ewa and Amsaleg, Laurent and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2311.05538},
   month = {11},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R45"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R45" id="tog-R45">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive Anchors Label Propagation for Transductive Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, G. Ren, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2310.19996" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R045.2310.19996v1.icip23.few-a2lp.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R45.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R045.2310.19996v1.icip23.few-a2lp.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R45">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2310.19996</em>  <span class="bull"></span> Oct 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R045.2310.19996.icip23.few-a2lp.png"><img src="../data/pub/thumb/wide/report/R045.2310.19996.icip23.few-a2lp.png" alt="R45 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning addresses the issue of classifying images using limited labeled data. Exploiting unlabeled data through the use of transductive inference methods such as label propagation has been shown to improve the performance of few-shot learning significantly. Label propagation infers pseudo-labels for unlabeled data by utilizing a constructed graph that exploits the underlying manifold structure of the data. However, a limitation of the existing label propagation approaches is that the positions of all data points are fixed and might be sub-optimal so that the algorithm is not as effective as possible. In this work, we propose a novel algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function optimizing their positions in the manifold in the process. Our novel algorithm, Adaptive Anchor Label Propagation}, outperforms the standard label propagation algorithm by as much as 7% and 2% in the 1-shot and 5-shot settings respectively. We provide experimental results highlighting the merits of our algorithm on four widely used few-shot benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS and two commonly used backbones, ResNet12 and WideResNet-28-10. The source code can be found at <a href="https://github.com/MichalisLazarou/A2LP">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R45,
   title = {Adaptive Anchors Label Propagation for Transductive Few-Shot Learning},
   author = {Lazarou, Michalis and Avrithis, Yannis and Ren, Guangyu and Stathaki, Tania},
   journal = {arXiv preprint arXiv:2310.19996},
   month = {10},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R44"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R44" id="tog-R44">
							<i class="left-60 tog far fa-chevron-down"></i>
							Is Imagenet Worth 1 Video? Learning Strong Image Encoders from 1 Long Unlabelled Video
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, M.N. Rizve, J. Carreira, Y.M. Asano, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2310.08584" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R044.2310.08584v1.walking-tours.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R44.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R044.2310.08584v1.walking-tours.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2310.08584" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2310-08584" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:jU7OWUQzBzMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2440536107760425863" title="Citations @ Google Scholar">1</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6faa30cbe2e9681f77fa22f7fe151453b9306b62" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6faa30cbe2e9681f77fa22f7fe151453b9306b62#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-R44">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2310.08584</em>  <span class="bull"></span> Oct 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R044.2310.08584.walking-tours.svg"><img src="../data/pub/thumb/wide/report/R044.2310.08584.walking-tours.svg" alt="R44 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a "Walking Tours" dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning.
								</p>
								<p>
									Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a "tracking to learn to recognize" approach. Our method called DoRA, leads to attention maps that Discover and tRAck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R44,
   title = {Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video},
   author = {Venkataramanan, Shashanka and Rizve, Mamshad Nayeem and Carreira, Jo\~ao and Asano, Yuki M. and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2310.08584},
   month = {10},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R43"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R43" id="tog-R43">
							<i class="left-60 tog far fa-chevron-down"></i>
							Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Engin, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2309.15915" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R043.2309.15915v1.vitis.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R43.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R043.2309.15915v1.vitis.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2309.15915" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2309-15915" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R43">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2309.15915</em>  <span class="bull"></span> Sep 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R043.2309.15915.vitis.png"><img src="../data/pub/thumb/wide/report/R043.2309.15915.vitis.png" alt="R43 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Recent vision-language models are driven by large-scale pretrained models. However, adapting pretrained models on limited data presents challenges such as overfitting, catastrophic forgetting, and the cross-modal gap between vision and language. We introduce a parameter-efficient method to address these challenges, combining multimodal prompt learning and a transformer-based mapping network, while keeping the pretrained models frozen. Our experiments on several video question answering benchmarks demonstrate the superiority of our approach in terms of performance and parameter efficiency on both zero-shot and few-shot settings. Our code is available at <a href="https://engindeniz.github.io/vitis">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R43,
   title = {Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts},
   author = {Engin, Deniz and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2309.15915},
   month = {9},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R42"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R42" id="tog-R42">
							<i class="left-60 tog far fa-chevron-down"></i>
							Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">B. Psomas, I. Kakogeorgiou, K. Karantzalos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2309.06891" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R042.2309.06891v1.simpool.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R42.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R042.2309.06891v1.simpool.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2309.06891" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2309-06891" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R42">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2309.06891</em>  <span class="bull"></span> Sep 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R042.2309.06891.simpool.svg"><img src="../data/pub/thumb/wide/report/R042.2309.06891.simpool.svg" alt="R42 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Convolutional networks and vision transformers have different forms of pairwise interactions, pooling across layers and pooling at the end of the network. Does the latter really need to be different? As a by-product of pooling, vision transformers provide spatial attention for free, but this is most often of low quality unless self-supervised, which is not well studied. Is supervision really the problem?
								</p>
								<p>
									In this work, we develop a generic pooling framework and then we formulate a number of existing methods as instantiations. By discussing the properties of each group of methods, we derive SimPool, a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. We find that, whether supervised or self-supervised, this improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. One could thus call SimPool universal. To our knowledge, we are the first to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. Code at: <a href="https://github.com/billpsomas/simpool">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R42,
   title = {Keep It {SimPool}: Who Said Supervised Transformers Suffer from Attention Deficit?},
   author = {Psomas, Bill and Kakogeorgiou, Ioannis and Karantzalos, Konstantinos and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2309.06891},
   month = {9},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R41"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R41" id="tog-R41">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive Manifold for Imbalanced Transductive Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2304.14281" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R041.2304.14281v1.few-imbalanced.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R41.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R041.2304.14281v1.few-imbalanced.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2304.14281" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2304-14281" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/82dabfc26c1e677ed7d347520059ecd954deca07" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/82dabfc26c1e677ed7d347520059ecd954deca07#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-R41">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2304.14281</em>  <span class="bull"></span> Apr 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R041.2304.14281.few-imbalanced.png"><img src="../data/pub/thumb/wide/report/R041.2304.14281.few-imbalanced.png" alt="R41 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Transductive few-shot learning algorithms have showed substantially superior performance over their inductive counterparts by leveraging the unlabeled queries. However, the vast majority of such methods are evaluated on perfectly class-balanced benchmarks. It has been shown that they undergo remarkable drop in performance under a more realistic, imbalanced setting. To this end, we propose a novel algorithm to address imbalanced transductive few-shot learning, named Adaptive Manifold. Our method exploits the underlying manifold of the labeled support examples and unlabeled queries by using manifold similarity to predict the class probability distribution per query. It is parameterized by one centroid per class as well as a set of graph-specific parameters that determine the manifold. All parameters are optimized through a loss function that can be tuned towards class-balanced or imbalanced distributions. The manifold similarity shows substantial improvement over Euclidean distance, especially in the 1-shot setting. Our algorithm outperforms or is on par with other state of the art methods in three benchmark datasets, namely miniImageNet, tieredImageNet and CUB, and three different backbones, namely ResNet-18, WideResNet-28-10 and DenseNet-121. In certain cases, our algorithm outperforms the previous state of the art by as much as 4.2%.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R41,
   title = {Adaptive manifold for imbalanced transductive few-shot learning},
   author = {Lazarou, Michalis and Avrithis, Yannis and Stathaki, Tania},
   journal = {arXiv preprint arXiv:2304.14281},
   month = {4},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R40"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R40" id="tog-R40">
							<i class="left-60 tog far fa-chevron-down"></i>
							PartNeRF: Generating Part-Aware Editable 3D Shapes without 3D Supervision
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Tertikas, D. Paschalidou, B. Pan, J.J. Park, M.A. Uy, I. Emiris, Y. Avrithis, L. Guibas</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2303.09554" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R040.2303.09554v3.part-nerf.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R40.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R040.2303.09554v3.part-nerf.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2303.09554" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2303-09554" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R40">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2303.09554</em>  <span class="bull"></span> Mar 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R040.2303.09554.part-nerf.png"><img src="../data/pub/thumb/wide/report/R040.2303.09554.part-nerf.png" alt="R40 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Impressive progress in generative models and implicit representations gave rise to methods that can generate 3D shapes of high quality. However, being able to locally control and edit shapes is another essential property that can unlock several content creation applications. Local control can be achieved with part-aware models, but existing methods require 3D supervision and cannot produce textures. In this work, we devise PartNeRF, a novel part-aware generative model for editable 3D shape synthesis that does not require any explicit 3D supervision. Our model generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables several editing operations such as applying transformations on parts, mixing parts from different objects etc. To ensure distinct, manipulable parts we enforce a hard assignment of rays to parts that makes sure that the color of each ray is only determined by a single NeRF. As a result, altering one part does not affect the appearance of the others. Evaluations on various ShapeNet categories demonstrate the ability of our model to generate editable 3D objects of improved fidelity, compared to previous part-based generative approaches that require 3D supervision or models relying on NeRFs.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R40,
   title = {{PartNeRF}: Generating Part-Aware Editable 3D Shapes without 3D Supervision},
   author = {Tertikas, Konstantinos and Paschalidou, Despoina and Pan, Boxiao and Park, Jeong Joon and Uy, Mikaela Angelina and Emiris, Ioannis and Avrithis, Yannis and Guibas, Leonidas},
   journal = {arXiv preprint arXiv:2303.09554},
   month = {3},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R39"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R39" id="tog-R39">
							<i class="left-60 tog far fa-chevron-down"></i>
							Opti-CAM: Optimizing Saliency Maps for Interpretability
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, F. Torres, R. Sicre, Y. Avrithis, S. Ayache</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2301.07002" title="Open access">arXiv,&nbsp;2023</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R039.2301.07002v2.opti-cam.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R39.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R039.2301.07002v2.opti-cam.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2301.07002" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2301-07002" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:zCSUwVk65WsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18076578276904571380" title="Citations @ Google Scholar">7</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/fcc080a18a4ff5fb0ff06be5f9c6bc65b0d292e4" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/fcc080a18a4ff5fb0ff06be5f9c6bc65b0d292e4#citing-papers" title="Citations @ Semantic Scholar">5</a>
						</div>

					</div>
					<div class="collapse" id="col-R39">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2301.07002</em>  <span class="bull"></span> Jan 2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R039.2301.07002.opti-cam.svg"><img src="../data/pub/thumb/wide/report/R039.2301.07002.opti-cam.svg" alt="R39 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Methods based on class activation maps (CAM) provide a simple mechanism to interpret predictions of convolutional neural networks by using linear combinations of feature maps as saliency maps. By contrast, masking-based methods optimize a saliency map directly in the image space or learn it by training another network on additional data.
								</p>
								<p>
									In this work we introduce Opti-CAM, combining ideas from CAM-based and masking-based approaches. Our saliency map is a linear combination of feature maps, where weights are optimized per image such that the logit of the masked image for a given class is maximized. We also fix a fundamental flaw in two of the most common evaluation metrics of attribution methods. On several datasets, Opti-CAM largely outperforms other CAM-based approaches according to the most relevant classification metrics. We provide empirical evidence supporting that localization and classifier interpretability are not necessarily aligned.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R39,
   title = {Opti-{CAM}: Optimizing saliency maps for interpretability},
   author = {Zhang, Hanwei and Torres, Felipe and Sicre, Ronan and Avrithis, Yannis and Ayache, Stephane},
   journal = {arXiv preprint arXiv:2301.07002},
   month = {1},
   year = {2023}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2022"></a>
					2022
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R38"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R38" id="tog-R38">
							<i class="left-60 tog far fa-chevron-down"></i>
							Boosting Vision Transformers for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, J. Yoon, S. Choi, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2210.11909" title="Open access">arXiv,&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R038.2210.11909v1.dtop.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R38.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R038.2210.11909v1.dtop.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2210.11909" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2210-11909" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R38">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2210.11909</em>  <span class="bull"></span> Oct 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R038.2210.11909.dtop.png"><img src="../data/pub/thumb/wide/report/R038.2210.11909.dtop.png" alt="R38 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Vision transformers have achieved remarkable progress in vision tasks such as image classification and detection. However, in instance-level image retrieval, transformers have not yet shown good performance compared to convolutional networks. We propose a number of improvements that make transformers outperform the state of the art for the first time. (1) We show that a hybrid architecture is more effective than plain transformers, by a large margin. (2) We introduce two branches collecting global (classification token) and local (patch tokens) information, from which we form a global image representation. (3) In each branch, we collect multi-layer features from the transformer encoder, corresponding to skip connections across distant layers. (4) We enhance locality of interactions at the deeper layers of the encoder, which is the relative weakness of vision transformers. We train our model on all commonly used training sets and, for the first time, we make fair comparisons separately per training set. In all cases, we outperform previous models based on global representation. Public code is available at <a href="https://github.com/dealicious-inc/DToP">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R38,
   title = {Boosting vision transformers for image retrieval},
   author = {Song, Chull Hwan and Yoon, Jooyoung and Choi, Shunghyun and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2210.11909},
   month = {10},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R37"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R37" id="tog-R37">
							<i class="left-60 tog far fa-chevron-down"></i>
							Teach Me How to Interpolate a Myriad of Embeddings
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, E. Kijak, L. Amsaleg, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2206.14868" title="Open access">arXiv,&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R037.2206.14868v1.multimix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R37.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R037.2206.14868v1.multimix.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2206.14868" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2206-14868" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:lvd772isFD0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3651012309860418583" title="Citations @ Google Scholar">1</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/a6620d4a44f21d70312dc1a7b53d6e3e21fdd532" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/a6620d4a44f21d70312dc1a7b53d6e3e21fdd532#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-R37">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2206.14868</em>  <span class="bull"></span> Jun 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R037.2206.14868.multimix.png"><img src="../data/pub/thumb/wide/report/R037.2206.14868.multimix.png" alt="R37 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Mixup refers to interpolation-based data augmentation, originally motivated as a way to go beyond empirical risk minimization (ERM). Yet, its extensions focus on the definition of interpolation and the space where it takes place, while the augmentation itself is less studied: For a mini-batch of size $m$, most methods interpolate between $m$ pairs with a single scalar interpolation factor $\lambda$.
								</p>
								<p>
									In this work, we make progress in this direction by introducing MultiMix, which interpolates an arbitrary number $n$ of tuples, each of length $m$, with one vector $\lambda$ per tuple. On sequence data, we further extend to dense interpolation and loss computation over all spatial positions. Overall, we increase the number of tuples per mini-batch by orders of magnitude at little additional cost. This is possible by interpolating at the very last layer before the classifier. Finally, to address inconsistencies due to linear target interpolation, we introduce a self-distillation approach to generate and interpolate synthetic targets.
								</p>
								<p>
									We empirically show that our contributions result in significant improvement over state-of-the-art mixup methods on four benchmarks. By analyzing the embedding space, we observe that the classes are more tightly clustered and uniformly spread over the embedding space, thereby explaining the improved behavior.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R37,
   title = {Teach me how to Interpolate a Myriad of Embeddings},
   author = {Venkataramanan, Shashanka and Kijak, Ewa and Amsaleg, Laurent and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2206.14868},
   month = {6},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R36"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R36" id="tog-R36">
							<i class="left-60 tog far fa-chevron-down"></i>
							What to Hide from Your Students: Attention-Guided Masked Image Modeling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">I. Kakogeorgiou, S. Gidaris, B. Psomas, Y. Avrithis, A. Bursuc, K. Karantzalos, N. Komodakis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2203.12719" title="Open access">arXiv,&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R036.2203.12719v2.attmask.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R36.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R036.2203.12719v2.attmask.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.48550/arxiv.2203.12719" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2203-12719" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R36">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2203.12719</em>  <span class="bull"></span> Jul 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R036.2203.12719.attmask.png"><img src="../data/pub/thumb/wide/report/R036.2203.12719.attmask.png" alt="R36 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Transformers and masked language modeling are quickly being adopted and explored in computer vision as vision transformers and masked image modeling (MIM). In this work, we argue that image token masking differs from token masking in text, due to the amount and correlation of tokens in an image. In particular, to generate a challenging pretext task for MIM, we advocate a shift from random masking to informed masking. We develop and exhibit this idea in the context of distillation-based MIM, where a teacher transformer encoder generates an attention map, which we use to guide masking for the student. We thus introduce a novel masking strategy, called attention-guided masking (AttMask), and we demonstrate its effectiveness over random masking for dense distillation-based MIM as well as plain distillation-based self-supervised learning on classification tokens. We confirm that AttMask accelerates the learning process and improves the performance on a variety of downstream tasks. We provide the implementation code at <a href="https://github.com/gkakogeorgiou/attmask">this https URL</a>.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R36,
   title = {What to Hide from Your Students: Attention-Guided Masked Image Modeling},
   author = {Kakogeorgiou, Ioannis and Gidaris, Spyros and Psomas, Bill and Avrithis, Yannis and Bursuc, Andrei and Karantzalos, Konstantinos and Komodakis, Nikos},
   journal = {arXiv preprint arXiv:2203.12719},
   month = {7},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2021"></a>
					2021
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R35"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R35" id="tog-R35">
							<i class="left-60 tog far fa-chevron-down"></i>
							All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C.H. Song, H.J. Han, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2107.08000" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R035.2107.08000v1.glam.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R35.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R035.2107.08000v1.glam.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2107-08000" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R35">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2107.08000</em>  <span class="bull"></span> Jul 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R035.2107.08000.glam.svg"><img src="../data/pub/thumb/wide/report/R035.2107.08000.glam.svg" alt="R35 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We address representation learning for large-scale instance-level image retrieval. Apart from backbone, training pipelines and loss functions, popular approaches have focused on different spatial pooling and attention mechanisms, which are at the core of learning a powerful global image representation. There are different forms of attention according to the interaction of elements of the feature tensor (local and global) and the dimensions where it is applied (spatial and channel). Unfortunately, each study addresses only one or two forms of attention and applies it to different problems like classification, detection or retrieval.
								</p>
								<p>
									We present global-local attention module (GLAM), which is attached at the end of a backbone network and incorporates all four forms of attention: local and global, spatial and channel. We obtain a new feature tensor and, by spatial pooling, we learn a powerful embedding for image retrieval. Focusing on global descriptors, we provide empirical evidence of the interaction of all forms of attention and improve the state of the art on standard benchmarks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R35,
   title = {All the attention you need: Global-local, spatial-channel attention for image retrieval},
   author = {Song, Chull Hwan and Han, Hye Joo and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2107.08000},
   month = {7},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R34"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R34" id="tog-R34">
							<i class="left-60 tog far fa-chevron-down"></i>
							Tensor Feature Hallucination for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, T. Stathaki, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2106.05321" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R034.2106.05321v2.few-gen.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R34.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R034.2106.05321v2.few-gen.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2106-05321" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R34">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2106.05321</em>  <span class="bull"></span> Jun 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R034.2106.05321.few-gen.png"><img src="../data/pub/thumb/wide/report/R034.2106.05321.few-gen.png" alt="R34 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot classification addresses the challenge of classifying examples given not just limited supervision but limited data as well. An attractive solution is synthetic data generation. However, most such methods are overly sophisticated, focusing on high-quality, realistic data in the input space. It is unclear whether adapting them to the few-shot regime and using them for the downstream task of classification is the right approach. Previous works on synthetic data generation for few-shot classification focus on exploiting complex models, e.g. a Wasserstein GAN with multiple regularizers or a network that transfers latent diversities from known to novel classes.
								</p>
								<p>
									We follow a different approach and investigate how a simple and straightforward synthetic data generation method can be used effectively. We make two contributions, namely we show that: (1) using a simple loss function is more than enough for training a feature generator in the few-shot setting; and (2) learning to generate tensor features instead of vector features is superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show that our method sets a new state of the art, outperforming more sophisticated few-shot data augmentation methods.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R34,
   title = {Tensor feature hallucination for few-shot learning},
   author = {Lazarou, Michalis and Stathaki, Tania and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2106.05321},
   month = {6},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R33"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R33" id="tog-R33">
							<i class="left-60 tog far fa-chevron-down"></i>
							It Takes Two to Tango: Mixup for Deep Metric Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, B. Psomas, Y. Avrithis, E. Kijak, L. Amsaleg, K. Karantzalos</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2106.04990" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R033.2106.04990v2.metrix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R33.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R033.2106.04990v2.metrix.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2106-04990" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:HeT0ZceujKMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11528364689956817661" title="Citations @ Google Scholar">25</a>
						</div>

					</div>
					<div class="collapse" id="col-R33">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2106.04990</em>  <span class="bull"></span> Jun 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R033.2106.04990.metrix.svg"><img src="../data/pub/thumb/wide/report/R033.2106.04990.metrix.svg" alt="R33 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Metric learning involves learning a discriminative representation such that embeddings of similar classes are encouraged to be close, while embeddings of dissimilar classes are pushed far apart. State-of-the-art methods focus mostly on sophisticated loss functions or mining strategies. On the one hand, metric learning losses consider two or more examples at a time. On the other hand, modern data augmentation methods for classification consider two or more examples at a time. The combination of the two ideas is under-studied.
								</p>
								<p>
									In this work, we aim to bridge this gap and improve representations using mixup, which is a powerful data augmentation approach interpolating two or more examples and corresponding target labels at a time. This task is challenging because, unlike classification, the loss functions used in metric learning are not additive over examples, so the idea of interpolating target labels is not straightforward. To the best of our knowledge, we are the first to investigate mixing examples and target labels for deep metric learning. We develop a generalized formulation that encompasses existing metric learning loss functions and modify it to accommodate for mixup, introducing Metric Mix, or Metrix. We show that mixing inputs, intermediate representations or embeddings along with target labels significantly improves representations and outperforms state-of-the-art metric learning methods on four benchmark datasets.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R33,
   title = {It Takes Two to Tango: Mixup for Deep Metric Learning},
   author = {Venkataramanan, Shashanka and Psomas, Bill and Avrithis, Yannis and Kijak, Ewa and Amsaleg, Laurent and Karantzalos, Konstantinos},
   journal = {arXiv preprint arXiv:2106.04990},
   month = {6},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R32"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R32" id="tog-R32">
							<i class="left-60 tog far fa-chevron-down"></i>
							Few-Shot Learning via Tensor Hallucination
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2104.09467" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R032.2104.09467v1.few-gen.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R32.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R032.2104.09467v1.few-gen.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2104-09467" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:fFSKOagxvKUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=597316785097767982" title="Citations @ Google Scholar">5</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/c2439182fc408a590f70da78560da2e0b2b925ab" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/c2439182fc408a590f70da78560da2e0b2b925ab#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-R32">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2104.09467</em>  <span class="bull"></span> Apr 2021
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Few-shot classification addresses the challenge of classifying examples given only limited labeled data. A powerful approach is to go beyond data augmentation, towards data synthesis. However, most of data augmentation/synthesis methods for few-shot classification are overly complex and sophisticated, e.g. training a wGAN with multiple regularizers or training a network to transfer latent diversities from known to novel classes. We make two contributions, namely we show that: (1) using a simple loss function is more than enough for training a feature generator in the few-shot setting; and (2) learning to generate tensor features instead of vector features is superior. Extensive experiments on miniImagenet, CUB and CIFAR-FS datasets show that our method sets a new state of the art, outperforming more sophisticated few-shot data augmentation methods.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R32,
   title = {Few-shot learning via tensor hallucination},
   author = {Lazarou, Michalis and Avrithis, Yannis and Stathaki, Tania},
   journal = {arXiv preprint arXiv:2104.09467},
   month = {4},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R31"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R31" id="tog-R31">
							<i class="left-60 tog far fa-chevron-down"></i>
							AlignMixup: Improving Representations by Interpolating Aligned Features
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Venkataramanan, Y. Avrithis, E. Kijak, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2103.15375" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R031.2103.15375v2.alignmix.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R31.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R031.2103.15375v2.alignmix.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R31">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2103.15375</em>  <span class="bull"></span> Mar 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R031.2103.15375.mixup.svg"><img src="../data/pub/thumb/wide/report/R031.2103.15375.mixup.svg" alt="R31 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Mixup is a powerful data augmentation method that interpolates between two or more examples in the input or feature space and between the corresponding target labels. Many recent mixup methods focus on cutting and pasting two or more objects into one image, which is more about efficient processing than interpolation. However, how to best interpolate images is not well defined. In this sense, mixup has been connected to autoencoders, because often autoencoders "interpolate well", for instance generating an image that continuously deforms into another.
								</p>
								<p>
									In this work, we revisit mixup from the interpolation perspective and introduce AlignMix, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. Interestingly, this gives rise to a situation where mixup retains mostly the geometry or pose of one image and the texture of the other, connecting it to style transfer. More than that, we show that an autoencoder can still improve representation learning under mixup, without the classifier ever seeing decoded images. AlignMix outperforms state-of-the-art mixup methods on five different benchmarks.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R31,
   title = {{AlignMixup}: Improving representations by interpolating aligned features},
   author = {Venkataramanan, Shashanka and Avrithis, Yannis and Kijak, Ewa and Amsaleg, Laurent},
   journal = {arXiv preprint arXiv:2103.15375},
   month = {3},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R30"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R30" id="tog-R30">
							<i class="left-60 tog far fa-chevron-down"></i>
							On the Hidden Treasure of Dialog in Video Question Answering
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Engin, N.Q.K. Duong, F. Schnitzler, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2103.14517" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R030.2103.14517v2.vqa.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R30.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R030.2103.14517v2.vqa.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://engindeniz.github.io/dialogsummary-videoqa" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2103-14517" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R30">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2103.14517</em>  <span class="bull"></span> Mar 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R030.2103.14517.vqa.svg"><img src="../data/pub/thumb/wide/report/R030.2103.14517.vqa.svg" alt="R30 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									High-level understanding of stories in video such as movies and TV shows from raw data is extremely challenging. Modern video question answering (VideoQA) systems often use additional human-made sources like plot synopses, scripts, video descriptions or knowledge bases. In this work, we present a new approach to understand the whole story without such external sources. The secret lies in the dialog: unlike any prior work, we treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, and a simple fusion method combines all modalities, using soft temporal attention for localization over long inputs. Our model outperforms the state of the art on the KnowIT VQA dataset by a large margin, without using question-specific human annotation or human-made plot summaries. It even outperforms human evaluators who have never watched any whole episode before.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R30,
   title = {On the hidden treasure of dialog in video question answering},
   author = {Engin, Deniz and Duong, Ngoc Q. K. and Schnitzler, Fran\c{c}ois and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2103.14517},
   month = {3},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R29"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R29" id="tog-R29">
							<i class="left-60 tog far fa-chevron-down"></i>
							Local Propagation for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2101.01480" title="Open access">arXiv,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R029.2101.01480v1.few-local.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R29.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R029.2101.01480v1.few-local.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2101-01480" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R29">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2101.01480</em>  <span class="bull"></span> Jan 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R029.2101.01480.few-local.svg"><img src="../data/pub/thumb/wide/report/R029.2101.01480.few-local.svg" alt="R29 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The challenge in few-shot learning is that available data is not enough to capture the underlying distribution. To mitigate this, two emerging directions are (a) using local image representations, essentially multiplying the amount of data by a constant factor, and (b) using more unlabeled data, for instance by transductive inference, jointly on a number of queries. In this work, we bring these two ideas together, introducing local propagation. We treat local image features as independent examples, we build a graph on them and we use it to propagate both the features themselves and the labels, known and unknown. Interestingly, since there is a number of features per image, even a single query gives rise to transductive inference. As a result, we provide a universally safe choice for few-shot inference under both non-transductive and transductive settings, improving accuracy over corresponding methods. This is in contrast to existing solutions, where one needs to choose the method depending on the quantity of available data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R29,
   title = {Local propagation for few-shot learning},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine},
   journal = {arXiv preprint arXiv:2101.01480},
   month = {1},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2020"></a>
					2020
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R28"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R28" id="tog-R28">
							<i class="left-60 tog far fa-chevron-down"></i>
							Iterative Label Cleaning for Transductive and Semi-Supervised Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Lazarou, Y. Avrithis, T. Stathaki</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2012.07962" title="Open access">arXiv,&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R028.2012.07962v3.few-ss.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R28.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R028.2012.07962v3.few-ss.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2012-07962" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R28">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2012.07962</em>  <span class="bull"></span> Dec 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R028.2012.07962.few-ss.svg"><img src="../data/pub/thumb/wide/report/R028.2012.07962.few-ss.svg" alt="R28 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. These problems are closely related because there is little or no adaptation of the representation in novel tasks.
								</p>
								<p>
									Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iterately improving the quality of pseudo-labels. Our solution sets new state of the art on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R28,
   title = {Iterative label cleaning for transductive and semi-supervised few-shot learning},
   author = {Lazarou, Michalis and Avrithis, Yannis and Stathaki, Tania},
   journal = {arXiv preprint arXiv:2012.07962},
   month = {12},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R27" id="tog-R27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Asymmetric Metric Learning for Knowledge Transfer
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Budnik, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2006.16331" title="Open access">arXiv,&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R027.2006.16331v1.aml.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R27.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R027.2006.16331v1.aml.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2006-16331" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R27">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2006.16331</em>  <span class="bull"></span> Jun 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R027.2006.16331.mobile.svg"><img src="../data/pub/thumb/wide/report/R027.2006.16331.mobile.svg" alt="R27 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Knowledge transfer from large teacher models to smaller student models has recently been studied for metric learning, focusing on fine-grained classification. In this work, focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce asymmetric metric learning, a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task.
								</p>
								<p>
									We systematically evaluate different teacher and student models, metric learning and knowledge transfer loss functions on the new asymmetric testing as well as the standard symmetric testing task, where database and queries are represented by the same model. We find that plain regression is surprisingly effective compared to more complex knowledge transfer mechanisms, working best in asymmetric testing. Interestingly, our asymmetric metric learning approach works best in symmetric testing, allowing the student to even outperform the teacher.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R27,
   title = {Asymmetric metric learning for knowledge transfer},
   author = {Budnik, Mateusz and Avrithis, Yannis},
   journal = {arXiv preprint arXiv:2006.16331},
   month = {6},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R26"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R26" id="tog-R26">
							<i class="left-60 tog far fa-chevron-down"></i>
							Few-Shot Few-Shot Learning and the Role of Spatial Attention
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/2002.07522" title="Open access">arXiv,&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R026.2002.07522v1.few-att.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R26.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R026.2002.07522v1.few-att.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-2002-07522" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R26">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:2002.07522</em>  <span class="bull"></span> Feb 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R026.2002.07522.few-att.png"><img src="../data/pub/thumb/wide/report/R026.2002.07522.few-att.png" alt="R26 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Few-shot learning is often motivated by the ability of humans to learn new tasks from few examples. However, standard few-shot classification benchmarks assume that the representation is learned on a limited amount of base class data, ignoring the amount of prior knowledge that a human may have accumulated before learning new tasks. At the same time, even if a powerful representation is available, it may happen in some domain that base class data are limited or non-existent. This motivates us to study a problem where the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, assuming no access to its training process, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. We adapt the representation in two stages, namely on the few base class data if available and on the even fewer data of new tasks. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. This is important in the new problem, because when base class data are few, the network cannot learn where to focus implicitly. We also show that a pre-trained network may be easily adapted to novel classes, without meta-learning.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R26,
   title = {Few-Shot Few-Shot Learning and the role of Spatial Attention},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine},
   journal = {arXiv preprint arXiv:2002.07522},
   month = {2},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2019"></a>
					2019
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R25"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R25" id="tog-R25">
							<i class="left-60 tog far fa-chevron-down"></i>
							Walking on the Edge: Fast, Low-Distortion Adversarial Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, Y. Avrithis, T. Furon, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1912.02153" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R025.1912.02153v2.bp.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R25.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R025.1912.02153v2.bp.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1912-02153" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R25">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1912.02153</em>  <span class="bull"></span> Dec 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R025.1912.02153.bp.png"><img src="../data/pub/thumb/wide/report/R025.1912.02153.bp.png" alt="R25 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Adversarial examples of deep neural networks are receiving ever increasing attention because they help in understanding and reducing the sensitivity to their input. This is natural given the increasing applications of deep neural networks in our everyday lives. When white-box attacks are almost always successful, it is typically only the distortion of the perturbations that matters in their evaluation.
								</p>
								<p>
									In this work, we argue that speed is important as well, especially when considering that fast attacks are required by adversarial training. Given more time, iterative methods can always find better solutions. We investigate this speed-distortion trade-off in some depth and introduce a new attack called boundary projection (BP) that improves upon existing methods by a large margin. Our key idea is that the classification boundary is a manifold in the image space: we therefore quickly reach the boundary and then optimize distortion on this manifold.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R25,
   title = {Walking on the Edge: Fast, Low-Distortion Adversarial Examples},
   author = {Zhang, Hanwei and Avrithis, Yannis and Furon, Teddy and Amsaleg, Laurent},
   journal = {arXiv preprint arXiv:1912.02153},
   month = {12},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R24"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R24" id="tog-R24">
							<i class="left-60 tog far fa-chevron-down"></i>
							Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Z. Yang, M. Shi, Y. Avrithis, C. Xu, V. Ferrari</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1912.00384" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R024.1912.00384v6.nsod.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R24.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R024.1912.00384v6.nsod.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1912-00384" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R24">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1912.00384</em>  <span class="bull"></span> Dec 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R024.1912.00384.nsod.svg"><img src="../data/pub/thumb/wide/report/R024.1912.00384.nsod.svg" alt="R24 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set are available. In this work, we study the problem of training an object detector from one or few clean images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a classifier or detector. Our solution is to use a standard weakly-supervised pipeline to train a student model from image-level pseudo-labels generated on the unlabeled set by a teacher model, bootstrapped by region-level similarities to clean labeled images. By using the recent pipeline of PCL and more unlabeled images, we achieve performance competitive or superior to many state of the art weakly-supervised detection solutions.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R24,
   title = {Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images},
   author = {Yang, Zhaohui and Shi, Miaojing and Avrithis, Yannis and Xu, Chao and Ferrari, Vittorio},
   journal = {arXiv preprint arXiv:1912.00384},
   month = {12},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R23"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R23" id="tog-R23">
							<i class="left-60 tog far fa-chevron-down"></i>
							Rethinking Deep Active Learning: Using Unlabeled Data at Model Training
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, M. Budnik, Y. Avrithis, G. Gravier</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1911.08177" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R023.1911.08177v1.active.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R23.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R023.1911.08177v1.active.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1911-08177" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R23">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1911.08177</em>  <span class="bull"></span> Nov 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R023.1911.08177.active.svg"><img src="../data/pub/thumb/wide/report/R023.1911.08177.active.svg" alt="R23 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Active learning typically focuses on training a model on few labeled examples alone, while unlabeled ones are only used for acquisition. In this work we depart from this setting by using both labeled and unlabeled data during model training across active learning cycles. We do so by using unsupervised feature learning at the beginning of the active learning pipeline and semi-supervised learning at every active learning cycle, on all available data. The former has not been investigated before in active learning, while the study of latter in the context of deep learning is scarce and recent findings are not conclusive with respect to its benefit. Our idea is orthogonal to acquisition strategies by using more data, much like ensemble methods use more models. By systematically evaluating on a number of popular acquisition strategies and datasets, we find that the use of unlabeled data during model training brings a spectacular accuracy improvement in image classification, compared to the differences between acquisition strategies. We thus explore smaller label budgets, even one label per class.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R23,
   title = {Rethinking deep active learning: Using unlabeled data at model training},
   author = {Sim\'eoni, Oriane and Budnik, Mateusz and Avrithis, Yannis and Gravier, Guillaume},
   journal = {arXiv preprint arXiv:1911.08177},
   month = {11},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R22"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R22" id="tog-R22">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph Convolutional Networks for Learning with Few Clean and Many Noisy Labels
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum, C. Schmid</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1910.00324" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R022.1910.00324v3.few-noisy.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R22.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R022.1910.00324v3.few-noisy.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1910-00324" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R22">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1910.00324</em>  <span class="bull"></span> Oct 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R022.1910.00324.few-noisy.svg"><img src="../data/pub/thumb/wide/report/R022.1910.00324.few-noisy.svg" alt="R22 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this work we consider the problem of learning a classifier from noisy labels when a few clean labeled examples are given. The structure of clean and noisy data is modeled by a graph per class and Graph Convolutional Networks (GCN) are used to predict class relevance of noisy examples. For each class, the GCN is treated as a binary classifier learning to discriminate clean from noisy examples using a weighted binary cross-entropy loss function, and then the GCN-inferred "clean" probability is exploited as a relevance measure. Each noisy example is weighted by its relevance when learning a classifier for the end task. We evaluate our method on an extended version of a few-shot learning problem, where the few clean examples of novel classes are supplemented with additional noisy data. Experimental results show that our GCN-based cleaning process significantly improves the classification accuracy over not cleaning the noisy data and standard few-shot classification where only few clean examples are used. The proposed GCN-based method outperforms the transductive approach (Douze et al., 2018) that is using the same additional data without labels.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R22,
   title = {Graph convolutional networks for learning with few clean and many noisy labels},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej and Schmid, Cordelia},
   journal = {arXiv preprint arXiv:1910.00324},
   month = {10},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R21"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R21" id="tog-R21">
							<i class="left-60 tog far fa-chevron-down"></i>
							Local Features and Visual Words Emerge in Activations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1905.06358" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R021.1905.06358v1.spatial.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R21.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R021.1905.06358v1.spatial.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1905-06358" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R21">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1905.06358</em>  <span class="bull"></span> May 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R021.1905.06358.spatial.svg"><img src="../data/pub/thumb/wide/report/R021.1905.06358.spatial.svg" alt="R21 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We propose a novel method of deep spatial matching (DSM) for image retrieval. Initial ranking is based on image descriptors extracted from convolutional neural network activations by global pooling, as in recent state-of-the-art work. However, the same sparse 3D activation tensor is also approximated by a collection of local features. These local features are then robustly matched to approximate the optimal alignment of the tensors. This happens without any network modification, additional layers or training. No local feature detection happens on the original image. No local feature descriptors and no visual vocabulary are needed throughout the whole process.
								</p>
								<p>
									We experimentally show that the proposed method achieves the state-of-the-art performance on standard benchmarks across different network architectures and different global pooling methods. The highest gain in performance is achieved when diffusion on the nearest-neighbor graph of global descriptors is initiated from spatially verified images.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R21,
   title = {Local Features and Visual Words Emerge in Activations},
   author = {Sim\'eoni, Oriane and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1905.06358},
   month = {5},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R20"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R20" id="tog-R20">
							<i class="left-60 tog far fa-chevron-down"></i>
							Label Propagation for Deep Semi-Supervised Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1904.04717" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R020.1904.04717v1.semi.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R20.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R020.1904.04717v1.semi.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1904-04717" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R20">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1904.04717</em>  <span class="bull"></span> Apr 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R020.1904.04717.semi.png"><img src="../data/pub/thumb/wide/report/R020.1904.04717.semi.png" alt="R20 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Semi-supervised learning is becoming increasingly important because it can combine data carefully labeled by humans with abundant unlabeled data to train deep neural networks. Classic methods on semi-supervised learning that have focused on transductive learning have not been fully exploited in the inductive framework followed by modern deep learning. The same holds for the manifold assumption---that similar examples should get the same prediction. In this work, we employ a transductive label propagation method that is based on the manifold assumption to make predictions on the entire dataset and use these predictions to generate pseudo-labels for the unlabeled data and train a deep neural network. At the core of the transductive method lies a nearest neighbor graph of the dataset that we create based on the embeddings of the same network.Therefore our learning process iterates between these two steps. We improve performance on several datasets especially in the few labels regime and show that our work is complementary to current state of the art.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R20,
   title = {Label propagation for Deep Semi-supervised Learning},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1904.04717},
   month = {4},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R19"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R19" id="tog-R19">
							<i class="left-60 tog far fa-chevron-down"></i>
							Smooth Adversarial Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, Y. Avrithis, T. Furon, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1903.11862" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R019.1903.11862v1.smooth.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R19.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R019.1903.11862v1.smooth.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1903-11862" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R19">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1903.11862</em>  <span class="bull"></span> Mar 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R019.1903.11862.smooth.svg"><img src="../data/pub/thumb/wide/report/R019.1903.11862.smooth.svg" alt="R19 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper investigates the visual quality of the adversarial examples. Recent papers propose to smooth the perturbations to get rid of high frequency artefacts. In this work, smoothing has a different meaning as it perceptually shapes the perturbation according to the visual content of the image to be attacked. The perturbation becomes locally smooth on the flat areas of the input image, but it may be noisy on its textured areas and sharp across its edges.
								</p>
								<p>
									This operation relies on Laplacian smoothing, well-known in graph signal processing, which we integrate in the attack pipeline. We benchmark several attacks with and without smoothing under a white-box scenario and evaluate their transferability. Despite the additional constraint of smoothness, our attack has the same probability of success at lower distortion.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R19,
   title = {Smooth Adversarial Examples},
   author = {Zhang, Hanwei and Avrithis, Yannis and Furon, Teddy and Amsaleg, Laurent},
   journal = {arXiv preprint arXiv:1903.11862},
   month = {3},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R18"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R18" id="tog-R18">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dense Classification and Implanting for Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Lifchitz, Y. Avrithis, S. Picard, A. Bursuc</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1903.05050" title="Open access">arXiv,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R018.1903.05050v1.few.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R18.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R018.1903.05050v1.few.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1903-05050" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R18">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1903.05050</em>  <span class="bull"></span> Mar 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R018.1903.05050.few.svg"><img src="../data/pub/thumb/wide/report/R018.1903.05050.few.svg" alt="R18 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Training deep neural networks from few examples is a highly challenging and key problem for many computer vision tasks. In this context, we are targeting knowledge transfer from a set with abundant data to other sets with few available examples. We propose two simple and effective solutions: (i) dense classification over feature maps, which for the first time studies local activations in the domain of few-shot learning, and (ii) implanting, that is, attaching new neurons to a previously trained network to learn new, task-specific features. On miniImageNet, we improve the prior state-of-the-art on few-shot classification, i.e., we achieve 62.5%, 79.8% and 83.8% on 5-way 1-shot, 5-shot and 10-shot settings respectively.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R18,
   title = {Dense Classification and Implanting for Few-Shot Learning},
   author = {Lifchitz, Yann and Avrithis, Yannis and Picard, Sylvaine and Bursuc, Andrei},
   journal = {arXiv preprint arXiv:1903.05050},
   month = {3},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2018"></a>
					2018
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R17"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R17" id="tog-R17">
							<i class="left-60 tog far fa-chevron-down"></i>
							Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, Y. Avrithis, G. Tolias, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1807.08692" title="Open access">arXiv,&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R017.1807.08692v2.hybrid.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R17.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R017.1807.08692v2.hybrid.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1807-08692" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R17">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1807.08692</em>  <span class="bull"></span> Jul 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R017.1807.08692.hybrid.svg"><img src="../data/pub/thumb/wide/report/R017.1807.08692.hybrid.svg" alt="R17 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									State of the art image retrieval performance is achieved with CNN features and manifold ranking using a k-NN similarity graph that is pre-computed off-line. The two most successful existing approaches are temporal filtering, where manifold ranking amounts to solving a sparse linear system online, and spectral filtering, where eigen-decomposition of the adjacency matrix is performed off-line and then manifold ranking amounts to dot-product search online. The former suffers from expensive queries and the latter from significant space overhead. Here we introduce a novel, theoretically well-founded hybrid filtering approach allowing full control of the space-time trade-off between these two extremes. Experimentally, we verify that our hybrid method delivers results on par with the state of the art, with lower memory demands compared to spectral filtering approaches and faster compared to temporal filtering.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R17,
   title = {Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking},
   author = {Iscen, Ahmet and Avrithis, Yannis and Tolias, Giorgos and Furon, Teddy and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1807.08692},
   month = {7},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R16"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R16" id="tog-R16">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">F. Radenovi, A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1803.11285" title="Open access">arXiv,&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R016.1803.11285v1.oxford.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R16.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R016.1803.11285v1.oxford.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://cmp.felk.cvut.cz/revisitop/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1803-11285" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R16">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1803.11285</em>  <span class="bull"></span> Mar 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R016.1803.11285.oxford.svg"><img src="../data/pub/thumb/wide/report/R016.1803.11285.oxford.svg" alt="R16 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this paper we address issues with image retrieval benchmarking on standard and popular Oxford 5k and Paris 6k datasets. In particular, annotation errors, the size of the dataset, and the level of challenge are addressed: new annotation for both datasets is created with an extra attention to the reliability of the ground truth. Three new protocols of varying difficulty are introduced. The protocols allow fair comparison between different methods, including those using a dataset pre-processing stage. For each dataset, 15 new challenging queries are introduced. Finally, a new set of 1M hard, semi-automatically cleaned distractors is selected.
								</p>
								<p>
									An extensive comparison of the state-of-the-art methods is performed on the new benchmark. Different types of methods are evaluated, ranging from local-feature-based to modern CNN based methods. The best results are achieved by taking the best of the two worlds. Most importantly, image retrieval appears far from being solved.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R16,
   title = {Revisiting Oxford and Paris: Large-Scale Image Retrieval Benchmarking},
   author = {Radenovi\'c, Filip and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1803.11285},
   month = {3},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R15"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R15" id="tog-R15">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mining on Manifolds: Metric Learning without Labels
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1803.11095" title="Open access">arXiv,&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R015.1803.11095v1.mom.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R15.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R015.1803.11095v1.mom.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/abs-1803-11095" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R15">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1803.11095</em>  <span class="bull"></span> Mar 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R015.1803.11095.mom.svg"><img src="../data/pub/thumb/wide/report/R015.1803.11095.mom.svg" alt="R15 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In this work we present a novel unsupervised framework for hard training example mining. The only input to the method is a collection of images relevant to the target application and a meaningful initial representation, provided e.g. by pre-trained CNN. Positive examples are distant points on a single manifold, while negative examples are nearby points on different manifolds. Both types of examples are revealed by disagreements between Euclidean and manifold similarities. The discovered examples can be used in training with any discriminative loss. The method is applied to unsupervised fine-tuning of pre-trained networks for fine-grained classification and particular object retrieval. Our models are on par or are outperforming prior models that are fully or partially supervised.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R15,
   title = {Mining on Manifolds: Metric Learning without Labels},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1803.11095},
   month = {3},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R14"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R14" id="tog-R14">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unsupervised Object Discovery for Instance Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://arxiv.org/abs/1709.04725" title="Open access">arXiv,&nbsp;2018</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R014.1709.04725v2.disco.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R14.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R014.1709.04725v2.disco.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R14">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1709.04725</em>  <span class="bull"></span> Sep 2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R014.1709.04725.disco.svg"><img src="../data/pub/thumb/wide/report/R014.1709.04725.disco.svg" alt="R14 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Severe background clutter is challenging in many computer vision tasks, including large-scale image retrieval. Global descriptors, that are popular due to their memory and search efficiency, are especially prone to corruption by such a clutter. Eliminating the impact of the clutter on the image descriptor increases the chance of retrieving relevant images and prevents topic drift due to actually retrieving the clutter in the case of query expansion. In this work, we propose a novel salient region detection method. It captures, in an unsupervised manner, patterns that are both discriminative and common in the dataset. Saliency is based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. The descriptors derived from the salient regions improve particular object retrieval, most noticeably in a large collections containing small objects.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R14,
   title = {Unsupervised object discovery for instance recognition},
   author = {Sim\'eoni, Oriane and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1709.04725},
   month = {9},
   year = {2018}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2017"></a>
					2017
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R13"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R13" id="tog-R13">
							<i class="left-60 tog far fa-chevron-down"></i>
							Panorama to Panorama Matching for Location Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1704.06591" title="Open access">arXiv,&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R013.1704.06591v1.pano.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R13.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R013.1704.06591v1.pano.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/IscenTAFC17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R13">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1704.06591</em>  <span class="bull"></span> Apr 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R013.1704.06591.pano.png"><img src="../data/pub/thumb/wide/report/R013.1704.06591.pano.png" alt="R13 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Location recognition is commonly treated as visual instance retrieval on "street view" imagery. The dataset items and queries are panoramic views, i.e. groups of images taken at a single location. This work introduces a novel panorama-to-panorama matching process, either by aggregating features of individual images in a group or by explicitly constructing a larger panorama. In either case, multiple views are used as queries. We reach near perfect location recognition on a standard benchmark with only four query views.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R13,
   title = {Panorama to panorama matching for location recognition},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Furon, Teddy and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1704.06591},
   month = {4},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R12"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R12" id="tog-R12">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unsupervised Part Learning for Visual Recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">R. Sicre, Y. Avrithis, E. Kijak, F. Jurie</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1704.03755" title="Open access">arXiv,&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R012.1704.03755v1.uparts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R12.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R012.1704.03755v1.uparts.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/SicreAKJ17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R12">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1704.03755</em>  <span class="bull"></span> Apr 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R012.1704.03755.uparts.svg"><img src="../data/pub/thumb/wide/report/R012.1704.03755.uparts.svg" alt="R12 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Part-based image classification aims at representing categories by small sets of learned discriminative parts, upon which an image representation is built. Considered as a promising avenue a decade ago, this direction has been neglected since the advent of deep neural networks. In this context, this paper brings two contributions: first, it shows that despite the recent success of end-to-end holistic models, explicit part learning can boosts classification performance. Second, this work proceeds one step further than recent part-based models (PBM), focusing on how to learn parts without using any labeled data. Instead of learning a set of parts per class, as generally done in the PBM literature, the proposed approach both constructs a partition of a given set of images into visually similar groups, and subsequently learn a set of discriminative parts per group in a fully unsupervised fashion. This strategy opens the door to the use of PBM in new applications for which the notion of image categories is irrelevant, such as instance-based image retrieval, for example. We experimentally show that our learned parts can help building efficient image representations, for classification as well as for indexing tasks, resulting in performance superior to holistic state-of-the art Deep Convolutional Neural Networks (DCNN) encoding.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R12,
   title = {Unsupervised part learning for visual recognition},
   author = {Sicre, Ronan and Avrithis, Yannis and Kijak, Ewa and Jurie, Fr\'ed\'eric},
   journal = {arXiv preprint arXiv:1704.03755},
   month = {4},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R11"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R11" id="tog-R11">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fast Spectral Ranking for Similarity Search
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, Y. Avrithis, G. Tolias, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1703.06935" title="Open access">arXiv,&nbsp;2017</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R011.1703.06935v3.fsr.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R11.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R011.1703.06935v3.fsr.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/IscenATFC17" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ruyezt5ZtCIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=8836572710472893482" title="Citations @ Google Scholar">57</a>
						</div>

					</div>
					<div class="collapse" id="col-R11">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1703.06935</em>  <span class="bull"></span> Mar 2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R011.1703.06935.fsr.svg"><img src="../data/pub/thumb/wide/report/R011.1703.06935.fsr.svg" alt="R11 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Despite the success of deep learning on representing images for particular object retrieval, recent studies show that the learned representations still lie on manifolds in a high dimensional space. Therefore, nearest neighbor search cannot be expected to be optimal for this task. Even if a nearest neighbor graph is computed offline, exploring the manifolds online remains expensive. This work introduces an explicit embedding reducing manifold search to Euclidean search followed by dot product similarity search. We show this is equivalent to linear graph filtering of a sparse signal in the frequency domain, and we introduce a scalable offline computation of an approximate Fourier basis of the graph. We improve the state of art on standard particular object retrieval datasets including a challenging one containing small objects. At a scale of 10^5 images, the offline cost is only a few hours, while query time is comparable to standard similarity search.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R11,
   title = {Fast Spectral Ranking for Similarity Search},
   author = {Iscen, Ahmet and Avrithis, Yannis and Tolias, Giorgos and Furon, Teddy and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1703.06935},
   month = {3},
   year = {2017}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2016"></a>
					2016
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R10"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R10" id="tog-R10">
							<i class="left-60 tog far fa-chevron-down"></i>
							Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact CNN Representations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Iscen, G. Tolias, Y. Avrithis, T. Furon, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1611.05113" title="Open access">arXiv,&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R010.1611.05113v3.diffusion.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R10.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R010.1611.05113v3.diffusion.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://cmp.felk.cvut.cz/~iscenahm/_pages/diffusion.html" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/IscenTAFC16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R10">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1611.05113</em>  <span class="bull"></span> Nov 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R010.1611.05113.diffusion.svg"><img src="../data/pub/thumb/wide/report/R010.1611.05113.diffusion.svg" alt="R10 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Query expansion is a popular method to improve the quality of image retrieval with both conventional and CNN representations. It has been so far limited to global image similarity. This work focuses on diffusion, a mechanism that captures the image manifold in the feature space. The diffusion is carried out on descriptors of overlapping image regions rather than on a global image descriptor like in previous approaches. An efficient off-line stage allows optional reduction in the number of stored regions. In the on-line stage, the proposed handling of unseen queries in the indexing stage removes additional computation to adjust the precomputed data. A novel way to perform diffusion through a sparse linear system solver yields practical query times well below one second. Experimentally, we observe a significant boost in performance of image retrieval with compact CNN descriptors on standard benchmarks, especially when the query object covers only a small part of the image. Small objects have been a common failure case of CNN-based retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R10,
   title = {Efficient Diffusion on Region Manifolds: Recovering Small Objects with Compact {CNN} Representations},
   author = {Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Furon, Teddy and Chum, Ond\v{r}ej},
   journal = {arXiv preprint arXiv:1611.05113},
   month = {11},
   year = {2016}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R9"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R9" id="tog-R9">
							<i class="left-60 tog far fa-chevron-down"></i>
							Automatic Discovery of Discriminative Parts as a Quadratic Assignment Problem
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">R. Sicre, J. Rabin, Y. Avrithis, T. Furon, F. Jurie</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1611.04413" title="Open access">arXiv,&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R009.1611.04413v1.parts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R9.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R009.1611.04413v1.parts.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/SicreRAFJ16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R9">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1611.04413</em>  <span class="bull"></span> Nov 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R009.1611.04413.parts.svg"><img src="../data/pub/thumb/wide/report/R009.1611.04413.parts.svg" alt="R9 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Part-based image classification consists in representing categories by small sets of discriminative parts upon which a representation of the images is built. This paper addresses the question of how to automatically learn such parts from a set of labeled training images. The training of parts is cast as a quadratic assignment problem in which optimal correspondences between image regions and parts are automatically learned. The paper analyses different assignment strategies and thoroughly evaluates them on two public datasets: Willow actions and MIT 67 scenes. State-of-the art results are obtained on these datasets.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R9,
   title = {Automatic Discovery of Discriminative Parts As a Quadratic Assignment Problem},
   author = {Sicre, Ronan and Rabin, Julien and Avrithis, Yannis and Furon, Teddy and Jurie, Fr\'ed\'eric},
   journal = {arXiv preprint arXiv:1611.04413},
   month = {11},
   year = {2016}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R8"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R8" id="tog-R8">
							<i class="left-60 tog far fa-chevron-down"></i>
							High-Dimensional Approximate Nearest Neighbor: k-d Generalized Randomized Forests
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, I. Emiris, G. Samaras</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://arxiv.org/abs/1603.09596" title="Open access">arXiv,&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R008.1603.09596v1.geraf.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R8.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/src/report/R008.1603.09596v1.geraf.tar.gz" title="Paper source">
								<i class="fal fa-file-zipper"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/corr/AvrithisES16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:M7yex6snE4oC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1381654625996490639" title="Citations @ Google Scholar">3</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6e822391351c3e15f16b8e37f6c9728946571603" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6e822391351c3e15f16b8e37f6c9728946571603#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-R8">
						<div class="pub-ref">
							<em>arXiv preprint arXiv:1603.09596</em>  <span class="bull"></span> Mar 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/report/R008.1603.09596.geraf.svg"><img src="../data/pub/thumb/wide/report/R008.1603.09596.geraf.svg" alt="R8 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We propose a new data-structure, the generalized randomized k-d forest, or k-d GeRaF, for approximate nearest neighbor searching in high dimensions. In particular, we introduce new randomization techniques to specify a set of independently constructed trees where search is performed simultaneously, hence increasing accuracy. We omit backtracking, and we optimize distance computations, thus accelerating queries. We release public domain software GeRaF and we compare it to existing implementations of state-of-the-art methods including BBD-trees, Locality Sensitive Hashing, randomized k-d forests, and product quantization. Experimental results indicate that our method would be the method of choice in dimensions around 1,000, and probably up to 10,000, and pointsets of cardinality up to a few hundred thousands or even one million; this range of inputs is encountered in many critical applications today. For instance, we handle a real dataset of 10^6 images represented in 960 dimensions with a query time of less than 1sec on average and 90% responses being true nearest neighbors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{R8,
   title = {High-Dimensional Approximate Nearest Neighbor: {k-d} Generalized Randomized Forests},
   author = {Avrithis, Yannis and Emiris, Ioannis and Samaras, Georgios},
   journal = {arXiv preprint arXiv:1603.09596},
   month = {3},
   year = {2016}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2008"></a>
					2008
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R7"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R7" id="tog-R7">
							<i class="left-60 tog far fa-chevron-down"></i>
							K-Space At TRECVID 2008
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/kspace.pdf" title="Electronic edition">TRECVID,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R007.trecvid08a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R7.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/WilkinsBJLKMOOSATABDHMTSAPMKMSBCIGHSCSPHHHVPGJ08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R7">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we describe K-Space's participation in TRECVid 2008 in the interactive search task. For 2008 the K-Space group performed one of the largest interactive video information retrieval experiments conducted in a laboratory setting. We had three institutions participating in a multi-site multi-system experiment. In total 36 users participated, 12 each from Dublin City University (DCU, Ireland), University of Glasgow (GU, Scotland) and Centrum Wiskunde & Informatica (CWI, the Netherlands). Three user interfaces were developed, two from DCU which were also used in 2007 as well as an interface from GU. All interfaces leveraged the same search service. Using a latin squares arrangement, each user conducted 12 topics, leading in total to 6 runs per site, 18 in total. We officially submitted for evaluation 3 of these runs to NIST with an additional expert run using a 4th system. Our submitted runs performed around the median. In this paper we will present an overview of the search system utilized, the experimental setup and a preliminary analysis of our results.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R7,
   title = {{K-Space} at {TRECVID} 2008},
   author = {Tolias, Giorgos and Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 6th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R6"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R6" id="tog-R6">
							<i class="left-60 tog far fa-chevron-down"></i>
							COST292 Experimental Framework for TRECVID 2008
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, E. Spyrou, P. Kapsalas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://www-nlpir.nist.gov/projects/tvpubs/tv8.papers/cost292.pdf" title="Electronic edition">TRECVID,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R006.trecvid08b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R6.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/ZhangCDPTAKMSMBSEAAGPACGMFPRJPVMKNDMMKNH08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R6">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, we give an overview of the four tasks submitted to TRECVID 2008 by COST292. The high-level feature extraction framework comprises four systems. The first system transforms a set of low-level descriptors into the semantic space using Latent Semantic Analysis and utilises neural networks for feature detection. The second system uses a multi-modal classifier based on SVMs and several descriptors. The third system uses three image classifiers based on ant colony optimisation, particle swarm optimisation and a multi-objective learning algorithm.The fourth system uses a Gaussian model for singing detection and a person detection algorithm. The search task is based on an interactive retrieval application combining retrieval functionalities in various modalities with a user interface supporting automatic and interactive search over all queries submitted. The rushes task submission is based on a spectral clustering approach for removing similar scenes based on eigenvalues of frame similarity matrix and and a redundancy removal strategy which depends on semantic features extraction such as camera motion and faces. Finally, the submission to the copy detection task is conducted by two different systems. The first system consists of a video module and an audio module. The second system is based on mid-level features that are related to the temporal structure of videos.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R6,
   title = {{COST292} experimental framework for {TRECVID} 2008},
   author = {Tolias, Giorgos and Spyrou, Evaggelos and Kapsalas, Petros and Avrithis, Yannis},
   booktitle = {Proceedings of 6th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2007"></a>
					2007
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R5"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R5" id="tog-R5">
							<i class="left-60 tog far fa-chevron-down"></i>
							K-Space At TRECVID 2007
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/kspace.pdf" title="Electronic edition">TRECVID,&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R005.trecvid07a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R5.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/WilkinsABJLKMOSAOBGHELVPVMKSAMSBPCIHGSCSP07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R5">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we describe K-Space participation in TRECVid 2007. K-Space participated in two tasks, high-level feature extraction and interactive search. We present our approaches for each of these activities and provide a brief analysis of our results. Our high-level feature submission utilized multi-modal low-level features which included visual, audio and temporal elements. Specific concept detectors (such as Face detectors) developed by K-Space partners were also used. We experimented with different machine learning approaches including logistic regression and support vector machines (SVM). Finally we also experimented with both early and late fusion for feature combination. This year we also participated in interactive search, submitting 6 runs. We developed two interfaces which both utilized the same retrieval functionality. Our objective was to measure the eect of context, which was supported to different degrees in each interface, on user performance. The first of the two systems was a 'shot' based interface, where the results from a query were presented as a ranked list of shots. The second interface was 'broadcast' based, where results were presented as a ranked list of broadcasts. Both systems made use of the outputs of our high-level feature submission as well as low-level visual features.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R5,
   title = {{K-Space} at {TRECVID} 2007},
   author = {Spyrou, Evaggelos and Avrithis, Yannis},
   booktitle = {Proceedings of 5th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R4"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R4" id="tog-R4">
							<i class="left-60 tog far fa-chevron-down"></i>
							The COST292 Experimental Framework for TRECVID 2007
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, P. Kapsalas, G. Tolias, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://www-nlpir.nist.gov/projects/tvpubs/tv7.papers/cost292.pdf" title="Electronic edition">TRECVID,&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R004.trecvid07b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R4.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/ZhangCDPICALYANHVMNMMKMBEASKTMARZPAAJKAG07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R4">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, we give an overview of the four tasks submitted to TRECVID 2007 by COST292. In shot boundary (SB) detection task, four SB detectors have been developed and the results are merged using two merging algorithms. The framework developed for the high-level feature extraction task comprises four systems. The first system transforms a set of low-level descriptors into the semantic space using Latent Semantic Analysis and utilises neural networks for feature detection. The second system uses a Bayesian classifier trained with a "bag of subregions". The third system uses a multi-modal classifier based on SVMs and several descriptors. The fourth system uses two image classifiers based on ant colony optimisation and particle swarm optimisation respectively. The system submitted to the search task is an interactive retrieval application combining retrieval functionalities in various modalities with a user interface supporting automatic and interactive search over all queries submitted. Finally, the rushes task submission is based on a video summarisation and browsing system comprising two different interest curve algorithms and three features.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R4,
   title = {The {COST292} experimental framework for {TRECVID} 2007},
   author = {Spyrou, Evaggelos and Kapsalas, Petros and Tolias, Giorgos and Mylonas, Phivos and Avrithis, Yannis},
   booktitle = {Proceedings of 5th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-2006"></a>
					2006
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R3" id="tog-R3">
							<i class="left-60 tog far fa-chevron-down"></i>
							K-Space At TRECVID 2006
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Koumoulos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/k-space.pdf" title="Electronic edition">TRECVID,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R003.trecvid06a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/WilkinsAFHJKMMO06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R3">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we describe the K-Space participation in TRECVid 2006. K-Space participated in two tasks, high-level feature extraction and search. We present our approaches for each of these activities and provide a brief analysis of our results. Our high-level feature submission made use of support vector machines (SVMs) created with low-level MPEG-7 visual features, fused with specific concept detectors. Search submissions were both manual and automatic and made use of both low- and high-level features. In the high-level feature extraction submission, four of our six runs achieved performance above the TRECVid median, whilst our search submission performed around the median. The K-Space team consisted of eight partner institutions from the EU-funded K-Space Network, and our submissions made use of tools and techniques from each partner. As such this paper will provide overviews of each partner's contributions and provide appropriate references for specific descriptions of individual components.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R3,
   title = {{K-Space} at {TRECVID} 2006},
   author = {Spyrou, Evaggelos and Koumoulos, George and Avrithis, Yannis},
   booktitle = {Proceedings of 4th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R2" id="tog-R2">
							<i class="left-60 tog far fa-chevron-down"></i>
							COST292 Experimental Framework for TRECVID 2006
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Koumoulos, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www-nlpir.nist.gov/projects/tvpubs/tv6.papers/cost292.pdf" title="Electronic edition">TRECVID,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/report/R002.trecvid06b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/trecvid/CalicKNVAZBSDJC06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-R2">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we give an overview of the four TRECVID tasks submitted by COST292, European network of institutions in the area of semantic multimodal analysis and retrieval of digital video media. Initially,we present shot boundary evaluation method based on results merged using a confidence measure. The two SB detectors user here are presented, one of the Technical University of Delft and one of the LaBRI, University of Bordeaux 1, followed by the description of the merging algorithm. The high-level feature extraction task comprises three separate systems. The first system, developed by the National Technical University of Athens (NTUA) utilises a set of MPEG-7 low-level descriptors and Latent Semantic Analysis to detect the features. The second system, developed by Bilkent University, uses a Bayesian classifier trained with a "bag of subregions" for each keyframe. The third system by the Middle East Technical University (METU) exploits textual information in the video using character recognition methodology. The system submitted to the search task is an interactive retrieval application developed by Queen Mary, University of London, University of Zilina and ITI from Thessaloniki, combining basic retrieval functionalities in various modalities (i.e. visual, audio, textual) with a user interface supporting the submission of queries using any combination of the available retrieval tools and the accumulation of relevant retrieval results over all queries submitted by a single user during a specified time interval. Finally, the rushes task submission comprises a video summarisation and browsing system specifically designed to intuitively and efficiently presents rushes material in video production environment. This system is a result of joint work of University of Bristol, Technical University of Delft and LaBRI, University of Bordeaux 1.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@inproceedings{R2,
   title = {{COST292} experimental framework for {TRECVID} 2006},
   author = {Spyrou, Evaggelos and Koumoulos, George and Avrithis, Yannis},
   booktitle = {Proceedings of 4th TRECVID Workshop},
   month = {11},
   address = {Gaithersburg, USA},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="report-1999"></a>
					1999
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="R1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-R1" id="tog-R1">
							<i class="left-60 tog far fa-chevron-down"></i>
							Verification Report of Core Experiment on Fast Block-Matching Motion Estimation Using Advanced Diamond Zonal Search with Embedded Radar
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tsechpenakis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">ISO/IEC,&nbsp;1999</span>
							<a class="lnk mr" href="../data/pub/pdf/report/R001.mpeg99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/report/R1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-R1">
						<div class="pub-ref">
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Motion Estimation (ME) is an important part of the MPEG-4 encoder, since it could significantly affect the output quality of the encoded sequence. Unfortunately this feature requires a significant part of the encoding time especially when using the straightforward Full Search (FS) Algorithm. The Diamond Search (DS) was recently accepted as a fast motion estimation algorithm for the MPEG4 VM. In this report we verify the results extracted by the Advanced Diamond Zonal Search with Embedded Radar algorithm (ADZS-ER), proposed by Alexis M. Tourapis, Oscar C. Au, Ming L. Liou, and Guobin Shen (ISO/IEC JTC1/SC29/WG11, MPEG99/M4980). The experiments were carried out under the same conditions and the results verify the superiority of the proposed algorithm towards the DS algorithm, especially in the high bit rate cases, regarding the speed (in terms of number of checking points and total encoding time) and the quality (in terms of PSNR) of the output sequence.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@techreport{R1,
   title = {Verification Report of Core Experiment on Fast Block-Matching Motion Estimation using Advanced Diamond Zonal Search with Embedded Radar},
   author = {Tsechpenakis, Gabriel and Avrithis, Yannis and Kollias, Stefanos},
   institution = {ISO/IEC JTC1/SC29/WG11},
   number = {MPEG99/M5116},
   month = {10},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="book"></a>
					<span class="mr">Book chapters</span>
					<i class="fal fa-book-section"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="book-2022"></a>
					2022
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B9"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B9" id="tog-B9">
							<i class="left-60 tog far fa-chevron-down"></i>
							Deep Neural Network Attacks and Defense: the Case of Image Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, T. Furon, L. Amsaleg, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119901808.ch2" title="Electronic edition">Wiley,&nbsp;2022</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B09.wiley22.adverse.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B9.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:u-coK7KVo8oC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16120084928827636725" title="Citations @ Google Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-B9">
						<div class="pub-ref">
							In <em>Multimedia Security 1: Authentication and Data Hiding</em><br>
							Ed. by William Puech<br>
							pp. 41-75 <span class="bull"></span> Wiley, 2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/book/B09.wiley22.adverse.svg"><img src="../data/pub/thumb/wide/book/B09.wiley22.adverse.svg" alt="B9 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Machine learning using deep neural networks applied to image recognition works extremely well. However, it is possible to modify the images very slightly and intentionally, with modifications almost invisible to the eye, to deceive the classification system into misclassifying such content into the incorrect visual category. This chapter provides an overview of these intentional attacks, as well as the defense mechanisms used to counter them.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B9,
   title = {Deep Neural Network Attacks and Defense: The Case of Image Classification},
   author = {Zhang, Hanwei and Furon, Teddy and Amsaleg, Laurent and Avrithis, Yannis},
   publisher = {Wiley},
   booktitle = {Multimedia Security 1: Authentication and Data Hiding},
   editor = {William Puech},
   pages = {41--75},
   year = {2022}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2011"></a>
					2011
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B8"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B8" id="tog-B8">
							<i class="left-60 tog far fa-chevron-down"></i>
							Vision, Attention Control, and Goals Creation System
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/chapter/10.1007/978-1-4419-1452-1_11" title="Electronic edition">Springer,&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B08.springer11.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B8.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:tOudhMTPpwUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7641470056454724743" title="Citations @ Google Scholar">4</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/304518a6352d1e079a21c74af2f5b4c6383c2f61" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/304518a6352d1e079a21c74af2f5b4c6383c2f61#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-B8">
						<div class="pub-ref">
							In <em>Perception-Action Cycle: Models, Architectures and Hardware</em><br>
							Ed. by V. Cutsuridis, A. Hussain, J.G. Taylor<br>
							pp. 363-386 <span class="bull"></span> Springer, 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/book/B08.springer11.png"><img src="../data/pub/thumb/wide/book/B08.springer11.png" alt="B8 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Biological visual attention has been long studied by experts in the field of cognitive psychology. The Holy Grail of this study is the exact modeling of the interaction between the visual sensory and the process of perception. It seems that there is an informal agreement on the four important functions of the attention process: (a) the bottom-up process, which is responsible for the saliency of the input stimuli; (b) the top-down process that bias attention toward known areas or regions of predefined characteristics; (c) the attentional selection that fuses information derived from the two previous processes and enables focus; and (d) the dynamic evolution of the attentional selection process. In the following, we will outline established computational solutions for each of the four functions.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B8,
   title = {Vision, Attention Control, and Goals Creation System},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   publisher = {Springer},
   booktitle = {Perception-Action Cycle: Models, Architectures and Hardware},
   editor = {V. Cutsuridis and A. Hussain and J.G. Taylor},
   pages = {363--386},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2009"></a>
					2009
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B7"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B7" id="tog-B7">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge Driven Segmentation and Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, Ph. Mylonas, G. Papadopoulos, V. Mezaris, Y. Avrithis, I. Kompatsiaris, M.G. Strintzis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1002/9781119970231.ch10" title="DOI">Wiley,&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B07.wiley09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B7.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/books/wi/11/AthanasiadisMPMAKS11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:f2IySw72cVMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14395130531078287760" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/70dbaafa5839241bbe3e2ca2954ad77988b1c8d1" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-B7">
						<div class="pub-ref">
							In <em>Multimedia Semantics: Metadata, Analysis and Interaction</em><br>
							Ed. by R. Troncy, B. Huet, S. Schenk<br>
							pp. 163-181 <span class="bull"></span> Wiley, 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this chapter a first attempt will be made to examine how the coupling of multimedia processing and knowledge representation techniques, presented separately in previous chapters, can improve analysis. No formal reasoning techniques will be introduced at this stage; our exploration of how multimedia analysis and knowledge can be combined will start by revisiting the image and video segmentation problem. Semantic segmentation, presented in the first section of this chapter, starts with an elementary segmentation and region classification and refines it using similarity measures and merging criteria defined at the semantic level. Our discussion will continue in the next sections of the chapter with knowledge-driven classification approaches, which exploit knowledge in the form of contextual information for refining elementary classification results obtained via machine learning. Two relevant approaches will be presented. The first one deals with visual context and treats it as interaction between global classification and local region labels. The second one deals with spatial context and formulates the exploitation of it as a global optimization problem.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B7,
   title = {Knowledge Driven Segmentation and Classification},
   author = {Athanasiadis, Thanos and Mylonas, Phivos and Papadopoulos, Georgios and Mezaris, Vasileios and Avrithis, Yannis and Kompatsiaris, Ioannis and Strintzis, Michael G.},
   publisher = {Wiley},
   booktitle = {Multimedia Semantics: Metadata, Analysis and Interaction},
   editor = {R. Troncy and B. Huet and S. Schenk},
   pages = {163--181},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2008"></a>
					2008
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B6"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B6" id="tog-B6">
							<i class="left-60 tog far fa-chevron-down"></i>
							Audiovisual Attention Modeling and Salient Event Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Evangelopoulos, K. Rapantzikos, P. Maragos, Y. Avrithis, A. Potamianos</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-0-387-76316-3_8" title="DOI">Springer,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B06.springer08.muscle.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B6.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/books/daglib/p/EvangelopoulosRMAP08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:g5m5HwL7SMYC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18170350812852719194" title="Citations @ Google Scholar">32</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d30406a20eaa04d85fa521b4140edbf6a46fb9b3" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d30406a20eaa04d85fa521b4140edbf6a46fb9b3#citing-papers" title="Citations @ Semantic Scholar">23</a>
						</div>

					</div>
					<div class="collapse" id="col-B6">
						<div class="pub-ref">
							In <em>Multimodal Processing and Interaction: Audio, Video, Text</em><br>
							Ed. by P. Maragos, A. Potamianos, P. Gros<br>
							pp. 179-199 <span class="bull"></span> Springer, 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Although human perception appears to be automatic and unconscious, complex sensory mechanisms exist that form the preattentive component of understanding and lead to awareness. Considerable research has been carried out into these preattentive mechanisms and computational models have been developed for similar problems in the fields of computer vision and speech analysis. The focus here is to explore aural nd visual information in video streams for modeling attention and detecting salient events. The separate aural and visual modules may convey explicit, complementary or mutually exclusive information around the detected audio-visual events. Based on recent studies on perceptual and computational attention modeling, we formulate measures of attention using features of saliency for the audio-visual stream. Audio saliency is captured by signal modulations and related multifrequency band features, extracted through nonlinear operators and energy tracking. Visual saliency is measured by means of a spatiotemporal attention model driven by various feature cues (intensity, color, motion). Features from both modules mapped to one-dimensional, time-varying saliency curves, from which statistics of salient segments can be extracted and important audio or visual events can be detected through adaptive, threshold-based mechanisms. Audio and video curves are integrated in a single attention curve, where events may be enhanced, suppressed or vanished. Salient events from the audio-visual curve are detected through geometrical features such as local extrema, sharp transitions and level sets. The potential of inter-module fusion and audio-visual event detection is demonstrated in applications such as video key-frame selection, video skimming and video annotation.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B6,
   title = {Audiovisual Attention Modeling and Salient Event Detection},
   author = {Evangelopoulos, Georgios and Rapantzikos, Konstantinos and Maragos, Petros and Avrithis, Yannis and Potamianos, Alexandros},
   publisher = {Springer},
   booktitle = {Multimodal Processing and Interaction: Audio, Video, Text},
   editor = {P. Maragos and A. Potamianos and P. Gros},
   pages = {179--199},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B5"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B5" id="tog-B5">
							<i class="left-60 tog far fa-chevron-down"></i>
							Introducing Context and Reasoning in Visual Content Analysis: an Ontology-Based Framework
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Dasiopoulou, C. Saathoff, Ph. Mylonas, Y. Avrithis, Y. Kompatsiaris, S. Staab</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1007/978-1-84800-076-6_4" title="DOI">Springer,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B05.springer08.ace.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B5.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:iH-uZ7U-co4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10934569234147116680" title="Citations @ Google Scholar">17</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/20f9c77514a929f10554ac9c86c24df30c796ed7" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/20f9c77514a929f10554ac9c86c24df30c796ed7#citing-papers" title="Citations @ Semantic Scholar">14</a>
						</div>

					</div>
					<div class="collapse" id="col-B5">
						<div class="pub-ref">
							In <em>Semantic Multimedia and Ontologies: Theory and Applications</em><br>
							Ed. by Y. Kompatsiaris, P. Hobson<br>
							pp. 99-122 <span class="bull"></span> Springer, 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this chapter,we propose an ontology-based framework for enhancing segment-level annotations resulting from typical image analysis, through the exploitation of visual context and topological information. The concepts (objects) of interest and their spatial topology are modelled in RDFS ontologies, and through the use of reification, a fuzzy ontological representation is achieved, enabling the seamless integration of contextual knowledge. The formalisation of contextual information enables a first refinement of the input image analysis annotations utilising the semantic associations that characterise the context of appearance.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B5,
   title = {Introducing Context and Reasoning in Visual Content Analysis: An Ontology-based Framework},
   author = {Dasiopoulou, Stamatia and Saathoff, Carsten and Mylonas, Phivos and Avrithis, Yannis and Kompatsiaris, Yiannis and Staab, Steffen},
   publisher = {Springer},
   booktitle = {Semantic Multimedia and Ontologies: Theory and Applications},
   editor = {Y. Kompatsiaris and P. Hobson},
   month = {1},
   pages = {99--122},
   edition = {1st},
   isbn = {978-1-84800-075-9},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2006"></a>
					2006
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B4"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B4" id="tog-B4">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Processing of Color Images
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Dasiopoulou, E. Spyrou, Y. Avrithis, Y. Kompatsiaris, M.G. Strintzis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">CRC Press,&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/book/B04.crc05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B4.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:GnPB-g6toBAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5727733769232036984" title="Citations @ Google Scholar">11</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5794ba8b3df4386c436532b08e4d01aa4b91c426" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/5794ba8b3df4386c436532b08e4d01aa4b91c426#citing-papers" title="Citations @ Semantic Scholar">3</a>
						</div>

					</div>
					<div class="collapse" id="col-B4">
						<div class="pub-ref">
							In <em>Color Image Processing: Emerging Applications</em><br>
							Ed. by R. Lukac, K.N. Plataniotis<br>
							pp. 259-284 <span class="bull"></span> CRC Press, 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This chapter discusses semantic image analysis for the purpose of automatic image understanding and efficient visual content access and retrieval at semantic level. It presents the current state of the art analysis approaches aiming at bridging the "semantic gap" in image analysis and retrieval, highlights the major achievements of the existing approaches and sheds light to the challenges still unsolved. Its main subject is to present a generic framework for performing knowledge-assisted semantic analysis of images and also to present the  Knowledge-Assisted Analysis as performed in the aceMedia project.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B4,
   title = {Semantic Processing of Color Images},
   author = {Dasiopoulou, Stamatia and Spyrou, Evaggelos and Avrithis, Yannis and Kompatsiaris, Yiannis and Strintzis, Michael G.},
   publisher = {CRC Press},
   booktitle = {Color Image Processing: Emerging Applications},
   editor = {R. Lukac and K.N. Plataniotis},
   pages = {259--284},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B3" id="tog-B3">
							<i class="left-60 tog far fa-chevron-down"></i>
							Automatic Thematic Categorization of Multimedia Documents Using Ontological Information and Fuzzy Algebra
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Ph. Mylonas, G. Akrivas, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Springer,&nbsp;2006</span>
							<a class="lnk mr" href="../data/pub/pdf/book/B03.springer05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:j3f4tGmQtD8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7870775477861273686" title="Citations @ Google Scholar">11</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/ff192797d824f11a19c42e1b3845ac9957387279" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/ff192797d824f11a19c42e1b3845ac9957387279#citing-papers" title="Citations @ Semantic Scholar">7</a>
						</div>

					</div>
					<div class="collapse" id="col-B3">
						<div class="pub-ref">
							In <em>Soft Computing in Ontologies and Semantic Web</em><br>
							Ed. by Z. Ma<br>
							pp. 247-272 <span class="bull"></span> Springer, 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The semantic gap is the main problem of content based multimedia retrieval. This refers to the extraction of the semantic content of multimedia documents, the understanding of user information needs and requests, as well as to the matching between the two. In this chapter we focus on the analysis of multimedia documents for the extraction of their semantic content. Our approach is based on fuzzy algebra, as well as fuzzy ontological information. We start by outlining the methodologies that may lead to the creation of a semantic index; these methodologies are integrated in a video annotating environment. Based on the semantic index, we then explain how multimedia content may be analyzed for the extraction of semantic information in the form of thematic categorization. The latter relies on stored knowledge and a fuzzy hierarchical clustering algorithm that uses a similarity measure that is based on the notion of context.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B3,
   title = {Automatic thematic categorization of multimedia documents using ontological information and fuzzy algebra},
   author = {Wallace, Manolis and Mylonas, Phivos and Akrivas, Giorgos and Avrithis, Yannis and Kollias, Stefanos},
   publisher = {Springer},
   booktitle = {Soft Computing in Ontologies and Semantic Web},
   editor = {Z. Ma},
   volume = {204},
   pages = {247--272},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2005"></a>
					2005
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B2" id="tog-B2">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge-Based Multimedia Content Indexing and Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Y. Avrithis, G. Stamou, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://doi.org/10.1002/0470012617.ch12" title="DOI">Wiley,&nbsp;2005</a>
							<a class="lnk mr" href="../data/pub/pdf/book/B02.wiley04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/books/wi/05/WallaceASK05" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:qxL8FJ1GzNcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=2808772078925026507" title="Citations @ Google Scholar">14</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/f6a5d0155c95b1b67ce040b0f101204b4e635c5f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/f6a5d0155c95b1b67ce040b0f101204b4e635c5f#citing-papers" title="Citations @ Semantic Scholar">13</a>
						</div>

					</div>
					<div class="collapse" id="col-B2">
						<div class="pub-ref">
							In <em>Multimedia Content and Semantic Web: Methods, Standards and Tools</em><br>
							Ed. by G. Stamou, S. Kollias<br>
							pp. 299-338 <span class="bull"></span> Wiley, 2005
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this chapter, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual archives. This novel system exploits the advances in handling a/v content related metadata, as introduced by MPEG-7 and worked out by MPEG-21, to offer advanced access services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" of multimedia documents and "personalized response". The proposed system is targeting the intelligent extraction of semantic information from multimedia document descriptions, taking into account the nature of useful queries that users may issue, and the context determined by user profiles. From a technical point of view, it plays the role of an intermediate access server residing between the end users and multiple diverse in nature audiovisual archives, organized according to the latest MPEG standards.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B2,
   title = {Knowledge-Based Multimedia Content Indexing and Retrieval},
   author = {Wallace, Manolis and Avrithis, Yannis and Stamou, Giorgos and Kollias, Stefanos},
   publisher = {Wiley},
   booktitle = {Multimedia Content and Semantic Web: Methods, Standards and Tools},
   editor = {G. Stamou and S. Kollias},
   month = {8},
   pages = {299-338},
   year = {2005}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="book-2002"></a>
					2002
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="B1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-B1" id="tog-B1">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fuzzy Data Fusion for Multiple Cue Image and Video Segmentation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">S. Ioannou, Y. Avrithis, G. Stamou, S. Kollias</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Springer,&nbsp;2002</span>
							<a class="lnk mr" href="../data/pub/pdf/book/B01.ffip02.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/book/B1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:b0M2c_1WBrUC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=18252186955946962997" title="Citations @ Google Scholar">5</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/d470cd38a66f08fa3600479f9363aba622f801cb" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/d470cd38a66f08fa3600479f9363aba622f801cb#citing-papers" title="Citations @ Semantic Scholar">4</a>
						</div>

					</div>
					<div class="collapse" id="col-B1">
						<div class="pub-ref">
							In <em>Fuzzy Technologies and Applications</em><br>
							Ed. by E. Kerre<br>
							pp. 195-215 <span class="bull"></span> Springer, 2002
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Fusion of multiple cue image partitions is described as an indispensable tool towards the goal of automatic object-based image and video segmentation, interpretation and coding. Since these tasks involve human cognition and knowledge of image semantics, which are absent in most cases, fusion of all available cues is crucial for effective segmentation of generic video sequences. This chapter investigates fuzzy data fusion techniques which are capable of integrating the results of multiple cue segmentation and provide time consistent spatiotemporal image partitions corresponding to moving objects.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@incollection{B1,
   title = {Fuzzy Data Fusion For Multiple Cue Image And Video Segmentation},
   author = {Ioannou, Spyros and Avrithis, Yannis and Stamou, Giorgos and Kollias, Stefanos},
   publisher = {Springer},
   booktitle = {Fuzzy Technologies and Applications},
   editor = {E. Kerre},
   month = {5},
   pages = {195--215},
   year = {2002}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="journ"></a>
					<span class="mr">Journals</span>
					<i class="fal fa-book-open"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="journ-2021"></a>
					2021
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J31"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J31" id="tog-J31">
							<i class="left-60 tog far fa-chevron-down"></i>
							Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Z. Yang, M. Shi, C. Xu, V. Ferrari, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321003514" title="Electronic edition">PR,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J31.pr21.nsod.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J31.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.patcog.2021.108164" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/pr/YangS0FA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:OTTXONDVkokC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3207389759502341029" title="Citations @ Google Scholar">9</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/fdcdd006e4d0fd27fd373c0a7b80dacd312c569f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/fdcdd006e4d0fd27fd373c0a7b80dacd312c569f#citing-papers" title="Citations @ Semantic Scholar">10</a>
						</div>

					</div>
					<div class="collapse" id="col-J31">
						<div class="pub-ref">
							<em>Pattern Recognition</em><br>
							120:108164  <span class="bull"></span> Dec 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J31.pr20.nsod.svg"><img src="../data/pub/thumb/wide/journ/J31.pr20.nsod.svg" alt="J31 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Weakly-supervised object detection attempts to limit the amount of supervision by dispensing the need for bounding boxes, but still assumes image-level labels on the entire training set. In this work, we study the problem of training an object detector from one or few images with image-level labels and a larger set of completely unlabeled images. This is an extreme case of semi-supervised learning where the labeled data are not enough to bootstrap the learning of a detector. Our solution is to train a weakly-supervised student detector model from image-level pseudo-labels generated on the unlabeled set by a teacher classifier model, bootstrapped by region-level similarities to labeled images. Building upon the recent representative weakly-supervised pipeline PCL, our method can use more unlabeled images to achieve performance competitive or superior to many recent weakly-supervised detection solutions.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J31,
   title = {Training Object Detectors from Few Weakly-Labeled and Many Unlabeled Images},
   author = {Yang, Zhaohui and Shi, Miaojing and Xu, Chao and Ferrari, Vittorio and Avrithis, Yannis},
   journal = {Pattern Recognition (PR)},
   volume = {120},
   pages = {108164},
   month = {12},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J30"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J30" id="tog-J30">
							<i class="left-60 tog far fa-chevron-down"></i>
							Walking on the Edge: Fast, Low-Distortion Adversarial Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, Y. Avrithis, T. Furon, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/9186644" title="Electronic edition">TIFS,&nbsp;2021</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J30.tifs20.bp.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J30.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#bp" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tifs.2020.3021899" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tifs/ZhangAFA21" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:IUKN3-7HHlwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7146835260599927994" title="Citations @ Google Scholar">54</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/580305c4bdda4131f4e4b48ca08f473997268bbf" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/580305c4bdda4131f4e4b48ca08f473997268bbf#citing-papers" title="Citations @ Semantic Scholar">36</a>
						</div>

					</div>
					<div class="collapse" id="col-J30">
						<div class="pub-ref">
							<em>IEEE Transactions on Information Forensics and Security</em><br>
							16:701-713  <span class="bull"></span> Sep 2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J30.tifs20.bp.png"><img src="../data/pub/thumb/wide/journ/J30.tifs20.bp.png" alt="J30 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Adversarial examples of deep neural networks are receiving ever increasing attention because they help in understanding and reducing the sensitivity to their input. This is natural given the increasing applications of deep neural networks in our everyday lives. When white-box attacks are almost always successful, it is typically only the distortion of the perturbations that matters in their evaluation. In this work, we argue that speed is important as well, especially when considering that fast attacks are required by adversarial training. Given more time, iterative methods can always find better solutions. We investigate this speed-distortion trade-off in some depth and introduce a new attack called boundary projection (BP) that improves upon existing methods by a large margin. Our key idea is that the classification boundary is a manifold in the image space: we therefore quickly reach the boundary and then optimize distortion on this manifold.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J30,
   title = {Walking on the Edge: Fast, Low-Distortion Adversarial Examples},
   author = {Zhang, Hanwei and Avrithis, Yannis and Furon, Teddy and Amsaleg, Laurent},
   journal = {IEEE Transactions on Information Forensics and Security (TIFS)},
   volume = {16},
   pages = {701--713},
   month = {9},
   year = {2021}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2020"></a>
					2020
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J29"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J29" id="tog-J29">
							<i class="left-60 tog far fa-chevron-down"></i>
							Smooth Adversarial Examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">H. Zhang, Y. Avrithis, T. Furon, L. Amsaleg</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-020-00112-z" title="Open access">JIS,&nbsp;2020</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J29.jis20.smooth.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J29.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#sae" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1186/s13635-020-00112-z" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ejisec/ZhangAFA20" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:2VqYfGB8ITEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=3490524563568283987" title="Citations @ Google Scholar">27</a>
						</div>

					</div>
					<div class="collapse" id="col-J29">
						<div class="pub-ref">
							<em>EURASIP Journal on Information Security</em><br>
							2020:15-26  <span class="bull"></span> Nov 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J29.jis20.smooth.svg"><img src="../data/pub/thumb/wide/journ/J29.jis20.smooth.svg" alt="J29 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper investigates the visual quality of the adversarial examples. Recent papers propose to smooth the perturbations to get rid of high frequency artefacts. In this work, smoothing has a different meaning as it perceptually shapes the perturbation according to the visual content of the image to be attacked. The perturbation becomes locally smooth on the flat areas of the input image, but it may be noisy on its textured areas and sharp across its edges.
								</p>
								<p>
									This operation relies on Laplacian smoothing, well-known in graph signal processing, which we integrate in the attack pipeline. We benchmark several attacks with and without smoothing under a white-box scenario and evaluate their transferability. Despite the additional constraint of smoothness, our attack has the same probability of success at lower distortion.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J29,
   title = {Smooth Adversarial Examples},
   author = {Zhang, Hanwei and Avrithis, Yannis and Furon, Teddy and Amsaleg, Laurent},
   journal = {EURASIP Journal on Information Security (JIS)},
   volume = {2020},
   pages = {15--26},
   month = {11},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2019"></a>
					2019
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J28"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J28" id="tog-J28">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph-Based Particular Object Discovery
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">O. Simoni, A. Iscen, G. Tolias, Y. Avrithis, O. Chum</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s00138-019-01005-z" title="Electronic edition">MVA,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J28.mva18.disco.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J28.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s00138-019-01005-z" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/mva/SimeoniITAC19" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:LO7wyVUgiFcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10261177982408521771" title="Citations @ Google Scholar">8</a>
						</div>

					</div>
					<div class="collapse" id="col-J28">
						<div class="pub-ref">
							<em>Machine Vision and Applications</em><br>
							30(2):243-254  <span class="bull"></span> Mar 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J28.mva18.disco.svg"><img src="../data/pub/thumb/wide/journ/J28.mva18.disco.svg" alt="J28 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Severe background clutter is challenging in many computer vision tasks, including large-scale image retrieval. Global descriptors, that are popular due to their memory and search efficiency, are especially prone to corruption by such a clutter. Eliminating the impact of the clutter on the image descriptor increases the chance of retrieving relevant images and prevents topic drift due to actually retrieving the clutter in the case of query expansion. In this work, we propose a novel salient region detection method. It captures, in an unsupervised manner, patterns that are both discriminative and common in the dataset. Saliency is based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. The proposed method exploits recent CNN architectures trained for object retrieval to construct the image representation from the salient regions. We improve particular object retrieval on challenging datasets containing small objects.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J28,
   title = {Graph-based Particular Object Discovery},
   author = {Sim\'eoni, Oriane and Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond\v{r}ej},
   journal = {Machine Vision and Applications (MVA)},
   volume = {30},
   number = {2},
   month = {3},
   pages = {243--254},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J27" id="tog-J27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting the Medial Axis for Planar Shape Decomposition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Papanelopoulos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/pii/S1077314218304284" title="Electronic edition">CVIU,&nbsp;2019</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J27.cviu18.cuts.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J27.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.cviu.2018.10.007" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/cviu/PapanelopoulosA19" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:FAceZFleit8C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14957936024211021508" title="Citations @ Google Scholar">11</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/b7b259ec8fb9a7598dd3b756c320a012077a11ff" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/b7b259ec8fb9a7598dd3b756c320a012077a11ff#citing-papers" title="Citations @ Semantic Scholar">9</a>
						</div>

					</div>
					<div class="collapse" id="col-J27">
						<div class="pub-ref">
							<em>Computer Vision and Image Understanding</em><br>
							179:66-78  <span class="bull"></span> Feb 2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J27.cviu18.cuts.png"><img src="../data/pub/thumb/wide/journ/J27.cviu18.cuts.png" alt="J27 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a simple computational model for planar shape decomposition that naturally captures most of the rules and salience measures suggested by psychophysical studies, including the minima and short-cut rules, convexity, and symmetry. It is based on a medial axis representation in ways that have not been explored before and sheds more light into the connection between existing rules like minima and convexity. In particular, vertices of the exterior medial axis directly provide the position and extent of negative minima of curvature, while a traversal of the interior medial axis directly provides a small set of candidate endpoints for part-cuts. The final selection follows a prioritized processing of candidate part-cuts according to a local convexity rule that can incorporate arbitrary salience measures. Neither global optimization nor differentiation is involved. We provide qualitative and quantitative evaluation and comparisons on ground-truth data from psychophysical experiments. With our single computational model, we outperform even an ensemble method on several other competing models.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J27,
   title = {Revisiting the Medial Axis for Planar Shape Decomposition},
   author = {Papanelopoulos, Nikos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Computer Vision and Image Understanding (CVIU)},
   volume = {179},
   month = {2},
   pages = {66--78},
   year = {2019}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2016"></a>
					2016
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J26"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J26" id="tog-J26">
							<i class="left-60 tog far fa-chevron-down"></i>
							$\alpha$-Shapes for Local Feature Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C. Varytimidis, K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S0031320315003040" title="Electronic edition">PR,&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J26.pr16.wash.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J26.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#wash" title="Binary">
									<i class="fal fa-binary"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/wash/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.patcog.2015.08.016" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/pr/VarytimidisRAK16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:b1wdh0AR-JQC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14190546233819644429" title="Citations @ Google Scholar">17</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/66de0c6bcd90ef333a26842b5f0be05f9966ef8f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/66de0c6bcd90ef333a26842b5f0be05f9966ef8f#citing-papers" title="Citations @ Semantic Scholar">16</a>
						</div>

					</div>
					<div class="collapse" id="col-J26">
						<div class="pub-ref">
							<em>Pattern Recognition</em><br>
							50(1):56-73  <span class="bull"></span> Feb 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J26.pr16.wash.svg"><img src="../data/pub/thumb/wide/journ/J26.pr16.wash.svg" alt="J26 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Local image features are routinely used in state-of-the-art methods to solve many computer vision problems like image retrieval, classification, or 3D registration. As the applications become more complex, the research for better visual features is still active. In this paper we present a feature detector that exploits the inherent geometry of sampled image edges using -shapes. We propose a novel edge sampling scheme that exploits local shape and investigate different triangulations of sampled points. We also introduce a novel approach to represent the anisotropy in a triangulation along with different feature selection methods. Our detector provides a small number of distinctive features that is ideal for large scale applications, while achieving competitive performance in a series of matching and retrieval experiments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J26,
   title = {$\alpha$-shapes for local feature detection},
   author = {Varytimidis, Christos and Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Pattern Recognition (PR)},
   volume = {50},
   number = {1},
   month = {2},
   pages = {56--73},
   year = {2016}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J25"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J25" id="tog-J25">
							<i class="left-60 tog far fa-chevron-down"></i>
							Image Search with Selective Match Kernels: Aggregation Across Single and Multiple Images
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, Y. Avrithis, H. Jgou</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s11263-015-0810-4" title="Electronic edition">IJCV,&nbsp;2016</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J25.ijcv15.asmk.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J25.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#asmk" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/asmk/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s11263-015-0810-4" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ijcv/ToliasAJ16" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:TIZ-Mc8IlK0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16715237654263341073" title="Citations @ Google Scholar">170</a>
						</div>

					</div>
					<div class="collapse" id="col-J25">
						<div class="pub-ref">
							<em>International Journal of Computer Vision</em><br>
							116(3):247-261  <span class="bull"></span> Feb 2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J25.ijcv15.asmk.png"><img src="../data/pub/thumb/wide/journ/J25.ijcv15.asmk.png" alt="J25 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This paper considers a family of metrics to compare images based on their local descriptors. It encompasses the VLAD descriptor and matching techniques such as Hamming Embedding. Making the bridge between these approaches leads us to propose a match kernel that takes the best of existing techniques by combining an aggregation procedure with a selective match kernel. The representation underpinning this kernel is approximated, providing a large scale image search both precise and scalable, as shown by our experiments on several benchmarks.
								</p>
								<p>
									We show that the same aggregation procedure, originally applied per image, can effectively operate on groups of similar features found across multiple images. This method implicitly performs feature set augmentation, while enjoying savings in memory requirements at the same time. Finally, the proposed method is shown effective for place recognition, outperforming state of the art methods on a large scale landmark recognition benchmark.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J25,
   title = {Image search with selective match kernels: aggregation across single and multiple images},
   author = {Tolias, Giorgos and Avrithis, Yannis and J\'egou, Herv\'e},
   journal = {International Journal of Computer Vision (IJCV)},
   volume = {116},
   number = {3},
   month = {2},
   pages = {247--261},
   year = {2016}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2015"></a>
					2015
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J24"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J24" id="tog-J24">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dithering-Based Sampling and Weighted $\alpha$-Shapes for Local Feature Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">C. Varytimidis, K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.jstage.jst.go.jp/article/ipsjtcva/7/0/7_189/_article" title="Open access">CVA,&nbsp;2015</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J24.cva15.accv.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J24.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.2197/ipsjtcva.7.189" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ipsjtcva/VarytimidisRAK15" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:VaXvl8Fpj5cC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12559381394443289282" title="Citations @ Google Scholar">2</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/1796c93de9bb4b67758eb82f0b58b5d52c52d9b4" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/1796c93de9bb4b67758eb82f0b58b5d52c52d9b4#citing-papers" title="Citations @ Semantic Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-J24">
						<div class="pub-ref">
							<em>IPSJ Transactions on Computer Vision and Applications</em><br>
							7(1):189-200  <span class="bull"></span> Dec 2015
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J24.cva15.accv.svg"><img src="../data/pub/thumb/wide/journ/J24.cva15.accv.svg" alt="J24 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Local feature detection has been an essential part of many methods for computer vision applications like large scale image retrieval, object detection, or tracking. Recently, structure-guided feature detectors have been proposed, exploiting image edges to accurately capture local shape. Among them, the WSH detector [Varytimidis et al., 2012] starts from sampling binary edges and exploits -shapes, a computational geometry representation that describes local shape in different scales. In this work, we propose a novel image sampling method, based on dithering smooth image functions other than intensity. Samples are extracted on image contours representing the underlying shapes, with sampling density determined by image functions like the gradient or Hessian response, rather than being fixed. We thoroughly evaluate the parameters of the method, and achieve state-of-the-art performance on a series of matching and retrieval experiments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J24,
   title = {Dithering-based Sampling and Weighted $\alpha$-shapes for Local Feature Detection},
   author = {Varytimidis, Christos and Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {IPSJ Transactions on Computer Vision and Applications (CVA)},
   publisher = {Information Processing Society of Japan},
   volume = {7},
   number = {1},
   month = {12},
   pages = {189--200},
   year = {2015}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2014"></a>
					2014
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J23"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J23" id="tog-J23">
							<i class="left-60 tog far fa-chevron-down"></i>
							Towards Large-Scale Geometry Indexing by Feature Selection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Tolias, Y. Kalantidis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S1077314213002336" title="Electronic edition">CVIU,&nbsp;2014</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J23.cviu14.fms.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J23.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#fmh" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/feature_map_hashing" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.cviu.2013.12.002" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/cviu/ToliasKAK14" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:BUYA1_V_uYcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=877079881430987503" title="Citations @ Google Scholar">27</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/81878059305ed27fb8ecaf6a12957fe881b43c70" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/81878059305ed27fb8ecaf6a12957fe881b43c70#citing-papers" title="Citations @ Semantic Scholar">22</a>
						</div>

					</div>
					<div class="collapse" id="col-J23">
						<div class="pub-ref">
							<em>Computer Vision and Image Understanding</em><br>
							120:31-45  <span class="bull"></span> Mar 2014
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J23.cviu14.fms.png"><img src="../data/pub/thumb/wide/journ/J23.cviu14.fms.png" alt="J23 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We present a new approach to image indexing and retrieval, which integrates appearance with global image geometry in the indexing process, while enjoying robustness against viewpoint change, photometric variations, occlusion, and background clutter. We exploit shape parameters of local features to estimate image alignment via a single correspondence. Then, for each feature, we construct a sparse spatial map of all remaining features, encoding their normalized position and appearance, typically vector quantized to visual word. An image is represented by a collection of such feature maps and RANSAC-like matching is reduced to a number of set intersections. The required index space is still quadratic in the number of features. To make it linear, we propose a novel feature selection model tailored to our feature map representation, replacing our earlier hashing approach. The resulting index space is comparable to baseline bag-of-words, scaling up to one million images while outperforming the state of the art on three publicly available datasets. To our knowledge, this is the first geometry indexing method to dispense with spatial verification at this scale, bringing query times down to milliseconds.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J23,
   title = {Towards large-scale geometry indexing by feature selection},
   author = {Tolias, Giorgos and Kalantidis, Yannis and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Computer Vision and Image Understanding (CVIU)},
   volume = {120},
   pages = {31--45},
   month = {03},
   year = {2014}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J22"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J22" id="tog-J22">
							<i class="left-60 tog far fa-chevron-down"></i>
							Hough Pyramid Matching: Speeded-Up Geometry Re-Ranking for Large Scale Image Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Tolias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s11263-013-0659-3" title="Electronic edition">IJCV,&nbsp;2014</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J22.ijcv14.hpm.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J22.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#hpm-int" title="Source code">
									<i class="fal fa-gear"></i>
								</a>
								<a class="lnk mr2" href="../code/#hpm" title="Binary">
									<i class="fal fa-binary"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/relaxed_spatial_matching/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s11263-013-0659-3" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ijcv/AvrithisT14" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:vDijr-p_gm4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11775912909455267723" title="Citations @ Google Scholar">99</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/15c5b2ec8af109f56c64a300a3fef4d75a0a51bd" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/15c5b2ec8af109f56c64a300a3fef4d75a0a51bd#citing-papers" title="Citations @ Semantic Scholar">78</a>
						</div>

					</div>
					<div class="collapse" id="col-J22">
						<div class="pub-ref">
							<em>International Journal of Computer Vision</em><br>
							107(1):1-19  <span class="bull"></span> Mar 2014
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J22.ijcv14.hpm.png"><img src="../data/pub/thumb/wide/journ/J22.ijcv14.hpm.png" alt="J22 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Exploiting local feature shape has made geometry indexing possible, but at a high cost of index space, while a sequential spatial verification and re-ranking stage is still indispensable for large scale image retrieval. In this work we investigate an accelerated approach for the latter problem. We develop a simple spatial matching model inspired by Hough voting in the transformation space, where votes arise from single feature correspondences. Using a histogram pyramid, we effectively compute pair-wise affinities of correspondences without ever enumerating all pairs. Our Hough pyramid matching algorithm is linear in the number of correspondences and allows for multiple matching surfaces or non-rigid objects under one-to-one mapping. We achieve re-ranking one order of magnitude more images at the same query time with superior performance compared to state of the art methods, while requiring the same index space. We show that soft assignment is compatible with this matching scheme, preserving one-to-one mapping and further increasing performance.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J22,
   title = {Hough Pyramid Matching: Speeded-up geometry re-ranking for large scale image retrieval},
   author = {Avrithis, Yannis and Tolias, Giorgos},
   journal = {International Journal of Computer Vision (IJCV)},
   volume = {107},
   number = {1},
   month = {3},
   pages = {1--19},
   year = {2014}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2013"></a>
					2013
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J21"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J21" id="tog-J21">
							<i class="left-60 tog far fa-chevron-down"></i>
							Multimodal Saliency and Fusion for Movie Summarization Based on Aural, Visual, and Textual Attention
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Evangelopoulos, A. Zlatintsi, A. Potamianos, P. Maragos, K. Rapantzikos, G. Skoumas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/6527322" title="Electronic edition">TMM,&nbsp;2013</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J21.tmm13.moviesum.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J21.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/visual_saliency" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tmm.2013.2267205" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tmm/EvangelopoulosZPMRSA13" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:evX43VCCuoAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9585966805379596992" title="Citations @ Google Scholar">300</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/463df0d0d0731e62315b5c9322e2f8e8757014f7" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/463df0d0d0731e62315b5c9322e2f8e8757014f7#citing-papers" title="Citations @ Semantic Scholar">228</a>
						</div>

					</div>
					<div class="collapse" id="col-J21">
						<div class="pub-ref">
							<em>IEEE Transactions on Multimedia</em><br>
							15(7):1553-1568  <span class="bull"></span> Nov 2013
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J21.tmm13.moviesum.svg"><img src="../data/pub/thumb/wide/journ/J21.tmm13.moviesum.svg" alt="J21 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Multimodal streams of sensory information are naturally parsed and integrated by humans using signal-level feature extraction and higher-level cognitive processes. Detection of attention-invoking audiovisual segments is formulated in this work on the basis of saliency models for the audio, visual and textual information conveyed in a video stream. Aural or auditory saliency is assessed by cues that quantify multifrequency waveform modulations, extracted through nonlinear operators and energy tracking. Visual saliency is measured through a spatiotemporal attention model driven by intensity, color and orientation. Textual or linguistic saliency is extracted from part-of-speech tagging on the subtitles information available with most movie distributions. The individual saliency streams, obtained from modality-depended cues, are integrated in a multimodal saliency curve, modeling the time-varying perceptual importance of the composite video stream and signifying prevailing sensory events. The multimodal saliency representation forms the basis of a generic, bottom-up video summarization algorithm. Different fusion schemes are evaluated on a movie database of multimodal saliency annotations with comparative results provided across modalities. The produced summaries, based on low-level features and content-independent fusion and selection, are of subjectively high aesthetic and informative quality.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J21,
   title = {Multimodal Saliency and Fusion for Movie Summarization based on Aural, Visual, and Textual Attention},
   author = {Evangelopoulos, Georgios and Zlatintsi, Athanasia and Potamianos, Alexandros and Maragos, Petros and Rapantzikos, Konstantinos and Skoumas, Georgios and Avrithis, Yannis},
   journal = {IEEE Transactions on Multimedia (TMM)},
   volume = {15},
   number = {7},
   month = {11},
   pages = {1553--1568},
   year = {2013}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2011"></a>
					2011
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J20"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J20" id="tog-J20">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal Features for Action Recognition and Salient Event Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s12559-011-9097-0" title="Electronic edition">CC,&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J20.ccj11.dense.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J20.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/spatiotemporal_feature_detection" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s12559-011-9097-0" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/cogcom/RapantzikosAK11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:u_35RYKgDlwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=289407331548126274" title="Citations @ Google Scholar">24</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/ab8994df510e41fd03d7aba7a61e8bedb3a54209" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/ab8994df510e41fd03d7aba7a61e8bedb3a54209#citing-papers" title="Citations @ Semantic Scholar">17</a>
						</div>

					</div>
					<div class="collapse" id="col-J20">
						<div class="pub-ref">
							<em>Cognitive Computation</em><br>
							Special issue on <em>Saliency, Attention, Visual Search and Picture Scanning</em><br>
							3(1):167-184  <span class="bull"></span> Mar 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J20.ccj11.dense.svg"><img src="../data/pub/thumb/wide/journ/J20.ccj11.dense.svg" alt="J20 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Although the mechanisms of human visual understanding remain partially unclear, computational models inspired by existing knowledge on human vision have emerged and applied to several fields. In this paper, we propose a novel method to compute visual saliency from video sequences by counting in the actual spatiotemporal nature of the video. The visual input is represented by a volume in spacetime and decomposed into a set of feature volumes in multiple resolutions. Feature competition is used to produce a saliency distribution of the input implemented by constrained minimization. The proposed constraints are inspired by and associated with the Gestalt laws. There are a number of contributions in this approach, namely extending existing visual feature models to a volumetric representation, allowing competition across features, scales and voxels, and formulating constraints in accordance with perceptual principles. The resulting saliency volume is used to detect prominent spatiotemporal regions and consequently applied to action recognition and perceptually salient event detection in video sequences. Comparisons against established methods on public datasets are given and reveal the potential of the proposed model. The experiments include three action recognition scenarios and salient temporal segment detection in a movie database annotated by humans.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J20,
   title = {Spatiotemporal features for action recognition and salient event detection},
   author = {Rapantzikos, Konstantinos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Cognitive Computation (CC) (Special Issue on Saliency, Attention, Visual Search and Picture Scanning)},
   editor = {J.G. Taylor and V. Cutsuridis},
   volume = {3},
   number = {1},
   month = {3},
   pages = {167--184},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J19"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J19" id="tog-J19">
							<i class="left-60 tog far fa-chevron-down"></i>
							VIRaL: Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Kalantidis, G. Tolias, Y. Avrithis, M. Phinikettos, E. Spyrou, Ph. Mylonas, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s11042-010-0651-7" title="Electronic edition">MTAP,&nbsp;2011</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J19.mtap11.viral.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J19.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
								<a class="lnk mr2" href="../code/#viral" title="Application">
									<i class="fal fa-browser"></i>
								</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/scene_maps/" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s11042-010-0651-7" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/mta/KalantidisTAPSMK11" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:maZDTaKrznsC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16892044061664725739" title="Citations @ Google Scholar">104</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/503d8ff4bdf9875fbfef77228533d305b4fcd344" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/503d8ff4bdf9875fbfef77228533d305b4fcd344#citing-papers" title="Citations @ Semantic Scholar">77</a>
						</div>

					</div>
					<div class="collapse" id="col-J19">
						<div class="pub-ref">
							<em>Multimedia Tools and Applications</em><br>
							51(2):555-592  <span class="bull"></span> Jan 2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J19.mtap11.viral.png"><img src="../data/pub/thumb/wide/journ/J19.mtap11.viral.png" alt="J19 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									New applications are emerging every day exploiting the huge data volume in community photo collections. Most focus on popular subsets, e.g. images containing landmarks or associated to Wikipedia articles. In this work we are concerned with the problem of accurately finding the location where a photo is taken without needing any metadata, that is, solely by its visual content. We also recognize landmarks where applicable, automatically linking to Wikipedia. We show that the time is right for automating the geo-tagging process, and we show how this can work at large scale. In doing so, we do exploit redundancy of content in popular locations - but unlike most existing solutions, we do not restrict to landmarks. In other words, we can compactly represent the visual content of all thousands of images depicting e.g. the Parthenon and still retrieve any single, isolated, non-landmark image like a house or a graffiti on a wall. Starting from an existing, geo-tagged dataset, we cluster images into sets of different views of the same scene. This is a very efficient, scalable, and fully automated mining process. We then align all views in a set to one reference image and construct a 2D scene map. Our indexing scheme operates directly on scene maps. We evaluate our solution on a challenging one million urban image dataset and provide public access to our service through our application, VIRaL.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J19,
   title = {{VIRaL}: Visual Image Retrieval and Localization},
   author = {Kalantidis, Yannis and Tolias, Giorgos and Avrithis, Yannis and Phinikettos, Marios and Spyrou, Evaggelos and Mylonas, Phivos and Kollias, Stefanos},
   journal = {Multimedia Tools and Applications (MTAP)},
   volume = {51},
   number = {2},
   month = {1},
   pages = {555--592},
   year = {2011}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2009"></a>
					2009
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J18"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J18" id="tog-J18">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal Saliency for Video Classification
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, N. Tsapatsoulis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S0923596509000320" title="Electronic edition">SP:IC,&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J18.spic09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J18.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/visual_saliency" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.image.2009.03.002" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/spic/RapantzikosTAK09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:IWHjjKOFINEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13697255406706060688" title="Citations @ Google Scholar">46</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0931de0dcff12f7bc7b570e76ff2b42c68260888" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0931de0dcff12f7bc7b570e76ff2b42c68260888#citing-papers" title="Citations @ Semantic Scholar">39</a>
						</div>

					</div>
					<div class="collapse" id="col-J18">
						<div class="pub-ref">
							<em>Signal Processing: Image Communication</em><br>
							24(7):557-571  <span class="bull"></span> Aug 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Computer vision applications often need to process only a representative part of the visual input rather than the whole image/sequence. Considerable research has been carried out into salient region detection methods based either on models emulating human visual attention (VA) mechanisms or on computational approximations. Most of the proposed methods are bottom-up and their major goal is to filter out redundant visual information. In this paper, we propose and elaborate on a saliency detection model that treats a video sequence as a spatiotemporal volume and generates a local saliency measure for each visual unit (voxel). This computation involves an optimization process incorporating inter- and intra-feature competition at the voxel level. Perceptual decomposition of the input, spatiotemporal center-surround interactions and the integration of heterogeneous feature conspicuity values are described and an experimental framework for video classification is set up. This framework consists of a series of experiments that shows the effect of saliency in classification performance and let us draw conclusions on how well the detected salient regions represent the visual input. A comparison is attempted that shows the potential of the proposed method.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J18,
   title = {Spatiotemporal Saliency for Video Classification},
   author = {Rapantzikos, Konstantinos and Tsapatsoulis, Nicolas and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Signal Processing: Image Communication (SP:IC)},
   volume = {24},
   number = {7},
   month = {8},
   pages = {557--571},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J17"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J17" id="tog-J17">
							<i class="left-60 tog far fa-chevron-down"></i>
							Using Visual Context and Region Semantics for High-Level Concept Detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, E. Spyrou, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/4757439" title="Electronic edition">TMM,&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J17.tmm09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J17.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tmm.2008.2009681" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tmm/MylonasSAK09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:qUcmZB5y_30C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=16308377765341038957" title="Citations @ Google Scholar">59</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/c0d893432c0cd2d722f6b087422fac54e0d4a777" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/c0d893432c0cd2d722f6b087422fac54e0d4a777#citing-papers" title="Citations @ Semantic Scholar">44</a>
						</div>

					</div>
					<div class="collapse" id="col-J17">
						<div class="pub-ref">
							<em>IEEE Transactions on Multimedia</em><br>
							11(11):229-243  <span class="bull"></span> Feb 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we investigate detection of high-level concepts in multimedia content through an integrated approach of visual thesaurus analysis and visual context. In the former, detection is based on model vectors that represent image composition in terms of region types, obtained through clustering over a large data set. The latter deals with two aspects, namely high-level concepts and region types of the thesaurus, employing a model of a priori specified semantic relations among concepts and automatically extracted topological relations among region types; thus it combines both conceptual and topological context. A set of algorithms is presented, which modify either the confidence values of detected concepts, or the model vectors based on which detection is performed. Visual context exploitation is evaluated on TRECVID and Corel data sets and compared to a number of related visual thesaurus approaches.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J17,
   title = {Using Visual Context and Region Semantics for High-Level Concept Detection},
   author = {Mylonas, Phivos and Spyrou, Evaggelos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {IEEE Transactions on Multimedia (TMM)},
   volume = {11},
   number = {11},
   month = {2},
   pages = {229--243},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J16"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J16" id="tog-J16">
							<i class="left-60 tog far fa-chevron-down"></i>
							Concept Detection and Keyframe Extraction Using a Visual Thesaurus
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">E. Spyrou, G. Tolias, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s11042-008-0237-9" title="Electronic edition">MTAP,&nbsp;2009</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J16.mtap09.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J16.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s11042-008-0237-9" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/mta/SpyrouTMA09" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:r0BpntZqJG4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7907460091692581971" title="Citations @ Google Scholar">39</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/cbcab919a715be6d44ca6cc9a47bcd63edbbc731" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/cbcab919a715be6d44ca6cc9a47bcd63edbbc731#citing-papers" title="Citations @ Semantic Scholar">32</a>
						</div>

					</div>
					<div class="collapse" id="col-J16">
						<div class="pub-ref">
							<em>Multimedia Tools and Applications</em><br>
							41(3):337-373  <span class="bull"></span> Feb 2009
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This paper presents a video analysis approach based on concept detection and keyframe extraction employing a visual thesaurus representation. Color and texture descriptors are extracted from coarse regions of each frame and a visual thesaurus is constructed after clustering regions. The clusters, called region types, are used as basis for representing local material information through the construction of a model vector for each frame, which reflects the composition of the image in terms of region types. Model vector representation is used for keyframe selection either in each video shot or across an entire sequence. The selection process ensures that all region types are represented. A number of high-level concept detectors is then trained using global annotation and Latent Semantic Analysis is applied. To enhance detection performance per shot, detection is employed on the selected keyframes of each shot, and a framework is proposed for working on very large data sets.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J16,
   title = {Concept detection and keyframe extraction using a visual thesaurus},
   author = {Spyrou, Evaggelos and Tolias, Giorgos and Mylonas, Phivos and Avrithis, Yannis},
   journal = {Multimedia Tools and Applications (MTAP)},
   volume = {41},
   number = {3},
   month = {2},
   pages = {337--373},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2008"></a>
					2008
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J15"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J15" id="tog-J15">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Representation of Multimedia Content: Knowledge Representation and Semantic Indexing
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, Th. Athanasiadis, M. Wallace, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/s11042-007-0161-4" title="Electronic edition">MTAP,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J15.mtap06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J15.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/s11042-007-0161-4" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/mta/MylonasAWAK08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:L8Ckcad2t8MC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=7794173787167805909" title="Citations @ Google Scholar">25</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/378c20f401936f6fd85fdfe509b60f235bce9caa" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/378c20f401936f6fd85fdfe509b60f235bce9caa#citing-papers" title="Citations @ Semantic Scholar">14</a>
						</div>

					</div>
					<div class="collapse" id="col-J15">
						<div class="pub-ref">
							<em>Multimedia Tools and Applications</em><br>
							39(3):293-327  <span class="bull"></span> Sep 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we present a framework for unified, personalized access to heterogeneous multimedia content in distributed repositories. Focusing on semantic analysis of multimedia documents, metadata, user queries and user profiles, it contributes to the bridging of the gap between the semantic nature of user queries and raw multimedia documents. The proposed approach utilizes as input visual content analysis results, as well as analyzes and exploits associated textual annotation, in order to extract the underlying semantics, construct a semantic index and classify documents to topics, based on a unified knowledge and semantics representation model. It may then accept user queries, and, carrying out semantic interpretation and expansion, retrieve documents from the index and rank them according to user preferences, similarly to text retrieval. All processes are based on a novel semantic processing methodology, employing fuzzy algebra and principles of taxonomic knowledge representation. Part I of this work presented in this paper deals with data and knowledge models, manipulation of multimedia content annotations and semantic indexing, while Part II will continue on the use of the extracted semantic information for personalized retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J15,
   title = {Semantic Representation of Multimedia Content: Knowledge Representation and Semantic Indexing},
   author = {Mylonas, Phivos and Athanasiadis, Thanos and Wallace, Manolis and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Multimedia Tools and Applications (MTAP)},
   publisher = {Springer},
   volume = {39},
   number = {3},
   month = {9},
   pages = {293--327},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J14"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J14" id="tog-J14">
							<i class="left-60 tog far fa-chevron-down"></i>
							Personalized Information Retrieval Based on Context and Ontological Knowledge
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Ph. Mylonas, D. Vallet, P. Castells, M. Fernndez, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.cambridge.org/core/journals/knowledge-engineering-review/article/abs/personalized-information-retrieval-based-on-context-and-ontological-knowledge/21C3E294B0DF264B785A3EEE3C823479" title="Electronic edition">KER,&nbsp;2008</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J14.ker08.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J14.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1017/s0269888907001282" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ker/MylonasVCFA08" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:ufrVoPGSRksC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=11292978604394267970" title="Citations @ Google Scholar">118</a>
						</div>

					</div>
					<div class="collapse" id="col-J14">
						<div class="pub-ref">
							<em>Knowledge Engineering Review</em><br>
							23(1):73-100  <span class="bull"></span> Mar 2008
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Context modeling has been long acknowledged as a key aspect in a wide variety of problem domains. In this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. The key aspects in our proposed approach are a) the explicit distinction between historic user context and live user context, b) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and c) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. Based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J14,
   title = {Personalized information retrieval based on context and ontological knowledge},
   author = {Mylonas, Phivos and Vallet, David and Castells, Pablo and Fern\'andez, Miriam and Avrithis, Yannis},
   journal = {Knowledge Engineering Review (KER)},
   volume = {23},
   number = {1},
   month = {3},
   pages = {73--100},
   year = {2008}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2007"></a>
					2007
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J13"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J13" id="tog-J13">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semantic Image Segmentation and Object Labeling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Th. Athanasiadis, Ph. Mylonas, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/4118230" title="Electronic edition">CSVT,&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J13.csvt06a.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J13.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/semantic_image_and_video_segmentation" title="Project home">
									<i class="fal fa-home"></i>
								</a>
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tcsvt.2007.890636" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tcsv/AthanasiadisMAK07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:2osOgNQ5qMEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=14331874375840928397" title="Citations @ Google Scholar">260</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/00f78fcfa652c94ff5883fd17c5b81846a863018" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/00f78fcfa652c94ff5883fd17c5b81846a863018#citing-papers" title="Citations @ Semantic Scholar">108</a>
						</div>

					</div>
					<div class="collapse" id="col-J13">
						<div class="pub-ref">
							<em>IEEE Transactions on Circuits and Systems for Video Technology</em><br>
							17(3):298-312  <span class="bull"></span> Mar 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper we present a framework for simultaneous image segmentation and object labeling leading to automatic image annotation. Focusing on semantic analysis of images, it contributes to knowledge-assisted multimedia analysis and the bridging of the gap between its semantics and low level visual features. The proposed framework operates at semantic level using possible semantic labels, formally defined as fuzzy sets, to make decisions on handling image regions instead of visual features used traditionally. In order to stress its independence of a specific image segmentation approach we have modified two well known region growing algorithms, i.e. watershed and recursive shortest spanning tree, and compared them with their traditional counterparts. Additionally, a visual context representation and analysis approach is presented, blending global knowledge in interpreting each object locally. Contextual information is based on a novel semantic processing methodology, employing fuzzy algebra and ontological taxonomic knowledge representation. In this process, utilization of contextual knowledge re-adjusts semantic region growing labeling results appropriately, by means of fine-tuning the membership degrees of detected concepts. The performance of the overall methodology is demonstrated on a real-life still image dataset from two popular domains.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J13,
   title = {Semantic Image Segmentation and Object Labeling},
   author = {Athanasiadis, Thanos and Mylonas, Phivos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology (CSVT)},
   volume = {17},
   number = {3},
   month = {3},
   pages = {298--312},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J12"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J12" id="tog-J12">
							<i class="left-60 tog far fa-chevron-down"></i>
							Personalized Content Retrieval in Context Using Ontological Knowledge
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">D. Vallet, P. Castells, M. Fernndez, Ph. Mylonas, Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/4118247" title="Electronic edition">CSVT,&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J12.csvt06b.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J12.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tcsvt.2007.890633" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tcsv/ValletCFMA07" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:qjMakFHDy7sC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17259659025035746184" title="Citations @ Google Scholar">167</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/e021998d77145dd7cb104fddc45c98a479d6e1bd" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/e021998d77145dd7cb104fddc45c98a479d6e1bd#citing-papers" title="Citations @ Semantic Scholar">105</a>
						</div>

					</div>
					<div class="collapse" id="col-J12">
						<div class="pub-ref">
							<em>IEEE Transactions on Circuits and Systems for Video Technology</em><br>
							17(3):336-346  <span class="bull"></span> Mar 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in a way that outof-context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J12,
   title = {Personalized Content Retrieval in Context Using Ontological Knowledge},
   author = {Vallet, David and Castells, Pablo and Fern\'andez, Miriam and Mylonas, Phivos and Avrithis, Yannis},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology (CSVT)},
   volume = {17},
   number = {3},
   month = {3},
   pages = {336--346},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J10"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J10" id="tog-J10">
							<i class="left-60 tog far fa-chevron-down"></i>
							Bottom-Up Spatiotemporal Visual Attention Model for Video Analysis
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Rapantzikos, N. Tsapatsoulis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://digital-library.theiet.org/content/journals/10.1049/iet-ipr_20060040" title="Electronic edition">IP,&nbsp;2007</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J10.iee-vis06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J10.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1049/iet-ipr:20060040" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:KlAtU1dfN6UC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13692820799238743717" title="Citations @ Google Scholar">57</a>
						</div>

					</div>
					<div class="collapse" id="col-J10">
						<div class="pub-ref">
							<em>IET Image Processing</em><br>
							1(2):237-248  <span class="bull"></span> Jun 2007
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A video analysis framework based on spatiotemporal saliency calculation is presented. We propose a novel scheme for generating saliency in video sequences by taking into account both the spatial extent and dynamic evolution of regions. Towards this goal we extend a common image-oriented computational model of saliency-based visual attention to handle spatiotemporal analysis of video in a volumetric framework. The main claim is that attention acts as an efficient preprocessing step of a video sequence in order to obtain a compact representation of its content in the form of salient events/objects. The model has been implemented and qualitative as well as quantitative examples illustrating its performance are shown.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J10,
   title = {Bottom-Up Spatiotemporal Visual Attention Model for Video Analysis},
   author = {Rapantzikos, Konstantinos and Tsapatsoulis, Nicolas and Avrithis, Yannis and Kollias, Stefanos},
   journal = {IET Image Processing (IP)},
   volume = {1},
   number = {2},
   month = {6},
   pages = {237--248},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2006"></a>
					2006
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J11"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J11" id="tog-J11">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge-Assisted Image Analysis Based on Context and Spatial Optimization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">G. Papadopoulos, Ph. Mylonas, V. Mezaris, Y. Avrithis, I. Kompatsiaris</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.igi-global.com/gateway/article/2822" title="Electronic edition">SWIS,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J11.ijswis06.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J11.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.4018/jswis.2006070102" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/ijswis/PapadopoulosMMAK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Zph67rFs4hoC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=793023555589292079" title="Citations @ Google Scholar">28</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0760787ef58985e3aa02b3102c379bcf32e551b1" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0760787ef58985e3aa02b3102c379bcf32e551b1#citing-papers" title="Citations @ Semantic Scholar">28</a>
						</div>

					</div>
					<div class="collapse" id="col-J11">
						<div class="pub-ref">
							<em>International Journal on Semantic Web and Information Systems</em><br>
							2(3):17-36  <span class="bull"></span> Jul 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this article, an approach to semantic image analysis is presented. Under the proposed approach, ontologies are used to capture general, spatial, and contextual knowledge of a domain, and a genetic algorithm is applied to realize the final annotation. The employed domain knowledge considers high-level information in terms of the concepts of interest of the examined domain, contextual information in the form of fuzzy ontological relations, as well as low-level information in terms of prototypical low-level visual descriptors. To account for the inherent ambiguity in visual information, uncertainty has been introduced in the spatial relations definition. First, an initial hypothesis set of graded annotations is produced for each image region, and then context is exploited to update appropriately the estimated degrees of confidence. Finally, a genetic algorithm is applied to decide the most plausible annotation by utilizing the visual and the spatial concepts definitions included in the domain ontology. Experiments with a collection of photographs belonging to two different domains demonstrate the performance of the proposed approach.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J11,
   title = {Knowledge-Assisted Image Analysis Based on Context and Spatial Optimization},
   author = {Papadopoulos, Georgios and Mylonas, Phivos and Mezaris, Vasileios and Avrithis, Yannis and Kompatsiaris, Ioannis},
   journal = {International Journal on Semantic Web and Information Systems (SWIS)},
   volume = {2},
   number = {3},
   month = {7},
   pages = {17--36},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J9"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J9" id="tog-J9">
							<i class="left-60 tog far fa-chevron-down"></i>
							Knowledge Representation and Semantic Annotation of Multimedia Content
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">K. Petridis, S. Bloehdorn, C. Saathoff, N. Simou, S. Dasiopoulou, V. Tzouvaras, S. Handschuh, Y. Avrithis, I. Kompatsiaris, S. Staab</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://digital-library.theiet.org/content/journals/10.1049/ip-vis_20050059" title="Electronic edition">VISP,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J09.iee-vis05.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J9.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1049/ip-vis:20050059" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:zYLM7Y9cAGgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=13040490746319243275" title="Citations @ Google Scholar">84</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/79941f6751e964e154e45920533c348f67216d32" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/79941f6751e964e154e45920533c348f67216d32#citing-papers" title="Citations @ Semantic Scholar">63</a>
						</div>

					</div>
					<div class="collapse" id="col-J9">
						<div class="pub-ref">
							<em>IEE Proceedings on Vision, Image and Signal Processing</em><br>
							Special issue on <em>Knowledge-Based Digital Media Processing</em><br>
							153(3):255-262  <span class="bull"></span> Jun 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Knowledge representation and annotation of multimedia documents typically have been pursued in two different directions. Previous approaches have focused either on low level descriptors, such as dominant color, or on the semantic content dimension and corresponding manual annotations, such as person or vehicle. In this paper, we present a knowledge infrastructure and a experimentation platform for semantic annotation to bridge the two directions. Ontologies are being extended and enriched to include low-level audiovisual features and descriptors. Additionally, we present a tool that allows for linking low-level MPEG-7 visual descriptions to ontologies and annotations. This way we construct ontologies that include prototypical instances of high-level domain concepts together with a formal specification of the corresponding visual descriptors. This infrastructure is exploited by a knowledge-assisted analysis framework that may handle problems like segmentation, tracking, feature extraction and matching in order to classify scenes, identify and label objects, thus automatically create the associated semantic metadata.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J9,
   title = {Knowledge Representation and Semantic Annotation of Multimedia Content},
   author = {Petridis, Kosmas and Bloehdorn, Stephan and Saathoff, Carsten and Simou, Nikolaos and Dasiopoulou, Stamatia and Tzouvaras, Vassilis and Handschuh, Siegfried and Avrithis, Yannis and Kompatsiaris, Ioannis and Staab, Steffen},
   journal = {IEE Proceedings on Vision, Image and Signal Processing (VISP) (Special Issue on Knowledge-Based Digital Media Processing)},
   volume = {153},
   number = {3},
   month = {6},
   pages = {255--262},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J8"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J8" id="tog-J8">
							<i class="left-60 tog far fa-chevron-down"></i>
							Computationally Efficient sup-$t$ Transitive Closure for Sparse Fuzzy Binary Relations
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S0165011405003118" title="Electronic edition">FSS,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J08.fss04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J8.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1016/j.fss.2005.06.005" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/fss/WallaceAK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:MXK_kJrjxJIC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=1085667665976160699" title="Citations @ Google Scholar">35</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/0e6a5b5506b76171d2aa5a346430f76052739a9e" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/0e6a5b5506b76171d2aa5a346430f76052739a9e#citing-papers" title="Citations @ Semantic Scholar">31</a>
						</div>

					</div>
					<div class="collapse" id="col-J8">
						<div class="pub-ref">
							<em>Fuzzy Sets and Systems</em><br>
							157(3):341-372  <span class="bull"></span> Feb 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									The property of transitivity is one of the most important for fuzzy binary relations, especially in the cases when they are used for the representation of real life similarity or ordering information. As far as the algorithmic part of the actual calculation of the transitive closure of such relations is concerned, works in the literature mainly focus on crisp symmetric relations, paying little attention to the case of general fuzzy binary relations. Most works that deal with the algorithmic part of the transitive closure of fuzzy relations only focus on the case of max-min transitivity, disregarding other types of transitivity. In this paper, after formalizing the notion of sparseness and providing a representation model for sparse relations that displays both computational and storage merits, we propose an algorithm for the incremental update of fuzzy sup-t transitive relations. The incremental transitive update (ITU) algorithm achieves the re-establishment of transitivity when an already transitive relation is only locally disturbed. Based on this algorithm, we propose an extension to handle the sup-t transitive closure of any fuzzy binary relation, through a novel incremental transitive closure (ITC) algorithm. The ITU and ITC algorithms can be applied on any fuzzy binary relation and t-norm; properties such as reflexivity, symmetricity and idempotency are not a requirement. Under the specified assumptions for the average sparse relation, both of the proposed algorithms have considerably smaller computational complexity than the conventional approach; this is both established theoretically and verified via appropriate computing experiments.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J8,
   title = {Computationally efficient {sup-$t$} transitive closure for sparse fuzzy binary relations},
   author = {Wallace, Manolis and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Fuzzy Sets and Systems (FSS)},
   volume = {157},
   number = {3},
   month = {2},
   pages = {341--372},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J7"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J7" id="tog-J7">
							<i class="left-60 tog far fa-chevron-down"></i>
							Integrating Multimedia Archives: the Architecture and the Content Layer
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">M. Wallace, Th. Athanasiadis, Y. Avrithis, A. Delopoulos, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/1561472" title="Electronic edition">SMC-A,&nbsp;2006</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J07.smc04.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J7.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/tsmca.2005.859184" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tsmc/WallaceAADK06" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:vV6vV6tmYwMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17185502894915623451" title="Citations @ Google Scholar">6</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5051345815abcb230e31cda3c144793cf16c4a82" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/5051345815abcb230e31cda3c144793cf16c4a82#citing-papers" title="Citations @ Semantic Scholar">6</a>
						</div>

					</div>
					<div class="collapse" id="col-J7">
						<div class="pub-ref">
							<em>IEEE Transactions on Systems, Man, and Cybernetics, Part A: Systems and Humans</em><br>
							36(1):34-52  <span class="bull"></span> Jan 2006
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									During the last few years numerous multimedia archives have made extensive use of digitized storage and annotation technologies. Still, the development of single points of access, providing common and uniform access to their data, despite the efforts and accomplishments of standardization organizations, has remained an open issue, as it involves the integration of various large scale heterogeneous and heterolingual systems. In this paper, we describe a mediator system that achieves architectural integration through an extended 3-tier architecture and content integration through semantic modeling. The described system has successfully integrated five multimedia archives, quite different in nature and content from each other, while also providing for easy and scalable inclusion of more archives in the future.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J7,
   title = {Integrating Multimedia Archives: The Architecture and the Content Layer},
   author = {Wallace, Manolis and Athanasiadis, Thanos and Avrithis, Yannis and Delopoulos, Anastasios and Kollias, Stefanos},
   journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part A: Systems and Humans (SMC-A)},
   volume = {36},
   number = {1},
   month = {1},
   pages = {34--52},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2003"></a>
					2003
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J6"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J6" id="tog-J6">
							<i class="left-60 tog far fa-chevron-down"></i>
							Unified Access to Heterogeneous Audiovisual Archives
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, G. Stamou, M. Wallace, F. Marques, Ph. Salembier, X. Giro, W. Haas, H. Vallant, M. Zufferey</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://lib.jucs.org/article/28031/" title="Open access">JUCS,&nbsp;2003</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J06.jucs03.iknow.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J6.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.3217/jucs-009-06-0510" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/jucs/AvrithisSWMSGHV03" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:4DMP91E08xMC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=9022365166745380021" title="Citations @ Google Scholar">11</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/56db283d974e087246b0707ffa145f6397818eb5" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/56db283d974e087246b0707ffa145f6397818eb5#citing-papers" title="Citations @ Semantic Scholar">8</a>
						</div>

					</div>
					<div class="collapse" id="col-J6">
						<div class="pub-ref">
							<em>Journal of Universal Computer Science</em><br>
							9(6):510-519  <span class="bull"></span> Jun 2003
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, an integrated information system is presented that offers enhanced search and retrieval capabilities to users of heterogeneous digital audiovisual (a/v) archives. This innovative system exploits the advances in handling a/v content and related metadata, as introduced by MPEG-4 and worked out by MPEG-7, to offer advanced services characterized by the tri-fold "semantic phrasing of the request (query)", "unified handling" and "personalized response". The proposed system is targeting the intelligent extraction of semantic information from a/v and text related data taking into account the nature of the queries that users my issue, and the context determined by user profiles. It also provides a personalization process of the response in order to provide end-users with desired information. From a technical point of view, the FAETHON system plays the role of an intermediate access server residing between the end users and multiple heterogeneous audiovisual archives organized according to the new MPEG standards.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J6,
   title = {Unified Access to Heterogeneous Audiovisual Archives},
   author = {Avrithis, Yannis and Stamou, Giorgos and Wallace, Manolis and Marques, Ferran and Salembier, Philippe and Giro, Xavier and Haas, Werner and Vallant, Heribert and Zufferey, Michael},
   journal = {Journal of Universal Computer Science (JUCS)},
   volume = {9},
   number = {6},
   month = {6},
   pages = {510--519},
   year = {2003}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2001"></a>
					2001
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J5"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J5" id="tog-J5">
							<i class="left-60 tog far fa-chevron-down"></i>
							Facial Image Indexing in Multimedia Databases
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Tsapatsoulis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/PL00014577" title="Electronic edition">PAA,&nbsp;2001</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J05.paa00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J5.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/pl00014577" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/paa/TsapatsoulisAK01" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:Se3iqnhoufwC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=12345430486462288941" title="Citations @ Google Scholar">37</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/6544eaadd08e6d02d0b4ff84250966dcb6d9cc7a" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/6544eaadd08e6d02d0b4ff84250966dcb6d9cc7a#citing-papers" title="Citations @ Semantic Scholar">31</a>
						</div>

					</div>
					<div class="collapse" id="col-J5">
						<div class="pub-ref">
							<em>Pattern Analysis and Applications</em><br>
							Special issue on <em>Image Indexation</em><br>
							4(2-3):93-107  <span class="bull"></span> Jun 2001
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									Pictures and video sequences showing human faces are of high importance in content-based retrieval systems, and consequently face detection has been established as an important tool in the framework of many multimedia applications like indexing, scene classification and news summarisation. In this work, we combine skin colour and shape features with template matching in an efficient way for the purpose of facial image indexing. We propose an adaptive two-dimensional Gaussian model of the skin colour distribution whose parameters are re-estimated based on the current image or frame, reducing generalisation problems. Masked areas obtained from skin colour detection are processed using morphological tools and assessed using global shape features. The verification stage is based on a template matching variation providing robust detection. Facial images and video sequences are indexed according to the number of included faces, their average colour components and their scale, leading to new types of content-based retrieval criteria in query-by-example frameworks. Experimental results have shown that the proposed implementation combines efficiency, robustness and speed, and could be easily embedded in generic visual information retrieval systems or video databases.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J5,
   title = {Facial Image Indexing in Multimedia Databases},
   author = {Tsapatsoulis, Nicolas and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Pattern Analysis and Applications (PAA) (Special Issue on Image Indexation)},
   volume = {4},
   number = {2--3},
   month = {6},
   pages = {93--107},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J4"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J4" id="tog-J4">
							<i class="left-60 tog far fa-chevron-down"></i>
							Affine-Invariant Curve Normalization for Object Shape Representation, Classification, and Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Xirouhakis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://link.springer.com/article/10.1007/PL00013272" title="Electronic edition">MVA,&nbsp;2001</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J04.mva00.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J4.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/pl00013272" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/mva/AvrithisXK01" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:LkGwnXOMwfcC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=17416973634088322409" title="Citations @ Google Scholar">55</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/355fb0a47d609723c83333602a5e1cbb681bd079" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/355fb0a47d609723c83333602a5e1cbb681bd079#citing-papers" title="Citations @ Semantic Scholar">46</a>
						</div>

					</div>
					<div class="collapse" id="col-J4">
						<div class="pub-ref">
							<em>Machine Vision and Applications</em><br>
							13(2):80-94  <span class="bull"></span> Nov 2001
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A novel method for two-dimensional curve normalization with respect to affine transformations is presented in this paper, which allows an affine-invariant curve representation to be obtained without any actual loss of information on the original curve. It can be applied as a preprocessing step to any shape representation, classification, recognition, or retrieval technique, since it effectively decouples the problem of affine-invariant description from feature extraction and pattern matching. Curves estimated from object contours are first modeled by cubic B-splines and then normalized in several steps in order to eliminate translation, scaling, skew, starting point, rotation, and reflection transformations, based on a combination of curve features including moments and Fourier descriptors.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J4,
   title = {Affine-Invariant Curve Normalization for Object Shape Representation, Classification, and Retrieval},
   author = {Avrithis, Yannis and Xirouhakis, Yiannis and Kollias, Stefanos},
   journal = {Machine Vision and Applications (MVA)},
   volume = {13},
   number = {2},
   month = {11},
   pages = {80--94},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-2000"></a>
					2000
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J3" id="tog-J3">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Fuzzy Video Content Representation for Video Summarization and Content-Based Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">A. Doulamis, N. Doulamis, Y. Avrithis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S0165168400000190" title="Electronic edition">SP,&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J03.spj99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1016/s0165-1684(00)00019-0" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:v1_lew4L6wgC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10952171234810203006" title="Citations @ Google Scholar">161</a>
						</div>

					</div>
					<div class="collapse" id="col-J3">
						<div class="pub-ref">
							<em>Signal Processing</em><br>
							Special issue on <em>Fuzzy Logic in Signal Processing</em><br>
							80(6):1049-1067  <span class="bull"></span> Jun 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									In this paper, a fuzzy representation of visual content is proposed, which is useful for the new emerging multimedia applications, such as content-based image indexing and retrieval, video browsing and summarization. In particular, a multidimensional fuzzy histogram is constructed for each video frame based on a collection of appropriate features, extracted using video sequence analysis techniques. This approach is then applied both for video summarization, in the context of a content-based sampling algorithm, and for content-based indexing and retrieval. In the first case, video summarization is accomplished by discarding shots or frames of similar visual content so that only a small but meaningful amount of information is retained (key-frames). In the second case, a content-based retrieval scheme is investigated, so that the most similar images to a query are extracted. Experimental results and comparison with other known methods are presented to indicate the good performance of the proposed scheme on real-life video recordings.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J3,
   title = {A Fuzzy Video Content Representation for Video Summarization and Content-Based Retrieval},
   author = {Doulamis, Anastasios and Doulamis, Nikolaos and Avrithis, Yannis and Kollias, Stefanos},
   journal = {Signal Processing (SP) (Special Issue on Fuzzy Logic in Signal Processing)},
   volume = {80},
   number = {6},
   month = {6},
   pages = {1049--1067},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J2" id="tog-J2">
							<i class="left-60 tog far fa-chevron-down"></i>
							Efficient Summarization of Stereoscopic Video Sequences
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">N. Doulamis, A. Doulamis, Y. Avrithis, K. Ntalianis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://ieeexplore.ieee.org/document/844996" title="Electronic edition">CSVT,&nbsp;2000</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J02.csvt99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1109/76.844996" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/tcsv/Doulamis00" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:d1gkVwhDpl0C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5921830990220809695" title="Citations @ Google Scholar">132</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/7d05c69c38b71defd06f4444601a2434c7cd7d6f" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/7d05c69c38b71defd06f4444601a2434c7cd7d6f#citing-papers" title="Citations @ Semantic Scholar">98</a>
						</div>

					</div>
					<div class="collapse" id="col-J2">
						<div class="pub-ref">
							<em>IEEE Transactions on Circuits and Systems for Video Technology</em><br>
							Special issue on <em>{3D} Video Technology</em><br>
							10(4):501-517  <span class="bull"></span> Jun 2000
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									An efficient technique for summarization of stereoscopic video sequences is presented in this paper, which extracts a small but meaningful set of video frames using a content-based sampling algorithm. The proposed video-content representation provides the capability of browsing digital stereoscopic video sequences and performing more efficient content-based queries and indexing. Each stereoscopic video sequence is first partitioned into shots by applying a shot-cut detection algorithm so that frames (or stereo pairs) of similar visual characteristics are gathered together. Each shot is then analyzed using stereo-imaging techniques, and the disparity field, occluded areas, and depth map are estimated. A multiresolution implementation of the Recursive Shortest Spanning Tree (RSST) algorithm is applied for color and depth segmentation, while fusion of color and depth segments is employed for reliable video object extraction. In particular, color segments are projected onto depth segments so that video objects on the same depth plane are retained, while at the same time accurate object boundaries are extracted. Feature vectors are then constructed using multidimensional fuzzy classification of segment features including size, location, color, and depth. Shot selection is accomplished by clustering similar shots based on the generalized Lloyd-Max algorithm, while for a given shot, key frames are extracted using an optimization method for locating frames of minimally correlated feature vectors. For efficient implementation of the latter method, a genetic algorithm is used. Experimental results are presented, which indicate the reliable performance of the proposed scheme on real-life stereoscopic video sequences.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J2,
   title = {Efficient Summarization of Stereoscopic Video Sequences},
   author = {Doulamis, Nikolaos and Doulamis, Anastasios and Avrithis, Yannis and Ntalianis, Klimis and Kollias, Stefanos},
   journal = {IEEE Transactions on Circuits and Systems for Video Technology (CSVT) (Special Issue on {3D} Video Technology)},
   volume = {10},
   number = {4},
   month = {6},
   pages = {501--517},
   year = {2000}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="journ-1999"></a>
					1999
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="J1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-J1" id="tog-J1">
							<i class="left-60 tog far fa-chevron-down"></i>
							A Stochastic Framework for Optimal Key Frame Extraction from MPEG Video Databases
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, A. Doulamis, N. Doulamis, S. Kollias</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.sciencedirect.com/science/article/abs/pii/S1077314299907610" title="Electronic edition">CVIU,&nbsp;1999</a>
							<a class="lnk mr" href="../data/pub/pdf/journ/J01.cviu99.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/journ/J1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1006/cviu.1999.0761" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/journals/cviu/AvrithisDDK99" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:N5tVd3kTz84C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5077865837034791527" title="Citations @ Google Scholar">137</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/45cefc106eee8bd424edb9fb565c6f13f9cbfcf8" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/45cefc106eee8bd424edb9fb565c6f13f9cbfcf8#citing-papers" title="Citations @ Semantic Scholar">127</a>
						</div>

					</div>
					<div class="collapse" id="col-J1">
						<div class="pub-ref">
							<em>Computer Vision and Image Understanding</em><br>
							Special issue on <em>Content-Based Access of Image and Video Libraries</em><br>
							75(1-2):3-24  <span class="bull"></span> Jul 1999
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									A video content representation framework is proposed in this paper for extracting limited, but meaningful, information of video data, directly from the MPEG compressed domain. A hierarchical color and motion segmentation scheme is applied to each video shot, transforming the frame-based representation to a feature-based one. The scheme is based on a multiresolution implementation of the recursive shortest spanning tree (RSST) algorithm. Then, all segment features are gathered together using a fuzzy multidimensional histogram to reduce the possibility of classifying similar segments to different classes. Extraction of several key frames is performed for each shot in a content-based rate-sampling framework. Two approaches are examined for key frame extraction. The first is based on examination of the temporal variation of the feature vector trajectory; the second is based on minimization of a cross-correlation criterion of the video frames. For efficient implementation of the latter approach, a logarithmic search (along with a stochastic version) and a genetic algorithm are proposed. Experimental results are presented which illustrate the performance of the proposed techniques, using synthetic and real life MPEG video sequences.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@article{J1,
   title = {A Stochastic Framework for Optimal Key Frame Extraction from {MPEG} Video Databases},
   author = {Avrithis, Yannis and Doulamis, Anastasios and Doulamis, Nikolaos and Kollias, Stefanos},
   journal = {Computer Vision and Image Understanding (CVIU) (Special Issue on Content-Based Access of Image and Video Libraries)},
   volume = {75},
   number = {1--2},
   month = {7},
   pages = {3--24},
   year = {1999}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="thesis"></a>
					<span class="mr">Theses</span>
					<i class="fal fa-scroll"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="thesis-2020"></a>
					2020
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="T4"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-T4" id="tog-T4">
							<i class="left-60 tog far fa-chevron-down"></i>
							Exploring and Learning from Visual Data
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://hal.inria.fr/tel-03047624" title="Open access">UR1,&nbsp;2020</a>
							<a class="xtra but mr" href="https://videos-rennes.inria.fr/video/BJ4pOU0ND">Talk</a>
							<a class="lnk mr" href="../data/pub/pdf/thesis/T04.hdr-v0.4.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/thesis/T4.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/thesis/T04.hdr-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://dblp.org/rec/books/hal/Avrithis20" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:yFnVuubrUp4C" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/5dba0e9da00ebbb0b6afa60dfab82a5c170949dc" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-T4">
						<div class="pub-ref">
							HDR Thesis<br>
							University of Rennes 1<br>
							Rennes, France  <span class="bull"></span> Jul 2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/thesis/T04.hdr.svg"><img src="../data/pub/thumb/wide/thesis/T04.hdr.svg" alt="T4 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This manuscript is about a journey. The journey of computer vision and machine learning research from the early years of Gabor filters and linear classifiers to surpassing human skills in several tasks today. The journey of the author's own research, designing representations and matching processes to explore visual data and exploring visual data to learn better representations.
								</p>
								<p>
									Part I addresses instance-level visual search and clustering, building on shallow visual representations and matching processes. The representation is obtained by a pipeline of local features, hand-crafted descriptors and visual vocabularies. Improvements in the pipeline are introduced, including the construction of large-scale vocabularies, spatial matching for geometry verification, representations beyond vocabularies and nearest neighbor search. Applications to exploring photo collections are discussed, including location recognition, landmark recognition and automatic discovery of photos depicting the same scene.
								</p>
								<p>
									Part II addresses instance-level visual search and object discovery, building on deep visual representations and matching processes, focusing on the manifold structure of the feature space. The representation is obtained by deep parametric models learned from visual data. Contributions are made to advancing manifold search over global or regional CNN representations. This process is seen as graph filtering, including spatial and spectral. Spatial matching is revisited with local features detected on CNN activations. Finally, a method is introduced for object discovery from CNN activations over an unlabeled image collection.
								</p>
								<p>
									Part III addresses learning deep visual representations by exploring visual data, focusing on limited or no supervision. It progresses from instance-level to category-level tasks and studies the sensitivity of models to their input. It introduces methods for unsupervised metric learning and semi-supervised learning, based again on the manifold structure of the feature space. It contributes to few-shot learning, studying activation maps and learning multiple layers to convergence for the first time. Finally, it introduces an attack as an attempt to improve upon the visual quality of adversarial examples in terms of imperceptibility.
								</p>
								<p>
									Part IV summarizes more of the author's past and present contributions, reflects on these contributions in the present context and consolidates the ideas exposed in this manuscript. It then attempts to draw a road map of ideas that are likely to come.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@phdthesis{T4,
   type = {HDR Thesis},
   title = {Exploring and Learning from Visual Data},
   author = {Avrithis, Yannis},
   school = {University of Rennes 1 (UR1)},
   month = {7},
   address = {Rennes, France},
   year = {2020}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="thesis-2001"></a>
					2001
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="T3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-T3" id="tog-T3">
							<i class="left-60 tog far fa-chevron-down"></i>
							Video Sequence Analysis for Content Description, Summarization and Content-Based Retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.didaktorika.gr/eadd/handle/10442/0778" title="Electronic edition">NTUA,&nbsp;2001</a>
							<a class="lnk mr" href="../data/pub/pdf/thesis/T03.phd.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/thesis/T3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/thesis/T03.phd-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.12681/eadd/0778" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:kVjdVfd2voEC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=10558507960745893862" title="Citations @ Google Scholar">2</a>
						</div>

					</div>
					<div class="collapse" id="col-T3">
						<div class="pub-ref">
							PhD Thesis<br>
							School of Electrical and Computer Engineering<br>
							National Technical University of Athens<br>
							Athens, Greece  <span class="bull"></span> Feb 2001
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/thesis/T03.phd.svg"><img src="../data/pub/thumb/wide/thesis/T03.phd.svg" alt="T3 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The main research area of this Ph.D. thesis is image and video sequence processing and analysis for description and indexing of their visual content. Its objective is to contribute to the development of an automated computational system that has the capabilities of object-based segmentation of audiovisual material, automatic content description and annotation, summarization for preview and browsing, as well as content-based search and retrieval. The thesis consists of four parts.
								</p>
								<p>
									The first part introduces video sequence analysis, segmentation and object extraction based on color, motion, as well as depth field in the case of stereoscopic video sequences. A fusion technique is proposed that combines individual cue segmentations and allows for reliable identification of semantic objects.
								</p>
								<p>
									The second part refers to automatic annotation of the visual content by means of feature vectors calculated by multidimensional fuzzy classification of low-level object descriptors. This information is used for summarization, which is implemented by optimal selection of a limited set of key frames and shots providing meaningful visual content description. The representation of the selected material by feature vectors is then employed for content-based search and retrieval.
								</p>
								<p>
									In the third part, the problem of object contour analysis and representation is examined, with application to shape-based object classification and retrieval. An original contour normalization scheme is presented, permitting invariant shape representation with respect to a large number of transformations without any actual loss of information.
								</p>
								<p>
									In the fourth part, a novel technique is proposed for temporal segmentation and parsing of broadcast news recordings into elementary story units or news topics using visual cues. The technique is based on an advanced algorithm for automatic detection of human faces; the extracted information is also employed for the development of new semantic criteria for content-based retrieval.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@phdthesis{T3,
   title = {Video Sequence Analysis for Content Description, Summarization and Content-Based Retrieval},
   author = {Avrithis, Yannis},
   school = {School of Electrical and Computer Engineering; National Technical University of Athens (NTUA)},
   month = {2},
   address = {Athens, Greece},
   year = {2001}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="thesis-1994"></a>
					1994
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="T2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-T2" id="tog-T2">
							<i class="left-60 tog far fa-chevron-down"></i>
							Investigating the Capacity of a Cellular CDMA System
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">IC,&nbsp;1994</span>
							<a class="lnk mr" href="../data/pub/pdf/thesis/T02.msc.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/thesis/T2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
							<a class="lnk mr2" href="../data/pub/slides/thesis/T02.msc-slides.pdf" title="Slides">
								<i class="fal fa-presentation-screen"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://scholar.google.com/citations?view_op=view_citation&citation_for_view=AF2SxG0AAAAJ:_FM0Bhl9EiAC" title="@ Google Scholar">
									<i class="faa fa-google-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-gs mr2" href="https://scholar.google.com/scholar?cites=5700482205992212873" title="Citations @ Google Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-T2">
						<div class="pub-ref">
							Masters Thesis<br>
							Department of Electrical and Electronic Engineering<br>
							Imperial College of Science, Technology and Medicine, University of London<br>
							London, UK  <span class="bull"></span> Oct 1994
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/thesis/T02.msc.png"><img src="../data/pub/thumb/wide/thesis/T02.msc.png" alt="T2 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Code Division Multiple Access (CDMA) is a multiple access scheme based on spread spectrum techniques, that has been used for many years for military communications, and quite recently for commercial applications, such as satellite and digital cellular radio communications. Certain inherent characteristics of spread spectrum, such as interference and multipath suppression capabilities, privacy, and more efficient spectrum reuse, make CDMA advantageous for mobile cellular communications.
								</p>
								<p>
									It is the intention of this project to provide an examination of how these properties of CDMA can be used to increase capacity comparing with conventional multiple access techniques. For this purpose, the capacity of a single-cell, power controlled, asynchronous direct-sequence (DS) CDMA system is first investigated using Gold codes and both binary and quadrature phase-shift-keying (BPSK and QPSK) modulation.
								</p>
								<p>
									The investigation carries on with the calculation of both the forward and reverse link capacity of a multiple-cell CDMA system by means of analytical calculations and Monte Carlo simulations. Finally, a straightforward comparison with conventional techniques such as FDMA and TDMA shows that CDMA can indeed provide much higher capacity.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@mastersthesis{T2,
   title = {Investigating the Capacity of a Cellular {CDMA} System},
   author = {Avrithis, Yannis},
   school = {Department of Electrical and Electronic Engineering; Imperial College of Science, Technology and Medicine, University of London (IC)},
   month = {10},
   address = {London, UK},
   year = {1994}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="thesis-1993"></a>
					1993
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="T1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-T1" id="tog-T1">
							<i class="left-60 tog far fa-chevron-down"></i>
							Fuzzy Logic Processor for Control Systems
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">NTUA,&nbsp;1993</span>
							<a class="lnk mr" href="../data/pub/pdf/thesis/T01.diploma.pdf" title="Paper PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/thesis/T1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-T1">
						<div class="pub-ref">
							Masters Thesis<br>
							School of Electrical and Computer Engineering<br>
							National Technical University of Athens<br>
							Athens, Greece  <span class="bull"></span> Sep 1993
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/thesis/T01.diploma.png"><img src="../data/pub/thumb/wide/thesis/T01.diploma.png" alt="T1 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Fuzzy logic is a recent branch of mathematics that allows modelling of the imprecise way of reasoning that plays an important role in the ability of humans to make decisions in uncertain environments. Interest in fuzzy logic has grown in the last decade, with its successful use in diverse fields of study including expert systems, control systems, artificial intelligence, signal and image processing, computer vision, robotics, medical diagnosis, finance and decision support systems.
								</p>
								<p>
									The first part of this thesis addresses the alternative types of operations that are available at each stage of computation, carries out a theoretical study of how these operations need to be combined into a consistent logical system and investigates new, simpler computational methods and conditions under which such methods can be used.
								</p>
								<p>
									The second part concerns the analysis, design and implementation of a fuzzy rule-based controller, including a mechanical construction and an electronic circuit. The objective is to control the position of a ball inside a vertical transparent tube by measuring the actual position via an ultrasound sensor on top of the tube and accordingly adjusting the voltage applied to a fan that is fixed underneath. The fuzzy processor is implemented with discrete components and all computations are entirely analog, without any intermediate A/D or D/A conversion. Fuzzy membership functions and rules are specified by manually adjusting an array of potentiometers. This allows studying the effect of different parameters on the behavior of the controller in tasks like stabilizing the ball or moving it quickly between two positions. This work serves as a demonstrator and a testbed for future implementations with analog VLSI circuits.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@mastersthesis{T1,
   title = {Fuzzy Logic Processor for Control Systems},
   author = {Avrithis, Yannis},
   school = {School of Electrical and Computer Engineering; National Technical University of Athens (NTUA)},
   month = {9},
   address = {Athens, Greece},
   year = {1993}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="vol"></a>
					<span class="mr">Edited volumes</span>
					<i class="fal fa-book"></i>
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="vol-2009"></a>
					2009
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="V3"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-V3" id="tog-V3">
							<i class="left-60 tog far fa-chevron-down"></i>
							<em>Advances in Multimedia Modeling</em>
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">B. Huet, A. Smeaton, K. Mayer-Patel, Y. Avrithis, eds.</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.springer.com/gp/book/9783540928911" title="Electronic edition">LNCS,&nbsp;2009</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/vol/V3.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/978-3-540-92892-8" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/mmm/2009" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
								<a class="lnk ext mr2" href="https://www.semanticscholar.org/paper/84db06ac24741f86bf43914f3c6dbce06854807d" title="@ Semantic Scholar">
									<i class="faa fa-semantic-scholar"></i>
								</a>
								<a class="cite dense strong lnk ext ml-ss mr2" href="https://www.semanticscholar.org/paper/84db06ac24741f86bf43914f3c6dbce06854807d#citing-papers" title="Citations @ Semantic Scholar">1</a>
						</div>

					</div>
					<div class="collapse" id="col-V3">
						<div class="pub-ref">
							Lecture Notes in Computer Science<br>
							Vol. 5371  <span class="bull"></span> Jan 2009<br>
							Springer <span class="bull"></span> ISBN 978-3-540-92891-1
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/vol/V03.jpg"><img src="../data/pub/thumb/wide/vol/V03.jpg" alt="V3 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This book constitutes the refereed proceedings of the 15th International Multimedia Modeling Conference, MMM 2009, held in Sophia-Antipolis, France, in January 2009. The 26 revised full papers and 20 revised poster papers presented together with 2 invited talks were carefully reviewed and selected from 135 submissions. The papers are organized in topical sections on automated annotation, coding and streaming, video semantics and relevance, audio, recognition, classification and retrieval, as well as query and summarization.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@book{V3,
   title = {Advances in Multimedia Modeling},
   editor = {Huet, Benoit and Smeaton, Alan and Mayer-Patel, Ketan and Avrithis, Yannis},
   publisher = {Springer},
   series = {Lecture Notes in Computer Science (LNCS)},
   volume = {5371},
   month = {1},
   isbn = {978-3-540-92891-1},
   year = {2009}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="vol-2007"></a>
					2007
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="V2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-V2" id="tog-V2">
							<i class="left-60 tog far fa-chevron-down"></i>
							<em>Semantic Multimedia</em>
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">B. Falcidieno, M. Spagnuolo, Y. Avrithis, I. Kompatsiaris, P. Buitelaar, eds.</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.springer.com/gp/book/9783540770336" title="Electronic edition">LNCS,&nbsp;2007</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/vol/V2.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/11930334" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/2006" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-V2">
						<div class="pub-ref">
							Lecture Notes in Computer Science<br>
							Vol. 4816  <span class="bull"></span> Dec 2007<br>
							Springer <span class="bull"></span> ISBN 978-3-540-77033-6
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/vol/V02.jpg"><img src="../data/pub/thumb/wide/vol/V02.jpg" alt="V2 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This book constitutes the refereed proceedings of the Second International Conference on Semantics and Digital Media Technologies, SAMT 2007, held in Genoa, Italy, in December 2007. The 16 revised full papers, 10 revised short papers and 10 poster papers presented together with three awarded PhD papers were carefully reviewed and selected from 55 submissions. The conference brings together forums, projects, institutions and individuals investigating the integration of knowledge, semantics and low-level multimedia processing, including new emerging media and application areas. The papers are organized in topical sections on knowledge based content processing, semantic multimedia annotation, domain-restricted generation of semantic metadata from multimodal sources, classification and annotation of multidimensional content, content adaptation, MX: the IEEE standard for interactive music, as well as poster papers and K-space awarded PhD papers.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@book{V2,
   title = {Semantic Multimedia},
   editor = {Falcidieno, Bianca and Spagnuolo, Michela and Avrithis, Yannis and Kompatsiaris, Ioannis and Buitelaar, Paul},
   publisher = {Springer},
   series = {Lecture Notes in Computer Science (LNCS)},
   volume = {4816},
   month = {12},
   isbn = {978-3-540-77033-6},
   year = {2007}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="vol-2006"></a>
					2006
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="V1"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-V1" id="tog-V1">
							<i class="left-60 tog far fa-chevron-down"></i>
							<em>Semantic Multimedia</em>
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">Y. Avrithis, Y. Kompatsiaris, S. Staab, N. O'Connor, eds.</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://www.springer.com/gp/book/9783540493358" title="Electronic edition">LNCS,&nbsp;2006</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../data/pub/bib/vol/V1.bib" title="BibTeX citation">
								<i class="fal fa-comment-quote"></i>
							</a>
						</div>
						<div class="part nw">
								<a class="lnk ext mr2" href="https://doi.org/10.1007/11930334" title="@ DOI">
									<i class="faa fa-doi"></i>
								</a>
								<a class="lnk ext mr2" href="https://dblp.org/rec/conf/samt/2006" title="@ DBLP">
									<i class="faa fa-dblp"></i>
								</a>
						</div>

					</div>
					<div class="collapse" id="col-V1">
						<div class="pub-ref">
							Lecture Notes in Computer Science<br>
							Vol. 4306  <span class="bull"></span> Dec 2006<br>
							Springer <span class="bull"></span> ISBN 978-3-540-49335-8
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/vol/V01.jpg"><img src="../data/pub/thumb/wide/vol/V01.jpg" alt="V1 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This book constitutes the refereed proceedings of the First International Conference on Semantics and Digital Media Technologies, SAMT 2006, held in Athens, Greece in December 2006. The 17 revised full papers presented together with a invited keynote paper were carefully reviewed and selected from 68 submissions. SAMT 2006 targets to narrow the "Semantic Gap", i.e. the large disparity between the low-level descriptors that can be computed automatically from multimedia content and the richness and subjectivity of semantics in user queries and human interpretations of audiovisual media. The papers address a wide area of integrative research on new knowledge-based forms of digital media systems, semantics and low-level multimedia processing.
								</p>
							</div>
							<figure class="pub-bib highlight">
								<pre><code>@book{V1,
   title = {Semantic Multimedia},
   editor = {Avrithis, Yannis and Kompatsiaris, Yiannis and Staab, Steffen and O'Connor, Noel},
   publisher = {Springer},
   series = {Lecture Notes in Computer Science (LNCS)},
   volume = {4306},
   month = {12},
   edition = {1st},
   isbn = {978-3-540-49335-8},
   year = {2006}
}</code></pre>
							</figure>
						</div>
					</div>
				</div>


			</div>
		</main>

	</div>
</div>
<footer class="bd-footer">
	<div class="container-fluid p-3 p-md-5">
		<p>
			Built using <a href="https://shopify.github.io/liquid/">Liquid</a>, <a href="https://jekyllrb.com/">Jekyll</a> and <span class="bright">my own code</span> on top
			<span class="bull">
			</span> Styled using <a href="https://getbootstrap.com/">Bootstrap</a>, <a href="https://www.mathjax.org/">MathJax</a>, <a href="https://fontawesome.com/">Font Awesome</a> and <a href="https://fonts.google.com/specimen/Roboto/">Google Fonts</a>
		</p>
		<p>
			Hosted by <a href="https://github.com/iavr/iavr.github.io">GitHub</a>
			<span class="bull"></span>
			Domain by <a href="https://www.name.com/">name.com</a>
			<span class="bull"></span>
			Last updated <span class="bright">May 16, 2024</span>
		</p>
		<p>
			Copyright notice applies to all <a href="../pub/#copyright">publications</a>
			<span class="bull"></span>
			All remaining material licensed <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>
		</p>
	</div>
</footer>

	</body>
</html>
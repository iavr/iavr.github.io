<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<meta name="author" content="Yannis Avrithis">
		<meta name="description" content="Yannis Avrithis - Home page">
		<meta name="keywords" content="Yannis Avrithis computer vision machine learning deep learning image search indexing retrieval">
		<title>Yannis Avrithis - Code and Data (by author)</title>

		<!-- bootstrap -->
		<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
		<script src="https://code.jquery.com/jquery-3.3.1.min.js"></script>
		<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
		<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

		<!-- fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500" rel="stylesheet">
		<link href="https://fonts.googleapis.com/css?family=Roboto+Condensed:300" rel="stylesheet">

		<!-- mathjax -->
		<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML" async></script>

		<!-- font awesome -->
		<script defer src="../web/js/fa.js"></script>

		<!-- styles -->
		<link rel="stylesheet" href="../web/css/home.css">

		<!-- scripts -->
		<script src="../web/js/bs-docs.min.js"></script>
		<script src="../web/js/email.js"></script>
		<script src="../web/js/scroll.js"></script>

		<!-- favicon -->
		<link rel="apple-touch-icon" sizes="180x180" href="../web/ico/apple-touch-icon.png">
		<link rel="icon" type="image/png" sizes="32x32" href="../web/ico/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="../web/ico/favicon-16x16.png">
		<link rel="manifest" href="/site.webmanifest">

	</head>
	<body data-spy="scroll" data-target="#nav">
		<header class="navbar navbar-expand navbar-dark flex-column flex-md-row bd-navbar">
			<a class="navbar-brand mr-0 mr-md-2 brand" href="../">Y</a>
			<div class="navbar-nav-scroll">
				<ul class="navbar-nav bd-navbar-nav flex-row">
					<li class="nav-item">
						<a class="nav-link" href="../">Home</a>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="../cv">Resume</a>
					</li>
					<li class="nav-item dropdown">
						<a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-expanded="false">
							Publications
						</a>
						<div class="dropdown-menu">
							<a class="dropdown-item" href="../pub">By year</a>
							<div class="dropdown-divider"></div>
							<a class="dropdown-item" href="../pub-cite">By citations</a>
							<a class="dropdown-item" href="../pub-type">By type</a>
							<a class="dropdown-item" href="../pub-auth">By author</a>
						</div>
					</li>
					</li>
					<li class="nav-item dropdown">
						<a class="nav-link dropdown-toggle active" href="#" role="button" data-toggle="dropdown" aria-expanded="false">
							Code/Data
						</a>
						<div class="dropdown-menu">
							<a class="dropdown-item" href="../code">By type</a>
							<div class="dropdown-divider"></div>
							<a class="dropdown-item" href="../code-star">By stars</a>
							<a class="dropdown-item" href="../code-year">By year</a>
							<a class="dropdown-item active" href="../code-auth">By author</a>
						</div>
					</li>
					</li>
					<li class="nav-item">
						<a class="nav-link" href="../time">Timeline</a>
					</li>
				</ul>
			</div>
			<ul class="navbar-nav flex-row ml-md-auto d-none d-md-flex">
				<li class="nav-item">
					<script>ema2(ema_net(), "nav-link p-2", "Email");</script>
						<i class="fal fa-at"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://arxiv.org/search/?searchtype=author&query=Avrithis%2C+Y" title="arXiv">
						<i class="faa fa-arxiv"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://dblp.org/pers/hd/a/Avrithis:Yannis" title="DBLP">
						<i class="faa fa-dblp"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://scholar.google.com/citations?user=AF2SxG0AAAAJ&sortby=pubdate" title="Google Scholar">
						<i class="faa fa-google-scholar"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://www.semanticscholar.org/author/Yannis-Avrithis/1744904" title="Semantic Scholar">
						<i class="faa fa-semantic-scholar"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://www.linkedin.com/in/yannisavrithis/" title="LinkedIn">
						<i class="fab fa-linkedin-in"></i>
					</a>
				</li>
				<li class="nav-item">
					<a class="nav-link p-2" href="https://github.com/iavr" title="GitHub">
						<i class="fab fa-github"></i>
					</a>
				</li>
			</ul>
		</header>



<div class="container-fluid" id="pub">
	<div class="row">

		<div class="col-md-auto side-bar">
			<nav id="nav">

				<div class="show-md">
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#A-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#A">
							A
						</a>
						<div class="collapse show nav side-nav full" id="A-col">
							<a class="nav-link" href="#yannis-avrithis">Avrithis, Y.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#B-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#B">
							B
						</a>
						<div class="collapse show nav side-nav full" id="B-col">
							<a class="nav-link" href="#mateusz-budnik">Budnik, M.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#E-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#E">
							E
						</a>
						<div class="collapse show nav side-nav full" id="E-col">
							<a class="nav-link" href="#deniz-engin">Engin, D.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#I-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#I">
							I
						</a>
						<div class="collapse show nav side-nav full" id="I-col">
							<a class="nav-link" href="#ahmet-iscen">Iscen, A.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#J-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#J">
							J
						</a>
						<div class="collapse show nav side-nav full" id="J-col">
							<a class="nav-link" href="#tomas-jenicek">Jenicek, T.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#K-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#K">
							K
						</a>
						<div class="collapse show nav side-nav full" id="K-col">
							<a class="nav-link" href="#ioannis-kakogeorgiou">Kakogeorgiou, I.</a>
							<a class="nav-link" href="#yannis-kalantidis">Kalantidis, Y.</a>
							<a class="nav-link" href="#kimon-kontosis">Kontosis, K.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#L-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#L">
							L
						</a>
						<div class="collapse show nav side-nav full" id="L-col">
							<a class="nav-link" href="#michalis-lazarou">Lazarou, M.</a>
							<a class="nav-link" href="#yann-raphael-lifchitz">Lifchitz, Y.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#M-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#M">
							M
						</a>
						<div class="collapse show nav side-nav full" id="M-col">
							<a class="nav-link" href="#clayton-melina">Melina, C.</a>
							<a class="nav-link" href="#phivos-mylonas">Mylonas, Ph.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#N-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#N">
							N
						</a>
						<div class="collapse show nav side-nav full" id="N-col">
							<a class="nav-link" href="#spyros-nathanail">Nathanail, S.</a>
							<a class="nav-link" href="#timothee-neitthoffer">Neitthoffer, T.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#P-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#P">
							P
						</a>
						<div class="collapse show nav side-nav full" id="P-col">
							<a class="nav-link" href="#marios-phinikettos">Phinikettos, M.</a>
							<a class="nav-link" href="#bill-psomas">Psomas, B.</a>
							<a class="nav-link" href="#lluis-garcia-pueyo">Pueyo, L.G.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#R-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#R">
							R
						</a>
						<div class="collapse show nav side-nav full" id="R-col">
							<a class="nav-link" href="#filip-radenovic">Radenovic, F.</a>
							<a class="nav-link" href="#konstantinos-rapantzikos">Rapantzikos, K.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#S-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#S">
							S
						</a>
						<div class="collapse show nav side-nav full" id="S-col">
							<a class="nav-link" href="#georgios-samaras">Samaras, G.</a>
							<a class="nav-link" href="#miaojing-shi">Shi, M.</a>
							<a class="nav-link" href="#ronan-sicre">Sicre, R.</a>
							<a class="nav-link" href="#oriane-simeoni">Siméoni, O.</a>
							<a class="nav-link" href="#nikos-skalkotos">Skalkotos, N.</a>
							<a class="nav-link" href="#chull-hwan-song">Song, C.H.</a>
							<a class="nav-link" href="#evaggelos-spyrou">Spyrou, E.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#T-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#T">
							T
						</a>
						<div class="collapse show nav side-nav full" id="T-col">
							<a class="nav-link" href="#konstantinos-tertikas">Tertikas, K.</a>
							<a class="nav-link" href="#giorgos-tolias">Tolias, G.</a>
							<a class="nav-link" href="#michele-trevisiol">Trevisiol, M.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#V-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#V">
							V
						</a>
						<div class="collapse show nav side-nav full" id="V-col">
							<a class="nav-link" href="#christos-varytimidis">Varytimidis, C.</a>
							<a class="nav-link" href="#shashanka-venkataramanan">Venkataramanan, S.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#X-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#X">
							X
						</a>
						<div class="collapse show nav side-nav full" id="X-col">
							<a class="nav-link" href="#yonghao-xu">Xu, Y.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#Y-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#Y">
							Y
						</a>
						<div class="collapse show nav side-nav full" id="Y-col">
							<a class="nav-link" href="#zhaohui-yang">Yang, Z.</a>
						</div>
					</div>
					<div class="toc-item">
						<a class="rel toggle" data-toggle="collapse" href="#Z-col">
							<i class="left-60 tog far fa-chevron-down"></i>
						</a>
						<a class="rel" href="#Z">
							Z
						</a>
						<div class="collapse show nav side-nav full" id="Z-col">
							<a class="nav-link" href="#hanwei-zhang">Zhang, H.</a>
						</div>
					</div>
				</div>

				<div class="hide-md">
					<div class="nav side-nav">
						<a class="nav-link" href="#A">A</a>
						<a class="nav-link" href="#B">B</a>
						<a class="nav-link" href="#E">E</a>
						<a class="nav-link" href="#I">I</a>
						<a class="nav-link" href="#J">J</a>
						<a class="nav-link" href="#K">K</a>
						<a class="nav-link" href="#L">L</a>
						<a class="nav-link" href="#M">M</a>
						<a class="nav-link" href="#N">N</a>
						<a class="nav-link" href="#P">P</a>
						<a class="nav-link" href="#R">R</a>
						<a class="nav-link" href="#S">S</a>
						<a class="nav-link" href="#T">T</a>
						<a class="nav-link" href="#V">V</a>
						<a class="nav-link" href="#X">X</a>
						<a class="nav-link" href="#Y">Y</a>
						<a class="nav-link" href="#Z">Z</a>
					</div>
				</div>

			</nav>
		</div>

		<main class="col-md">
			<div class="container bottom-pad">
				<h1 class="pub">
					<a class="anchor" id="A"></a>
					A
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="yannis-avrithis"></a>
					Avrithis, Yannis
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="pynet"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-pynet-yannis-avrithis" id="tog-pynet">
							<i class="left-60 tog far fa-chevron-down"></i>
							PyNet
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/iavr/pynet" title="Code repository">PyNet</a>
						</div>
						<div class="part nw">
							<a class="lnk ext mr2" href="https://github.com/iavr/pynet/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/pynet/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/iavr/pynet/stargazers" title="Stars @ Github">2</a>
						</div>

					</div>
					<div class="collapse" id="col-pynet-yannis-avrithis">
						<div class="pub-ref">
							Python <span class="bull"></span>
							<a href="https://github.com/iavr/pynet/blob/master/LICENSE">BSD</a> license <span class="bull"></span>
							2016
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/pynet.svg"><img src="../data/code/thumb/wide/code/pynet.svg" alt="pynet thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									PyNet is a minimal Python library for dynamic automatic differentiation. The focus is on simplicity and it is meant to accompany the differentiation lecture of <a href="https://sif-dlv.github.io/">Deep Learning for Vision</a> course. It provides a tape-based automatic differentiation mechanism similar to that of <a href="https://pytorch.org/"><a href="https://pytorch.org/">PyTorch</a></a>, allowing dynamic computational graph creation in plain Python code, including loops, conditionals etc. The initial implementation has included both a CPU backend in NumPy and a GPU backend in <a href="https://github.com/NervanaSystems/neon">Neon</a>. This version includes only the CPU backend and is meant for educational purposes.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="iqm"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-iqm-yannis-avrithis" id="tog-iqm">
							<i class="left-60 tog far fa-chevron-down"></i>
							Inverted-quantized $k$-means
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/iavr/iqm" title="Code repository">IQM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C99" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/iqm/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/iqm/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/iavr/iqm/stargazers" title="Stars @ Github">8</a>
						</div>

					</div>
					<div class="collapse" id="col-iqm-yannis-avrithis">
						<div class="pub-ref">
							Matlab, C++ <span class="bull"></span>
							based on <a href="/code/#agm">AGM</a>, <a href="http://yael.gforge.inria.fr/">Yael</a>, <a href="http://github.com/iavr/xio/">xio</a> <span class="bull"></span>
							<a href="https://github.com/iavr/iqm/blob/master/LICENSE">BSD</a> license <span class="bull"></span>
							published in <a href="/pub/#C99">ICCV 2015</a> <span class="bull"></span>
							2015
							<div class="p">
								<span class='struct'>E. Anagnostopoulos:</span> Framework and baselines for small-scale (1M) experiments. <br>
								<span class='struct'>Y. Kalantidis:</span> Framework and baselines for large-scale (100M) experiments. This includes extraction of CNN features for the 100M collection using Caffe and a distributed $k$-means baseline implemented in Spark. <br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C099.iccv15.iqm.svg"><img src="../data/pub/thumb/wide/conf/C099.iccv15.iqm.svg" alt="iqm thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									IQM is an extremely efficient clustering algorithm operating on an extremely compressed data representation, for instance 26 bits/vector. It is a variant of $k$-means that quantizes vectors and uses inverted search from centroids to cells, while dynamically determining the number of clusters, following <a href="/code/#agm">AGM</a>. Using global CNN image representations, IQM scales up to clustering of a collection of 100M images in less than an hour on a single processor.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="drvq"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-drvq-yannis-avrithis" id="tog-drvq">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dimensionality-recursive vector quantization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/iavr/drvq" title="Code repository">DRVQ</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C94" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/drvq" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://sourceforge.net/projects/drvq/" title="Download">
								<i class="faa fa-sourceforge"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/drvq/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/drvq/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/iavr/drvq/stargazers" title="Stars @ Github">10</a>
						</div>

					</div>
					<div class="collapse" id="col-drvq-yannis-avrithis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a> <span class="bull"></span>
							<a href="https://github.com/iavr/drvq/blob/master/LICENSE">BSD</a> license <span class="bull"></span>
							published in <a href="/pub/#C94">ICCV 2013</a> <span class="bull"></span>
							2013
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C094.iccv13b.qc.svg"><img src="../data/pub/thumb/wide/conf/C094.iccv13b.qc.svg" alt="drvq thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									DRVQ is a fast vector quantization method in high-dimensional Euclidean spaces under arbitrary data distributions. It is an approximation of $k$-means that is practically constant in data size and applies to arbitrarily high dimensions but can only scale to a few thousands of centroids. As a by-product of training, a tree structure performs either exact or approximate quantization on trained centroids, the latter being not very precise but extremely fast. The combination of C++ recursive virtual functions for tree implementation with Matlab-like syntax for matrix operations has allowed fast prototyping, readable code and optimal performance in one piece of software.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ivl2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ivl2-yannis-avrithis" id="tog-ivl2">
							<i class="left-60 tog far fa-chevron-down"></i>
							ivl2
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/iavr/ivl2" title="Code repository">ivl2</a>
						</div>
						<div class="part nw">
							<a class="lnk ext mr2" href="https://github.com/iavr/ivl2/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/iavr/ivl2/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/iavr/ivl2/stargazers" title="Stars @ Github">6</a>
						</div>

					</div>
					<div class="collapse" id="col-ivl2-yannis-avrithis">
						<div class="pub-ref">
							C++11 header-only <span class="bull"></span>
							<a href="https://github.com/iavr/ivl2/blob/master/LICENSE">GNU LGPL2/3</a> license <span class="bull"></span>
							2013-2014
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/ivl2.png"><img src="../data/code/thumb/wide/code/ivl2.png" alt="ivl2 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ivl2 is an effort to re-design and re-implement <a href="/code/#ivl">ivl</a> in the C++11 language standard. In contrast to <a href="/code/#ivl">ivl</a>, which has targeted wide adoption, this is an experimental effort targeting exploitation of latest progress in the language to simplify its implementation and generalize its functionality and syntax. It makes full use of new features including variadic templates, template aliases, type inference, <i>rvalue</i> references and move semantics.
								</p>
								<p>
									
								</p>
								<p>
									Its design is centered around a small number of orthogonal concepts that can be combined in arbitrary ways to yield an extremely powerful syntax. Among others, it offers a unique extension of <a href="http://en.cppreference.com/w/cpp/header/type_traits">std::type_traits</a> and <a href="http://en.cppreference.com/w/cpp/utility/tuple">std::tuple</a>, going far beyond the standard design to support views, expression templates, algorithms, and a unique common interface to tuples and static/dynamic arrays. It generalizes C++ iterators and D <a href="http://dlang.org/phobos/std_range.html">ranges</a>. It overloads all C++ operators and functions to automatically support arbitrary combinations of scalars, arrays or tuples in an arbitrary number of arguments, which is not possible with <a href="/code/#ivl">ivl</a> or any C++98 code.
								</p>
								<p>
									
								</p>
								<p>
									ivl2 is a complex and abstract piece of software consisting of hundreds of source files. It offers a unique blend of features not currently available in any other library or language. It is written from scratch and the code is clean, organized and optimized.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="agm"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-agm-yannis-avrithis" id="tog-agm">
							<i class="left-60 tog far fa-chevron-down"></i>
							Approximate Gaussian mixture (Demo version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/research/agm/" title="Home + download">AGM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C91" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-agm-yannis-avrithis">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							published in <a href="/pub/#C91">ECCV 2012</a> <span class="bull"></span>
							2011-2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C091.eccv12a.agm.png"><img src="../data/pub/thumb/wide/conf/C091.eccv12a.agm.png" alt="agm thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									AGM is a clustering method that combines the flexibility of Gaussian mixtures with the scaling properties needed to construct large visual vocabularies for image retrieval. The algorithm can dynamically estimate the number of clusters, which is referred to as <i>expanding Gaussian mixture</i> (EGM). It also provides significant speed-up by employing approximate nearest neighbor search in assigning points to clusters, which is referred to as <i>approximate Gaussian mixture</i> (AGM). This is a demo version on a toy 2D example. The production version is not public.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="hpm-int"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-hpm-int-yannis-avrithis" id="tog-hpm-int">
							<i class="left-60 tog far fa-chevron-down"></i>
							Hough pyramid matching (Internal version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">HPM-int</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C89" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J22" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/hpm/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<script>ema2(ema_net(), "lnk ext mr2", "Email request");</script>
								<i class="fal fa-at"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-hpm-int-yannis-avrithis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#hpm">HPM</a>, <a href="https://opencv.org/">OpenCV</a> <span class="bull"></span>
							published in <a href="/pub/#C89">ICCV 2011</a>, <a href="/pub/#J22">IJCV 2014</a> <span class="bull"></span>
							2012
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/hpm-int.svg"><img src="../data/code/thumb/wide/code/hpm-int.svg" alt="hpm-int thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is an internal version of <a href="/code/#hpm">HPM</a>. The dependence on <a href="/code/#ivl">ivl</a> has been removed and the code has been integrated with <a href="https://opencv.org/">OpenCV</a> data structures for local features. <span class="alrt">It is available upon request.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="fmh"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-fmh-yannis-avrithis" id="tog-fmh">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature map hashing/similarity (Demo version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">FMH</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C85" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J23" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/feature_map_hashing" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<script>ema2(ema_net(), "lnk ext mr2", "Email request");</script>
								<i class="fal fa-at"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-fmh-yannis-avrithis">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							published in <a href="/pub/#C85">ACM-MM 2010</a>, <a href="/pub/#J23">CVIU 2014</a> <span class="bull"></span>
							2009
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/fmh.svg"><img src="../data/code/thumb/wide/code/fmh.svg" alt="fmh thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									<a href="/pub/#C85">FMH</a> is a method for image indexing and retrieval, which integrates appearance with global image geometry in the indexing process, while enjoying robustness against viewpoint change, photometric variations, occlusion, and background clutter. To handle its increased memory requirements, hashing has been subsequently substituted with an automated and unsupervised feature selection model, leading to <a href="/pub/#J23">feature map similarity (FMS)</a>. This version is a prototype of the original idea on a toy 2D example. <span class="alrt">It is available upon request.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="mfd"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-mfd-yannis-avrithis" id="tog-mfd">
							<i class="left-60 tog far fa-chevron-down"></i>
							Medial feature detector
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Avrithis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/tools/mfd/" title="Home + download">MFD</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C88" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-mfd-yannis-avrithis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="https://opencv.org/">OpenCV</a>, <a href="http://www.robots.ox.ac.uk/~vgg/research/affine/det_eval_files/extract_features2.tar.gz">VGG Affine Features</a> <span class="bull"></span>
							published in <a href="/pub/#C88">ICCV 2011</a> <span class="bull"></span>
							2009-2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C088.iccv11b.mfd.svg"><img src="../data/pub/thumb/wide/conf/C088.iccv11b.mfd.svg" alt="mfd thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									MFD is a local feature detector. Given an input image, it gives access to all intermediate results including a (weighted) <i>distance transform</i>, (weighted) <i>medial axis</i>, an <i>image partition</i> generalizing topological watershed, and the detected features with optional descriptors using the <a href="http://www.robots.ox.ac.uk/~vgg/research/affine/">VGG software</a>. MFD also provides detailed statistics through several commands and options, including interactive visualization and debugging. It can operate in batch mode, optionally recursing subfolders. It has a special mode for binary images providing faster implementation, useful for binary distance transform and medial axis. In this case it also offers sub-pixel accuracy. The code is highly optimized, with running times in the order of 0.5 seconds for an image of 1Mpixel. A 15-page <a href="http://image.ntua.gr/iva/tools/mfd/mfd.0.6.pdf">documentation</a> is provided.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="B"></a>
					B
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="mateusz-budnik"></a>
					Budnik, Mateusz
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="aml"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-aml-mateusz-budnik" id="tog-aml">
							<i class="left-60 tog far fa-chevron-down"></i>
							Asymmetric metric learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Budnik
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/budnikm/asymmetric_metric_learning" title="Code repository">AML</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C117" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/budnikm/asymmetric_metric_learning/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/budnikm/asymmetric_metric_learning/stargazers" title="Stars @ Github">3</a>
						</div>

					</div>
					<div class="collapse" id="col-aml-mateusz-budnik">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/filipradenovic/cnnimageretrieval-pytorch">CIR-torch</a> <span class="bull"></span>
							published in <a href="/pub/#C117">CVPR 2021</a> <span class="bull"></span>
							2020-2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C117.cvpr21.aml.svg"><img src="../data/pub/thumb/wide/conf/C117.cvpr21.aml.svg" alt="aml thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Focusing on instance-level image retrieval, we study an asymmetric testing task, where the database is represented by the teacher and queries by the student. Inspired by this task, we introduce a novel paradigm of using asymmetric representations at training. This acts as a simple combination of knowledge transfer with the original metric learning task. The code allows the reproduction of the results of our <a href="/pub/#C117">CVPR 2021 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ssal"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ssal-mateusz-budnik" id="tog-ssal">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semi-supervised active learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Budnik, O. Siméoni
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/osimeoni/RethinkingDeepActiveLearning" title="Code repository">SSAL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C113" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/stargazers" title="Stars @ Github">15</a>
						</div>

					</div>
					<div class="collapse" id="col-ssal-mateusz-budnik">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/facebookresearch/deepcluster">DeepCluster</a>, <a href="/code/#dlp">DLP</a> <span class="bull"></span>
							<a href="https://github.com/osimeoni/RethinkingDeepActiveLearning/blob/main/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C113">ICPR 2020</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C113.icpr20.active.svg"><img src="../data/pub/thumb/wide/conf/C113.icpr20.active.svg" alt="ssal thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a deep active learning framework allowing a systematic evaluation of different acquisition functions with or without methods that make use of the unlabeled data during model training. In particular, this includes (i) unsupervised pre-training, as implemented by <a href="https://github.com/facebookresearch/deepcluster">DeepCluster</a>, and (ii) semi-supervised learning, as implemented by our <a href="/code/#dlp">deep label propagation</a>. The code allows the reproduction of the results of our <a href="/pub/#C113">ICPR 2020 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="E"></a>
					E
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="deniz-engin"></a>
					Engin, Deniz
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="vitis"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-vitis-deniz-engin" id="tog-vitis">
							<i class="left-60 tog far fa-chevron-down"></i>
							Video Question Answering with Multi-Modal Prompts
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								D. Engin
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/engindeniz/vitis" title="Code repository">ViTiS</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C130" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://engindeniz.github.io/vitis" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/engindeniz/vitis/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/engindeniz/vitis/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/engindeniz/vitis/stargazers" title="Stars @ Github">9</a>
						</div>

					</div>
					<div class="collapse" id="col-vitis-deniz-engin">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/antoyang/FrozenBiLM">FrozenBiLM</a>, <a href="https://github.com/THUDM/P-tuning-v2/">P-tuning-v2</a> <span class="bull"></span>
							<a href="https://github.com/engindeniz/vitis/blob/main/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C130">CLVL/ICCV 2023</a> <span class="bull"></span>
							2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C130.iccv-clvl23.vitis.png"><img src="../data/pub/thumb/wide/conf/C130.iccv-clvl23.vitis.png" alt="vitis thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ViTiS is a parameter-efficient method for adaptation of large-scale pretrained vision-language models on limited data, addressing challenges such as overfitting, catastrophic forgetting, and the cross-modal gap between vision and language. It combines multimodal prompt learning and a transformer-based mapping network, while keeping the pretrained models frozen. We apply it to Zero-Shot and Few-Shot Video Question Answering. The code allows the reproduction of the results of our <a href="/pub/#C130">CLVL/ICCV 2023 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="videoqa"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-videoqa-deniz-engin" id="tog-videoqa">
							<i class="left-60 tog far fa-chevron-down"></i>
							Video question answering
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								D. Engin
								(<span class="ref">advised by</span> N. Q. K. Duong, F. Schnitzler, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/InterDigitalInc/DialogSummary-VideoQA" title="Code repository">VideoQA</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C119" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://engindeniz.github.io/dialogsummary-videoqa" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/InterDigitalInc/DialogSummary-VideoQA/blob/main/license.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/InterDigitalInc/DialogSummary-VideoQA/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/InterDigitalInc/DialogSummary-VideoQA/stargazers" title="Stars @ Github">10</a>
						</div>

					</div>
					<div class="collapse" id="col-videoqa-deniz-engin">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/noagarcia/ROLL-VideoQA">ROLL-VideoQA</a> <span class="bull"></span>
							<a href="https://github.com/InterDigitalInc/DialogSummary-VideoQA/blob/main/license.txt">Proprietary</a> license <span class="bull"></span>
							published in <a href="/pub/#C119">ICCV 2021</a> <span class="bull"></span>
							2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C119.iccv21.vqa.svg"><img src="../data/pub/thumb/wide/conf/C119.iccv21.vqa.svg" alt="videoqa thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a Video Question Answering (VideoQA) method, where we address understanding of stories in video such as movies and TV shows from raw data, without external sources like plot synopses, scripts, video descriptions or knowledge bases. We treat dialog as a noisy source to be converted into text description via dialog summarization, much like recent methods treat video. The input of each modality is encoded by transformers independently, then we fuse all modalities using soft temporal attention for localization over long inputs. The code allows the reproduction of the results of our <a href="/pub/#C119">ICCV 2021 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="I"></a>
					I
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="ahmet-iscen"></a>
					Iscen, Ahmet
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="gcc"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-gcc-ahmet-iscen" id="tog-gcc">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph convolutional cleaning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, C. Schmid, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/google-research/noisy-fewshot-learning" title="Code repository">GCC</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C114" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/google-research/noisy-fewshot-learning/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/google-research/noisy-fewshot-learning/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/google-research/noisy-fewshot-learning/stargazers" title="Stars @ Github">23</a>
						</div>

					</div>
					<div class="collapse" id="col-gcc-ahmet-iscen">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/tkipf/gcn">GCN</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> <span class="bull"></span>
							<a href="https://github.com/google-research/noisy-fewshot-learning/blob/main/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C114">ECCV 2020</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/gcc.png"><img src="../data/code/thumb/wide/code/gcc.png" alt="gcc thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We learn a classifier from few clean and many noisy labels. The structure of clean and noisy data is modeled by a graph per class and <a href="https://github.com/tkipf/gcn">graph convolutional networks</a> are used to predict class relevance of noisy examples. This cleaning method is evaluated on an extended version of a <i>few-shot learning</i> problem, where the few clean examples of novel classes are supplemented with additional noisy data. The code allows the reproduction of the results of our <a href="/pub/#C114">ECCV 2020 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="dlp"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-dlp-ahmet-iscen" id="tog-dlp">
							<i class="left-60 tog far fa-chevron-down"></i>
							Deep label propagation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen
								(<span class="ref">advised by</span> G. Tolias, O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/ahmetius/LP-DeepSSL" title="Code repository">DLP</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C112" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/LP-DeepSSL/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/LP-DeepSSL/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/ahmetius/LP-DeepSSL/stargazers" title="Stars @ Github">112</a>
						</div>

					</div>
					<div class="collapse" id="col-dlp-ahmet-iscen">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/CuriousAI/mean-teacher/tree/master/pytorch">Mean Teacher</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> <span class="bull"></span>
							<a href="https://github.com/ahmetius/LP-DeepSSL/blob/master/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C112">CVPR 2019</a> <span class="bull"></span>
							2018-2019
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/dlp.svg"><img src="../data/code/thumb/wide/code/dlp.svg" alt="dlp thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									DLP is a modern deep learning approach and an inductive version of classic label propagation for semi-supervised learning based on manifold similarity. The code allows the reproduction of the results of <a href="/pub/#C112">CVPR 2019</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="mom"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-mom-ahmet-iscen" id="tog-mom">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mining on manifolds
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gtolias/mom" title="Code repository">MoM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C108" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gtolias/mom/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gtolias/mom/stargazers" title="Stars @ Github">34</a>
						</div>

					</div>
					<div class="collapse" id="col-mom-ahmet-iscen">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a>, <a href="https://github.com/vlfeat/matconvnet">MatConvNet</a> <span class="bull"></span>
							based on <a href="https://github.com/vadimkantorov/metriclearningbench">MLbench</a>, <a href="/code/#dlp">DLP</a> <span class="bull"></span>
							published in <a href="/pub/#C108">CVPR 2018</a> <span class="bull"></span>
							2017-2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png"><img src="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png" alt="mom thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									MoM is one of the very few <i>self-supervised</i> metric learning methods. Building on findings of manifold similarity, it learns a representation space where Euclidean neighbors are determined according to manifold neighbors in the original feature space. It is applied to fine-grained classification as well as particular object retrieval.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="revop"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-revop-ahmet-iscen" id="tog-revop">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting Oxford and Paris
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								F. Radenovic, A. Iscen
								(<span class="ref">advised by</span> G. Tolias, O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/filipradenovic/revisitop" title="Code repository">RevOP</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C107" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://cove.thecvf.com/datasets/652" title="Data repository">
								<i class="fal fa-cloud"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/filipradenovic/revisitop/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/filipradenovic/revisitop/stargazers" title="Stars @ Github">240</a>
						</div>

					</div>
					<div class="collapse" id="col-revop-ahmet-iscen">
						<div class="pub-ref">
							Matlab, Python <span class="bull"></span>
							based on <a href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford5k</a>, <a href="https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/">Paris6k</a> <span class="bull"></span>
							published in <a href="/pub/#C107">CVPR 2018</a> <span class="bull"></span>
							2017
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/revop.png"><img src="../data/code/thumb/wide/data/revop.png" alt="revop thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									RevOP is an image retrieval benchmark. It is the result of revisiting the two most popular image retrieval datasets, <a href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford5k</a> and <a href="https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/">Paris6k</a>. We provide new annotation for both datasets with an extra attention to the reliability of the ground truth. All co-authors have independently annotated the entire dataset; the final annotation is the result of merging all individual contributions with an automated voting process. We introduce 15 new, more difficult queries per dataset and update the evaluation protocol by introducing three new settings of varying difficulty. We also create a new set of one million challenging distractors. The package includes Matlab and Python code to download and process the data and evaluate results on the new benchmark.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="god"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-god-ahmet-iscen" id="tog-god">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph-based object discovery
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								O. Siméoni, A. Iscen
								(<span class="ref">advised by</span> G. Tolias, O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">GOD</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C105" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J28" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-god-ahmet-iscen">
						<div class="pub-ref">
							<a href="https://github.com/vlfeat/matconvnet">MatConvNet</a> <span class="bull"></span>
							based on <a href="/code/#diffusion">Diffusion</a>, <a href="/code/#agm">AGM</a> <span class="bull"></span>
							published in <a href="/pub/#C105">WACV 2018</a>, <a href="/pub/#J28">MVA 2019</a> <span class="bull"></span>
							2017-2018
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/god.svg"><img src="../data/code/thumb/wide/code/god.svg" alt="god thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									GOD captures discriminative patterns from regional CNN activations of an entire dataset, suppressing background clutter. A saliency measure is defined, based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. Salient regions are then detected using an extended version of <a href="/code/#agm">expanding Gaussian mixture</a>. <span class="alrt">The code is not public yet.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="instre2"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-instre2-ahmet-iscen" id="tog-instre2">
							<i class="left-60 tog far fa-chevron-down"></i>
							Instance retrieval benchmark 2
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen
								(<span class="ref">advised by</span> G. Tolias, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="ftp://ftp.irisa.fr/local/texmex/corpus/instre/" title="FTP download">INSTRE2</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C103" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-instre2-ahmet-iscen">
						<div class="pub-ref">
							based on <a href="https://doi.org/10.1145/2700292">INSTRE</a> <span class="bull"></span>
							published in <a href="/pub/#C103">CVPR 2017</a> <span class="bull"></span>
							2016
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/instre2.png"><img src="../data/code/thumb/wide/data/instre2.png" alt="instre2 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a new version of the <a href="https://doi.org/10.1145/2700292">INSTRE benchmark</a> for instance-level object retrieval and recognition. It has been developed as part of our work on <a href="/pub/#C102">diffusion</a>. In particular, we are re-hosting the dataset at <a href="ftp://ftp.irisa.fr/local/texmex/corpus/instre/">Inria</a> because the original version is unavailable, we introduce a new evaluation protocol that is in line with other well known datasets and we provide a rich set of baselines to facilitate comparisons.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="diffusion"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-diffusion-ahmet-iscen" id="tog-diffusion">
							<i class="left-60 tog far fa-chevron-down"></i>
							Diffusion for image retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, T. Furon, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/ahmetius/diffusion-retrieval" title="Code repository">Diffusion</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C103" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/diffusion-retrieval#readme" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/diffusion-retrieval/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/ahmetius/diffusion-retrieval/stargazers" title="Stars @ Github">87</a>
						</div>

					</div>
					<div class="collapse" id="col-diffusion-ahmet-iscen">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							based on <a href="http://yael.gforge.inria.fr/">Yael</a> <span class="bull"></span>
							<a href="https://github.com/ahmetius/diffusion-retrieval#readme">GNU GPL3+</a> license <span class="bull"></span>
							published in <a href="/pub/#C103">CVPR 2017</a> <span class="bull"></span>
							2016-2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png"><img src="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png" alt="diffusion thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Diffusion is a <a href="/pub/#C103">manifold search</a> method that uses a random walk on the nearest neighbor graph of a dataset. It has been extended to a <a href="/pub/#C106">spectral approach</a> and a <a href="/pub/#C109">hybrid variant</a> of the two for image retrieval. The code and data allows the reproduction of the results of our <a href="/pub/#C103">CVPR 2017 paper</a>. In particular, we provide the descriptors used and the necessary ground-truth files for mAP evaluation. We also make available the approximate $k$-NN graph computed off-line for large-scale datasets.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="J"></a>
					J
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="tomas-jenicek"></a>
					Jenicek, Tomas
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="asmk-py"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-asmk-py-tomas-jenicek" id="tog-asmk-py">
							<i class="left-60 tog far fa-chevron-down"></i>
							Aggregated selective match kernel (Python version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								T. Jenicek, G. Tolias
								(<span class="ref">advised by</span> O. Chum)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/jenicek/asmk" title="Code repository">ASMK-py</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C93" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/jenicek/asmk/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/jenicek/asmk/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/jenicek/asmk/stargazers" title="Stars @ Github">67</a>
						</div>

					</div>
					<div class="collapse" id="col-asmk-py-tomas-jenicek">
						<div class="pub-ref">
							Python <span class="bull"></span>
							based on <a href="/code/#asmk">ASMK</a>, <a href="https://github.com/filipradenovic/cnnimageretrieval-pytorch">CIR-torch</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> <span class="bull"></span>
							<a href="https://github.com/jenicek/asmk/blob/master/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C93">ICCV 2013</a>, <a href="https://arxiv.org/abs/2007.13172">HOW</a> <span class="bull"></span>
							2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/asmk-py.svg"><img src="../data/code/thumb/wide/code/asmk-py.svg" alt="asmk-py thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a Python implementation of <a href="/code/#asmk">ASMK</a>. There are minor differences compared to the original <a href="/pub/#C93">ASMK method (ICCV 2013)</a> and <a href="/code/#asmk">Matlab implementation</a>, which are described in the <a href="https://arxiv.org/abs/2007.13172">HOW paper (ECCV 2020)</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="K"></a>
					K
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="ioannis-kakogeorgiou"></a>
					Kakogeorgiou, Ioannis
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="simpool"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-simpool-ioannis-kakogeorgiou" id="tog-simpool">
							<i class="left-60 tog far fa-chevron-down"></i>
							Simple attention-based pooling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								B. Psomas, I. Kakogeorgiou
								(<span class="ref">advised by</span> K. Karantzalos, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/billpsomas/simpool" title="Code repository">SimPool</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C129" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/simpool/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/simpool/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/billpsomas/simpool/stargazers" title="Stars @ Github">90</a>
						</div>

					</div>
					<div class="collapse" id="col-simpool-ioannis-kakogeorgiou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/gkakogeorgiou/attmask">AttMask</a>, <a href="https://github.com/facebookresearch/dino">DINO</a>, <a href="https://github.com/facebookresearch/ConvNeXt">ConvNeXt</a>, <a href="https://github.com/facebookresearch/detr">DETR</a>, <a href="https://github.com/huggingface/pytorch-image-models">timm</a>, <a href="https://github.com/billpsomas/Metrix_ICLR22">Metrix</a> <span class="bull"></span>
							<a href="https://github.com/billpsomas/simpool/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C129">ICCV 2023</a> <span class="bull"></span>
							2023
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/simpool.svg"><img src="../data/code/thumb/wide/code/simpool.svg" alt="simpool thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									SimPool is a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. Whether supervised or self-supervised, it improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. SimPool is the first method to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. The code allows the reproduction of the results of our <a href="/pub/#C129">ICCV 2023 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="attmask"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-attmask-ioannis-kakogeorgiou" id="tog-attmask">
							<i class="left-60 tog far fa-chevron-down"></i>
							Attention-guided masked image modeling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								I. Kakogeorgiou, B. Psomas
								(<span class="ref">advised by</span> S. Gidaris, Y. Avrithis, K. Karantzalos, N. Komodakis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gkakogeorgiou/attmask" title="Code repository">AttMask</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C125" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gkakogeorgiou/attmask/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gkakogeorgiou/attmask/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gkakogeorgiou/attmask/stargazers" title="Stars @ Github">56</a>
						</div>

					</div>
					<div class="collapse" id="col-attmask-ioannis-kakogeorgiou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/bytedance/ibot">iBOT</a>, <a href="https://github.com/facebookresearch/dino">DINO</a>, <a href="https://github.com/microsoft/unilm/tree/master/beit">BEiT</a>, <a href="https://github.com/MadryLab/backgrounds_challenge">ImageNet-9</a> <span class="bull"></span>
							<a href="https://github.com/gkakogeorgiou/attmask/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C125">ECCV 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg"><img src="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg" alt="attmask thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In the context of self-supervised pretraining of vision transformers, this is a masking strategy that can be used as an alternative to random masking for dense distillation-based masked image modeliing (MIM) as well as plain distillation-based self-supervised learning on classification tokens. In particular, in the distillation-based setting, a teacher transformer encoder generates an attention map, which we use to guide masking for the student. The code allows the reproduction of the results of our <a href="/pub/#C125">ECCV 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="yannis-kalantidis"></a>
					Kalantidis, Yannis
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="lopq-prod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-lopq-prod-yannis-kalantidis" id="tog-lopq-prod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Locally optimized product quantization (Production version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								C. Melina, Y. Kalantidis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/yahoo/lopq" title="Code repository">LOPQ-prod</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C95" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/yahoo/lopq/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/yahoo/lopq/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/yahoo/lopq/stargazers" title="Stars @ Github">560</a>
						</div>

					</div>
					<div class="collapse" id="col-lopq-prod-yannis-kalantidis">
						<div class="pub-ref">
							Python, Spark <span class="bull"></span>
							based on <a href="/code/#lopq">LOPQ</a> <span class="bull"></span>
							<a href="https://github.com/yahoo/lopq/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C95">CVPR 2014</a> <span class="bull"></span>
							2016-2017
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/lopq-prod.gif"><img src="../data/code/thumb/wide/code/lopq-prod.gif" alt="lopq-prod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a Python/Spark implementation of <a href="/code/#lopq">LOPQ</a>. On top of CNN features, it has been used to power <a href="https://yahooresearch.tumblr.com/post/158115871236/introducing-similarity-search-at-flick">image similarity search</a> on the entire <a href="https://www.flickr.com/">Flickr</a> collection.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="lopq"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-lopq-yannis-kalantidis" id="tog-lopq">
							<i class="left-60 tog far fa-chevron-down"></i>
							Locally optimized product quantization (Demo version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/research/lopq/" title="Home + download">LOPQ</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C95" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-lopq-yannis-kalantidis">
						<div class="pub-ref">
							published in <a href="/pub/#C95">CVPR 2014</a> <span class="bull"></span>
							2013-2014
							<div class="p">
								<span class='struct'>Off-line learning:</span>
								Matlab
								<span class="bull"></span> based on <a href="http://yael.gforge.inria.fr/">Yael</a>
								<br>
								<span class='struct'>On-line search:</span>
								Python, C++
								<span class="bull"></span> based on <a href="/code/#ivl">ivl</a>
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C095.cvpr14.lopq.png"><img src="../data/pub/thumb/wide/conf/C095.cvpr14.lopq.png" alt="lopq thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									LOPQ a method for approximate nearest neighbor search that has remained state of the art for several years at a scale of one billion vectors. Leveraging the very same data structure that is used to provide non-exhaustive search, that is, inverted lists or a multi-index, the idea is to locally optimize an individual product quantizer per cell and use it to encode residuals. Local optimization is over rotation and space decomposition. This code is for demonstration only. Pre-computing projections for all queries is only done to facilitate parameter tuning and is suboptimal.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="symcity"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-symcity-yannis-kalantidis" id="tog-symcity">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature Selection by Symmetry
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SymCity</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C90" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/symcity/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-symcity-yannis-kalantidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#hpm">HPM</a>, <a href="/code/#ivl">ivl</a> <span class="bull"></span>
							published in <a href="/pub/#C90">ACM-MM 2012</a> <span class="bull"></span>
							2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png"><img src="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png" alt="symcity thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									To reduce the space required for the index in large scale search, several methods focus on feature selection based on multiple views. In practice however, most images are unique, in the sense that they depict a unique view of an object or scene in the dataset and there is nothing to compare to. SymCity selects features in such unique images by <i>self-similarity</i>. In effect, we detect repeating patterns or local symmetries and select the participating features. The method itself is a variant of <a href="/code/#hpm">HPM</a>, called <i>Hough pyramid self-matching</i> (HPSM) and maintains the same retrieval performance using only 20% of the required memory. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="agm-prod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-agm-prod-yannis-kalantidis" id="tog-agm-prod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Approximate Gaussian mixture (Production version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">AGM-prod</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C91" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/agm/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-agm-prod-yannis-kalantidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="http://www.cs.ubc.ca/research/flann/">FLANN</a> <span class="bull"></span>
							published in <a href="/pub/#C91">ECCV 2012</a> <span class="bull"></span>
							2011-2012
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/agm-prod.svg"><img src="../data/code/thumb/wide/code/agm-prod.svg" alt="agm-prod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is the production version of <a href="/code/#agm">AGM</a>, allowing the reproduction of the results of our <a href="/pub/#C91">ECCV 2012 paper</a>. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="logos27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-logos27-yannis-kalantidis" id="tog-logos27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Flickr Logos 27
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, L.G. Pueyo, M. Trevisiol
								(<span class="ref">advised by</span> R. van Zwol, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/flickr_logos/" title="Home + download">Logos27</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C87" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-logos27-yannis-kalantidis">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/groups/identitydesign/">Flickr Identity + Logo Design</a> <span class="bull"></span>
							published in <a href="/pub/#C87">ICMR 2011</a> <span class="bull"></span>
							2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C087.icmr11.logo.png"><img src="../data/pub/thumb/wide/conf/C087.icmr11.logo.png" alt="logos27 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is an annotated logo dataset downloaded from <a href="https://flickr.com/">Flickr</a> group <a href="https://www.flickr.com/groups/identitydesign/">Identity + Logo Design</a> and contains more than 4000 logo classes/brands in total. It consists of a training, a distractor and a query set, containing respectively 810 images with bounding boxes labeled into 27 classes, 4207 logo images/classes depicting clean logos and 270 images, half of which are annotated into 27 training classes and the other half do not depict logos.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ec1m"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ec1m-yannis-kalantidis" id="tog-ec1m">
							<i class="left-60 tog far fa-chevron-down"></i>
							European Cities 1M
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/ec1m/" title="Home + download">EC1M</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ec1m-yannis-kalantidis">
						<div class="pub-ref">
							based on <a href="https://flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a> <span class="bull"></span>
							2010
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/ec1m.png"><img src="../data/code/thumb/wide/data/ec1m.png" alt="ec1m thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									EC1M Consists of 909k geo-tagged images from 22 European cities, crawled from <a href="https://flickr.com/">Flickr</a> using geographic queries covering a window of each city center. A subset of 1,081 images from Barcelona is annotated into 35 groups depicting the same scene; 17 of the groups are landmark scenes and 18 are non-landmark. Annotation is based respectively on tags and visual search / manual clean-up. In total, 157 of those images are defined as queries (up to 5 per group). Images of the remaining 21 cities are used as distractors. Most depict urban scenery like the ground-truth, making a challenging distractor dataset.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="scene-maps"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-scene-maps-yannis-kalantidis" id="tog-scene-maps">
							<i class="left-60 tog far fa-chevron-down"></i>
							Scene maps
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, E. Spyrou, G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Scene-maps</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/scene_maps/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-scene-maps-yannis-kalantidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="http://sourceforge.net/projects/lpsolve/">LPSolve</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2010-2011
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/scene-maps.png"><img src="../data/code/thumb/wide/code/scene-maps.png" alt="scene-maps thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									<i>Scene maps</i> refers to a representation of image collections used for <a href="/pub/#C86">large scale image search and mining</a>, and applied to <a href="/pub/#J19">location and landmark recognition</a>. Starting from a geo-tagged dataset, we first group images geographically and then visually, where each visual cluster is assumed to depict different views of the the same scene. We align all views to one reference image and construct a 2D <i>scene map</i> by preserving details from all images while discarding repeating visual features. A scene map thus collectively represents a scene as seen from different viewpoints. The indexing, retrieval and spatial matching scheme then operates directly on scene maps. All clustering operations are based on <i>kernel vector quantization</i> (KVQ). <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ec50k"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ec50k-yannis-kalantidis" id="tog-ec50k">
							<i class="left-60 tog far fa-chevron-down"></i>
							European Cities 50k
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/ec50k/" title="Home + download">EC50k</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C85" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ec50k-yannis-kalantidis">
						<div class="pub-ref">
							based on <a href="https://flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C85">ACM-MM 2010</a> <span class="bull"></span>
							2010
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/ec50k.jpg"><img src="../data/code/thumb/wide/data/ec50k.jpg" alt="ec50k thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									EC50k consists of 50,767 geo-tagged images from 14 European cities, crawled from <a href="https://flickr.com/">Flickr</a> using geographic queries covering a window of each city center. A subset of 778 images from 9 cities are annotated into 20 groups depicting the same scene. Annotation is based on tags and visual search / manual clean-up. In total, 100 of those images are defined as queries (5 per group). Images of the remaining 5 cities are used as distractors. Most depict urban scenery like the ground-truth, making a challenging distractor dataset.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="fmh-prod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-fmh-prod-yannis-kalantidis" id="tog-fmh-prod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature map hashing/similarity (Production version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">FMH-prod</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C85" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J23" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/feature_map_hashing" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-fmh-prod-yannis-kalantidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="/code/#fmh">FMH</a> <span class="bull"></span>
							published in <a href="/pub/#C85">ACM-MM 2010</a>, <a href="/pub/#J23">CVIU 2014</a> <span class="bull"></span>
							2009-2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg"><img src="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg" alt="fmh-prod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is the production version of <a href="/code/#fmh">FMH</a>. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="viral"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-viral-yannis-kalantidis" id="tog-viral">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias, M. Phinikettos, E. Spyrou, Ph. Mylonas
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://viral.image.ntua.gr/" title="Access online">VIRaL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/viral" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-viral-yannis-kalantidis">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2008-2012
							<div class="p">
								<span class='struct'>Application interface:</span>
								M. Phinikettos <span class="bull"></span>
								PHP, Javascript
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Core search engine:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Explore/Routes:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++, PHP, Javascript
								<span class="bull"></span> based on <a href="/code/#scene-maps">Scene-maps</a>
								<span class="bull"></span> 2011
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/app/viral.png"><img src="../data/code/thumb/wide/app/viral.png" alt="viral thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									VIRaL is a visual search engine available online since 2008. The query is an image, either uploaded, fetched from a given URL, or chosen from the its database. Given this single image, it retrieves visually similar images and estimates its location on the map. It also suggests tags that may be attached to the query image, identifies known landmarks or points of interest, and provides links to relevant Wikipedia articles. Its database contains 2.7M <a href="https://www.flickr.com/">Flickr</a> images from 43 cities in the world. It is able to recognize tens of thousands of landmarks.
								</p>
								<p>
									
								</p>
								<p>
									Additional applications enhance its user experience. <a href="http://viral.image.ntua.gr/?explore">VIRaL Explore</a> enables browsing of the entire VIRaL image collection on the world map. Starting in a given city or at any zoom level on the map, it places icons corresponding to grouped photos, along with landmark names and Wikipedia links, if applicable. Photos are grouped off-line according to whether they depict the same object, building, or scene, and most popular groups are shown on the map, according to zoom level. <a href="http://viral.image.ntua.gr/?routes">VIRaL Routes</a> offers a unique browsing experience of personal photo collections. Collections are processed off-line to identify where they were taken and group them by scene; a route is then constructed on the map, showing icons of visited places.
								</p>
								<p>
									
								</p>
								<p>
									VIRaL targets general public to demonstrate results of our research. It has been disseminated in several technical and wide-audience venues. It is a unique application, and one of the very few non-commercial CBIR engines listed by <a href="http://en.wikipedia.org/wiki/List_of_CBIR_engines">Wikipedia</a> that is really operating online.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="iva"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-iva-yannis-kalantidis" id="tog-iva">
							<i class="left-60 tog far fa-chevron-down"></i>
							IVA visual representation, matching and search infrastructure
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">iva</span>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-iva-yannis-kalantidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="https://opencv.org/">OpenCV</a> <span class="bull"></span>
							2008-2014
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This is a collection of software that has been used internally as infrastructure within the <a href="http://image.ntua.gr/iva/">IVA research team</a> for several other projects, most notably <a href="/code/#scene-maps">Scene-maps</a>, <a href="/code/#fmh">FMH</a>, <a href="/code/#hpm">HPM</a>, <a href="/code/#symcity">SymCity</a>, <a href="/code/#agm">AGM</a>, <a href="/code/#drvq">DRVQ</a> and <a href="/code/#viral">VIRaL</a>. It provides a common interface to frequently used data structures and a number of individual software components to support common tasks. Such tasks include local feature detection and descriptor computation, nearest neighbor search and clustering, aggregated representations like histograms and sparse sets used e.g. for bag-of-words and related models, matching methods including pyramid matching, algorithms like radix sort, set operations like intersection and unique element count, inverted file structures for indexing, as well as dataset organization and evaluation protocols.
								</p>
								<p>
									
								</p>
								<p>
									Most software is using <a href="/code/#ivl">ivl</a>, which has evolved itself to support the needs of the software. In many cases <a href="https://opencv.org/">OpenCV</a> is also required, but otherwise dependencies are kept to a minimum and constrained to individual components. The software includes dozens of individual components and hundreds of source files. Each component is typically accompanied by a sample project in Linux and Windows, demonstrating its use. <span class="alrt">The code is not public.</span>
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="kimon-kontosis"></a>
					Kontosis, Kimon
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ivl"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ivl-kimon-kontosis" id="tog-ivl">
							<i class="left-60 tog far fa-chevron-down"></i>
							ivl
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								K. Kontosis, N. Skalkotos, S. Nathanail
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/ivl" title="Home + download">ivl</a>
							<a class="lnk mr" href="../data/code/pdf/code/ivl.by-example.pdf" title="Article PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/ivl" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl" title="Repository">
								<i class="fab fa-github"></i>
							</a>
							<a class="lnk ext mr2" href="https://sourceforge.net/projects/ivl" title="Download">
								<i class="faa fa-sourceforge"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ivl-kimon-kontosis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							<a href="https://github.com/cpplibivl/ivl/blob/master/LICENSE">GNU LGPL2/3</a> license <span class="bull"></span>
							2007-2013
							<div class="p">
								<span class='struct'>2007:</span> S. Nathanail <br>
								<span class='struct'>2008:</span> N. Skalkotos <br>
								<span class='struct'>2009-2013:</span> K. Kontosis <br>
							</div>
							<div class="p">
								<span class='struct'>ivl-lina:</span>
								N. Skalkotos <span class="bull"></span>
								Linear algebra (<a href="http://netlib.org/lapack/">LAPACK</a>)
								<span class="bull"></span> 2008
								<br>
								<span class='struct'>ivl-cv:</span>
								K. Kontosis <span class="bull"></span>
								Computer vision (<a href="https://opencv.org/">OpenCV</a>)
								<span class="bull"></span> 2009-2010
								<br>
								<span class='struct'>ivl-qt:</span>
								K. Kontosis <span class="bull"></span>
								GUI (<a href="https://qt-project.org/">Qt</a>)
								<span class="bull"></span> 2011-2012
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/ivl.svg"><img src="../data/code/thumb/wide/code/ivl.svg" alt="ivl thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ivl a full-header template C++98 general purpose library with convenient and powerful syntax. It extends C++ syntax towards mathematical notation, while making use of language features like classes, functions, operators, templates and type safety. It allows simple and expressive statements, while taking care of the underlying representation and optimization. Often resembling a new language, it targets abstract, concise, readable, yet efficient code. It supports the principle that <i>the path from theory through rapid prototyping to production quality software should be as short as possible</i>. In fact, the actual code should not differ much from pseudocode.
								</p>
								<p>
									
								</p>
								<p>
									ivl features static and dynamic arrays, ranges, tuples, matrices, images and function objects supporting multiple return arguments, left/right overloading, function pipelining and vectorization, expression templates, automatic lazy evaluation, and dynamic multi-threading. Other features include sub-arrays and other lazy views of one- or multi-dimensional arrays and tuples, STL-compatible and multidimensional iterators, and extended compound operators. It is easy to use, with most syntax being self explanatory. It is fully optimized, with minimal or no runtime overhead, no temporaries or copies, and with most expressions boiling down to a single <i>for</i> loop.
								</p>
								<p>
									
								</p>
								<p>
									ivl <i>core</i> is a header-only library, with no need for separate linking. It is fully template, supporting user-defined types. Separate modules are available that smoothly integrate with <a href="http://netlib.org/lapack/">LAPACK</a>, <a href="https://opencv.org/">OpenCV</a> and <a href="http://qt-project.org/">Qt</a> for linear algebra, computer vision and GUI respectively. In each case, ivl shares its data representation with the underlying external library and combines its convenient syntax with a rich collection of software. Separate linking is needed for the modules used, since external libraries are not template.
								</p>
								<p>
									
								</p>
								<p>
									The library is available as open source under a dual LGPL3.0 and GPL2.0 license at <a href="https://sourceforge.net/projects/ivl/">SourceForge</a> and at its dedicated <a href="http://image.ntua.gr/ivl/">web site</a>, which includes extended examples and documentation. A unique article <a href="http://image.ntua.gr/ivl/files/ivl-by-example.pdf">ivl by example</a> explains in less than eight pages how to build a <i>randomized decision forest classifier</i> from scratch with ivl, including the complete code of just 120 lines. The article and code behave like one entity, as in <a href="http://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.
								</p>
								<p>
									
								</p>
								<p>
									Over the years, ivl has been influenced by several C++ numerical libraries, for instance <a href="http://eigen.tuxfamily.org/">Eigen</a>, or <a href="http://www.boost.org/doc/libs/1_49_0/libs/multi_array/">Boost.Multi-Array</a> and <a href="http://www.boost.org/doc/libs/1_49_0/libs/tuple/">Boost.Tuple</a> for data representation and manipulation. At a more foundational level, it includes its own <i>template metaprogramming</i> library similar to <a href="http://www.boost.org/doc/libs/1_50_0/libs/mpl/doc/">Boost.MPL</a>, heavily used for code optimization. A great motivation has been the Matlab language syntax, and in this sense a related project is <a href="http://arma.sourceforge.net/">Armadillo</a>. Most of this syntax is supported, without the computational overhead and other known issues. In fact, ivl provides a unique integration of all the above functionalities.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="L"></a>
					L
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="michalis-lazarou"></a>
					Lazarou, Michalis
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="am"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-am-michalis-lazarou" id="tog-am">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive Manifold for Imbalanced Transductive Few-Shot Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Lazarou
								(<span class="ref">advised by</span> T. Stathaki, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/MichalisLazarou/AM" title="Code repository">AM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C132" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/AM/blob/main/LICENSE.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-am-michalis-lazarou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/oveilleux/Realistic_Transductive_Few_Shot">α-ΤΙΜ</a>, <a href="https://github.com/mboudiaf/TIM">ΤΙΜ</a>, <a href="https://github.com/MichalisLazarou/iLPC">iLPC</a>, <a href="https://github.com/nupurkmr9/S2M2_fewshot">S2M2_fewshot</a> <span class="bull"></span>
							<a href="https://github.com/MichalisLazarou/AM/blob/main/LICENSE.txt">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C132">WACV 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C132.wacv24.few-imbalanced.png"><img src="../data/pub/thumb/wide/conf/C132.wacv24.few-imbalanced.png" alt="am thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Adaptive Manifold is an algorithm for transductive few-shot learning on class-imbalanced data. It exploits the underlying manifold of the labeled examples and unlabeled queries by using manifold similarity to predict the class probability distribution of every query. It is parameterized by one centroid per class and a set of manifold parameters that determine the manifold. All parameters are optimized by minimizing a loss function that can be tuned towards class-balanced or imbalanced distributions. The code allows the reproduction of the results of our <a href="/pub/#C132">WACV 2024 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="a2lp"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-a2lp-michalis-lazarou" id="tog-a2lp">
							<i class="left-60 tog far fa-chevron-down"></i>
							Adaptive anchor label propagation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Lazarou
								(<span class="ref">advised by</span> T. Stathaki, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/MichalisLazarou/A2LP" title="Code repository">A2LP</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C128" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/A2LP/blob/main/LICENSE.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-a2lp-michalis-lazarou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/MichalisLazarou/iLPC">iLPC</a>, <a href="https://github.com/Yikai-Wang/ICI-FSL">LR+ICI</a>, <a href="https://github.com/nupurkmr9/S2M2_fewshot">S2M2_fewshot</a> <span class="bull"></span>
							<a href="https://github.com/MichalisLazarou/A2LP/blob/main/LICENSE.txt">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C128">ICIP 2023</a> <span class="bull"></span>
							2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C128.icip23.few-a2lp.png"><img src="../data/pub/thumb/wide/conf/C128.icip23.few-a2lp.png" alt="a2lp thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In the context of transductive inference for few-shot learning, label propagation infers pseudo-labels for unlabeled data by using a graph that exploits the manifold structure of the data. Adaptive anchor label propagation (A2LP) is an algorithm that adapts the feature embeddings of the labeled data by minimizing a differentiable loss function, optimizing their positions in the manifold in the process. The code allows the reproduction of the results of our <a href="/pub/#C128">ICIP 2023 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ilpc"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ilpc-michalis-lazarou" id="tog-ilpc">
							<i class="left-60 tog far fa-chevron-down"></i>
							Iterative label propagation and cleaning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Lazarou
								(<span class="ref">advised by</span> T. Stathaki, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/MichalisLazarou/iLPC" title="Code repository">iLPC</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C118" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/iLPC/blob/main/LICENSE.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/iLPC/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/MichalisLazarou/iLPC/stargazers" title="Stars @ Github">19</a>
						</div>

					</div>
					<div class="collapse" id="col-ilpc-michalis-lazarou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/nupurkmr9/S2M2_fewshot">S2M2</a>, <a href="https://github.com/yhu01/PT-MAP">PT-MAP</a>, <a href="https://github.com/Yikai-Wang/ICI-FSL">LR+ICI</a>, <a href="https://github.com/wyharveychen/CloserLookFewShot">CloserLook</a>, <a href="https://github.com/seongmin-kye/MCT">MCT</a> <span class="bull"></span>
							<a href="https://github.com/MichalisLazarou/iLPC/blob/main/LICENSE.txt">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C118">ICCV 2021</a> <span class="bull"></span>
							2021
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C118.iccv21.few-ss.svg"><img src="../data/pub/thumb/wide/conf/C118.iccv21.few-ss.svg" alt="ilpc thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is an algorithm for transductive and semi-supervised few-shot learning. It leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. The code allows the reproduction of the results of our <a href="/pub/#C118">ICCV 2021 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="tfh"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-tfh-michalis-lazarou" id="tog-tfh">
							<i class="left-60 tog far fa-chevron-down"></i>
							Tensor feature hallucination
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Lazarou
								(<span class="ref">advised by</span> T. Stathaki, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/MichalisLazarou/TFH_fewshot" title="Code repository">TFH</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C121" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/TFH_fewshot/blob/main/LICENSE.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/MichalisLazarou/TFH_fewshot/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/MichalisLazarou/TFH_fewshot/stargazers" title="Stars @ Github">5</a>
						</div>

					</div>
					<div class="collapse" id="col-tfh-michalis-lazarou">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/WangYueFt/rfs">RFS</a>, <a href="https://github.com/tankche1/IDeMe-Net">IDeMe-Net</a>, <a href="https://github.com/tankche1/Semantic-Feature-Augmentation-in-Few-shot-Learning">Dual TriNet</a>, <a href="https://github.com/nupurkmr9/S2M2_fewshot">S2M2</a>, <a href="https://github.com/icoz69/DeepEMD">DeepEMD</a> <span class="bull"></span>
							<a href="https://github.com/MichalisLazarou/TFH_fewshot/blob/main/LICENSE.txt">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C121">WACV 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C121.wacv22.few-gen.png"><img src="../data/pub/thumb/wide/conf/C121.wacv22.few-gen.png" alt="tfh thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a simple synthetic data generation method few-shot learning. It involves a simple loss function for training a feature generator and it learns to generate tensor features instead of vector features. The code allows the reproduction of the results of our <a href="/pub/#C121">WACV 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="yann-raphael-lifchitz"></a>
					Lifchitz, Yann-Raphaël
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="fsfsl"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-fsfsl-yann-raphael-lifchitz" id="tog-fsfsl">
							<i class="left-60 tog far fa-chevron-down"></i>
							Few-shot few-shot learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Lifchitz
								(<span class="ref">advised by</span> S. Picard, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">FSFSL</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C115" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-fsfsl-yann-raphael-lifchitz">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							published in <a href="/pub/#C115">ICPR 2020</a> <span class="bull"></span>
							2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C115.icpr20.few-att.png"><img src="../data/pub/thumb/wide/conf/C115.icpr20.few-att.png" alt="fsfsl thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We depart from the standard setting of <i>few-shot learning</i> in that the representation is obtained from a classifier pre-trained on a large-scale dataset of a different domain, while the base class data are limited to few examples per class and their role is to adapt the representation to the domain at hand rather than learn from scratch. In doing so, we obtain from the pre-trained classifier a spatial attention map that allows focusing on objects and suppressing background clutter. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="dci"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-dci-yann-raphael-lifchitz" id="tog-dci">
							<i class="left-60 tog far fa-chevron-down"></i>
							Dense classification and implanting
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Lifchitz
								(<span class="ref">advised by</span> A. Bursuc, S. Picard, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">DCI</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C111" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-dci-yann-raphael-lifchitz">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/gidariss/FewShotWithoutForgetting">FSwF</a> <span class="bull"></span>
							published in <a href="/pub/#C111">CVPR 2019</a> <span class="bull"></span>
							2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C111.cvpr19.few.svg"><img src="../data/pub/thumb/wide/conf/C111.cvpr19.few.svg" alt="dci thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									<i>Dense classification</i> over feature maps studies for the first time local activations in the domain of few-shot learning. <i>Implanting</i>, that is, attaching new neurons to a previously trained network to learn new, task-specific features, achieves for the first time fine-tuning of the entire network to convergence without overfitting on novel classes. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="M"></a>
					M
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="clayton-melina"></a>
					Melina, Clayton
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="lopq-prod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-lopq-prod-clayton-melina" id="tog-lopq-prod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Locally optimized product quantization (Production version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								C. Melina, Y. Kalantidis
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/yahoo/lopq" title="Code repository">LOPQ-prod</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C95" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/yahoo/lopq/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/yahoo/lopq/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/yahoo/lopq/stargazers" title="Stars @ Github">560</a>
						</div>

					</div>
					<div class="collapse" id="col-lopq-prod-clayton-melina">
						<div class="pub-ref">
							Python, Spark <span class="bull"></span>
							based on <a href="/code/#lopq">LOPQ</a> <span class="bull"></span>
							<a href="https://github.com/yahoo/lopq/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C95">CVPR 2014</a> <span class="bull"></span>
							2016-2017
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/lopq-prod.gif"><img src="../data/code/thumb/wide/code/lopq-prod.gif" alt="lopq-prod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a Python/Spark implementation of <a href="/code/#lopq">LOPQ</a>. On top of CNN features, it has been used to power <a href="https://yahooresearch.tumblr.com/post/158115871236/introducing-similarity-search-at-flick">image similarity search</a> on the entire <a href="https://www.flickr.com/">Flickr</a> collection.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="phivos-mylonas"></a>
					Mylonas, Phivos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="viral"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-viral-phivos-mylonas" id="tog-viral">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias, M. Phinikettos, E. Spyrou, Ph. Mylonas
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://viral.image.ntua.gr/" title="Access online">VIRaL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/viral" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-viral-phivos-mylonas">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2008-2012
							<div class="p">
								<span class='struct'>Application interface:</span>
								M. Phinikettos <span class="bull"></span>
								PHP, Javascript
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Core search engine:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Explore/Routes:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++, PHP, Javascript
								<span class="bull"></span> based on <a href="/code/#scene-maps">Scene-maps</a>
								<span class="bull"></span> 2011
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/app/viral.png"><img src="../data/code/thumb/wide/app/viral.png" alt="viral thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									VIRaL is a visual search engine available online since 2008. The query is an image, either uploaded, fetched from a given URL, or chosen from the its database. Given this single image, it retrieves visually similar images and estimates its location on the map. It also suggests tags that may be attached to the query image, identifies known landmarks or points of interest, and provides links to relevant Wikipedia articles. Its database contains 2.7M <a href="https://www.flickr.com/">Flickr</a> images from 43 cities in the world. It is able to recognize tens of thousands of landmarks.
								</p>
								<p>
									
								</p>
								<p>
									Additional applications enhance its user experience. <a href="http://viral.image.ntua.gr/?explore">VIRaL Explore</a> enables browsing of the entire VIRaL image collection on the world map. Starting in a given city or at any zoom level on the map, it places icons corresponding to grouped photos, along with landmark names and Wikipedia links, if applicable. Photos are grouped off-line according to whether they depict the same object, building, or scene, and most popular groups are shown on the map, according to zoom level. <a href="http://viral.image.ntua.gr/?routes">VIRaL Routes</a> offers a unique browsing experience of personal photo collections. Collections are processed off-line to identify where they were taken and group them by scene; a route is then constructed on the map, showing icons of visited places.
								</p>
								<p>
									
								</p>
								<p>
									VIRaL targets general public to demonstrate results of our research. It has been disseminated in several technical and wide-audience venues. It is a unique application, and one of the very few non-commercial CBIR engines listed by <a href="http://en.wikipedia.org/wiki/List_of_CBIR_engines">Wikipedia</a> that is really operating online.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="N"></a>
					N
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="spyros-nathanail"></a>
					Nathanail, Spyros
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ivl"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ivl-spyros-nathanail" id="tog-ivl">
							<i class="left-60 tog far fa-chevron-down"></i>
							ivl
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								K. Kontosis, N. Skalkotos, S. Nathanail
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/ivl" title="Home + download">ivl</a>
							<a class="lnk mr" href="../data/code/pdf/code/ivl.by-example.pdf" title="Article PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/ivl" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl" title="Repository">
								<i class="fab fa-github"></i>
							</a>
							<a class="lnk ext mr2" href="https://sourceforge.net/projects/ivl" title="Download">
								<i class="faa fa-sourceforge"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ivl-spyros-nathanail">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							<a href="https://github.com/cpplibivl/ivl/blob/master/LICENSE">GNU LGPL2/3</a> license <span class="bull"></span>
							2007-2013
							<div class="p">
								<span class='struct'>2007:</span> S. Nathanail <br>
								<span class='struct'>2008:</span> N. Skalkotos <br>
								<span class='struct'>2009-2013:</span> K. Kontosis <br>
							</div>
							<div class="p">
								<span class='struct'>ivl-lina:</span>
								N. Skalkotos <span class="bull"></span>
								Linear algebra (<a href="http://netlib.org/lapack/">LAPACK</a>)
								<span class="bull"></span> 2008
								<br>
								<span class='struct'>ivl-cv:</span>
								K. Kontosis <span class="bull"></span>
								Computer vision (<a href="https://opencv.org/">OpenCV</a>)
								<span class="bull"></span> 2009-2010
								<br>
								<span class='struct'>ivl-qt:</span>
								K. Kontosis <span class="bull"></span>
								GUI (<a href="https://qt-project.org/">Qt</a>)
								<span class="bull"></span> 2011-2012
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/ivl.svg"><img src="../data/code/thumb/wide/code/ivl.svg" alt="ivl thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ivl a full-header template C++98 general purpose library with convenient and powerful syntax. It extends C++ syntax towards mathematical notation, while making use of language features like classes, functions, operators, templates and type safety. It allows simple and expressive statements, while taking care of the underlying representation and optimization. Often resembling a new language, it targets abstract, concise, readable, yet efficient code. It supports the principle that <i>the path from theory through rapid prototyping to production quality software should be as short as possible</i>. In fact, the actual code should not differ much from pseudocode.
								</p>
								<p>
									
								</p>
								<p>
									ivl features static and dynamic arrays, ranges, tuples, matrices, images and function objects supporting multiple return arguments, left/right overloading, function pipelining and vectorization, expression templates, automatic lazy evaluation, and dynamic multi-threading. Other features include sub-arrays and other lazy views of one- or multi-dimensional arrays and tuples, STL-compatible and multidimensional iterators, and extended compound operators. It is easy to use, with most syntax being self explanatory. It is fully optimized, with minimal or no runtime overhead, no temporaries or copies, and with most expressions boiling down to a single <i>for</i> loop.
								</p>
								<p>
									
								</p>
								<p>
									ivl <i>core</i> is a header-only library, with no need for separate linking. It is fully template, supporting user-defined types. Separate modules are available that smoothly integrate with <a href="http://netlib.org/lapack/">LAPACK</a>, <a href="https://opencv.org/">OpenCV</a> and <a href="http://qt-project.org/">Qt</a> for linear algebra, computer vision and GUI respectively. In each case, ivl shares its data representation with the underlying external library and combines its convenient syntax with a rich collection of software. Separate linking is needed for the modules used, since external libraries are not template.
								</p>
								<p>
									
								</p>
								<p>
									The library is available as open source under a dual LGPL3.0 and GPL2.0 license at <a href="https://sourceforge.net/projects/ivl/">SourceForge</a> and at its dedicated <a href="http://image.ntua.gr/ivl/">web site</a>, which includes extended examples and documentation. A unique article <a href="http://image.ntua.gr/ivl/files/ivl-by-example.pdf">ivl by example</a> explains in less than eight pages how to build a <i>randomized decision forest classifier</i> from scratch with ivl, including the complete code of just 120 lines. The article and code behave like one entity, as in <a href="http://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.
								</p>
								<p>
									
								</p>
								<p>
									Over the years, ivl has been influenced by several C++ numerical libraries, for instance <a href="http://eigen.tuxfamily.org/">Eigen</a>, or <a href="http://www.boost.org/doc/libs/1_49_0/libs/multi_array/">Boost.Multi-Array</a> and <a href="http://www.boost.org/doc/libs/1_49_0/libs/tuple/">Boost.Tuple</a> for data representation and manipulation. At a more foundational level, it includes its own <i>template metaprogramming</i> library similar to <a href="http://www.boost.org/doc/libs/1_50_0/libs/mpl/doc/">Boost.MPL</a>, heavily used for code optimization. A great motivation has been the Matlab language syntax, and in this sense a related project is <a href="http://arma.sourceforge.net/">Armadillo</a>. Most of this syntax is supported, without the computational overhead and other known issues. In fact, ivl provides a unique integration of all the above functionalities.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="timothee-neitthoffer"></a>
					Neitthoffer, Timothée
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="nagp"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-nagp-timothee-neitthoffer" id="tog-nagp">
							<i class="left-60 tog far fa-chevron-down"></i>
							Neural architecture growing, pruning and search
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								T. Neitthoffer
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/shymine/neural-architecture-growing-pruning-and-search" title="Code repository">NAGP</a>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-nagp-timothee-neitthoffer">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/yigitcankaya/Shallow-Deep-Networks">SDN</a>, <a href="https://github.com/mil-ad/snip">SNIP</a> <span class="bull"></span>
							2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/nagp.svg"><img src="../data/code/thumb/wide/code/nagp.svg" alt="nagp thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The goal of neural architecture search is to automatically find the optimal network architecture, that is, the optimal succession and interconnection of layers. This is an intractable combinatorial optimization problem. We define a <i>fully-dense</i> super-network, out of which we select the most important connections by pruning. Still, training a deep super-network is not practical, so we devise a greedy algorithm: We grow the super-network a few layers at a time, training it and pruning its connections at each iteration. The code allows the reproduction of the results of the <a href="/cv/#msc-2020-neitthoffer">2020 MSc thesis</a>.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="P"></a>
					P
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="marios-phinikettos"></a>
					Phinikettos, Marios
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="viral"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-viral-marios-phinikettos" id="tog-viral">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias, M. Phinikettos, E. Spyrou, Ph. Mylonas
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://viral.image.ntua.gr/" title="Access online">VIRaL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/viral" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-viral-marios-phinikettos">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2008-2012
							<div class="p">
								<span class='struct'>Application interface:</span>
								M. Phinikettos <span class="bull"></span>
								PHP, Javascript
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Core search engine:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Explore/Routes:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++, PHP, Javascript
								<span class="bull"></span> based on <a href="/code/#scene-maps">Scene-maps</a>
								<span class="bull"></span> 2011
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/app/viral.png"><img src="../data/code/thumb/wide/app/viral.png" alt="viral thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									VIRaL is a visual search engine available online since 2008. The query is an image, either uploaded, fetched from a given URL, or chosen from the its database. Given this single image, it retrieves visually similar images and estimates its location on the map. It also suggests tags that may be attached to the query image, identifies known landmarks or points of interest, and provides links to relevant Wikipedia articles. Its database contains 2.7M <a href="https://www.flickr.com/">Flickr</a> images from 43 cities in the world. It is able to recognize tens of thousands of landmarks.
								</p>
								<p>
									
								</p>
								<p>
									Additional applications enhance its user experience. <a href="http://viral.image.ntua.gr/?explore">VIRaL Explore</a> enables browsing of the entire VIRaL image collection on the world map. Starting in a given city or at any zoom level on the map, it places icons corresponding to grouped photos, along with landmark names and Wikipedia links, if applicable. Photos are grouped off-line according to whether they depict the same object, building, or scene, and most popular groups are shown on the map, according to zoom level. <a href="http://viral.image.ntua.gr/?routes">VIRaL Routes</a> offers a unique browsing experience of personal photo collections. Collections are processed off-line to identify where they were taken and group them by scene; a route is then constructed on the map, showing icons of visited places.
								</p>
								<p>
									
								</p>
								<p>
									VIRaL targets general public to demonstrate results of our research. It has been disseminated in several technical and wide-audience venues. It is a unique application, and one of the very few non-commercial CBIR engines listed by <a href="http://en.wikipedia.org/wiki/List_of_CBIR_engines">Wikipedia</a> that is really operating online.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="bill-psomas"></a>
					Psomas, Bill
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="simpool"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-simpool-bill-psomas" id="tog-simpool">
							<i class="left-60 tog far fa-chevron-down"></i>
							Simple attention-based pooling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								B. Psomas, I. Kakogeorgiou
								(<span class="ref">advised by</span> K. Karantzalos, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/billpsomas/simpool" title="Code repository">SimPool</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C129" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/simpool/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/simpool/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/billpsomas/simpool/stargazers" title="Stars @ Github">90</a>
						</div>

					</div>
					<div class="collapse" id="col-simpool-bill-psomas">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/gkakogeorgiou/attmask">AttMask</a>, <a href="https://github.com/facebookresearch/dino">DINO</a>, <a href="https://github.com/facebookresearch/ConvNeXt">ConvNeXt</a>, <a href="https://github.com/facebookresearch/detr">DETR</a>, <a href="https://github.com/huggingface/pytorch-image-models">timm</a>, <a href="https://github.com/billpsomas/Metrix_ICLR22">Metrix</a> <span class="bull"></span>
							<a href="https://github.com/billpsomas/simpool/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C129">ICCV 2023</a> <span class="bull"></span>
							2023
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/simpool.svg"><img src="../data/code/thumb/wide/code/simpool.svg" alt="simpool thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									SimPool is a simple attention-based pooling mechanism as a replacement of the default one for both convolutional and transformer encoders. Whether supervised or self-supervised, it improves performance on pre-training and downstream tasks and provides attention maps delineating object boundaries in all cases. SimPool is the first method to obtain attention maps in supervised transformers of at least as good quality as self-supervised, without explicit losses or modifying the architecture. The code allows the reproduction of the results of our <a href="/pub/#C129">ICCV 2023 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="metrix"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-metrix-bill-psomas" id="tog-metrix">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mixup for Deep Metric Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								B. Psomas, S. Venkataramanan
								(<span class="ref">advised by</span> E. Kijak, L. Amsaleg, K. Karantzalos, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/billpsomas/Metrix_ICLR22" title="Code repository">Metrix</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C123" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/Metrix_ICLR22/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/billpsomas/Metrix_ICLR22/stargazers" title="Stars @ Github">3</a>
						</div>

					</div>
					<div class="collapse" id="col-metrix-bill-psomas">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/tjddus9597/Proxy-Anchor-CVPR2020">Proxy Anchor</a>, <a href="https://github.com/KevinMusgrave/pytorch-metric-learning">PyTorch Metric Learning</a>, <a href="https://github.com/billpsomas/Deep_Metric_Learning_Pytorch">DML Benchmark</a> <span class="bull"></span>
							published in <a href="/pub/#C123">ICLR 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/metrix.svg"><img src="../data/code/thumb/wide/code/metrix.svg" alt="metrix thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Metric Mix, or Metrix, is an algorithm for mixup-based interpolation as a data augmentation method for metric learning. It uses a generalized formulation that encompasses existing metric learning loss functions that is modified to accommodate for mixup. Mixing takes place at the input space, intermediate representations as well as the embedding space. It refers to both examples and target labels. The code allows the reproduction of the results of our <a href="/pub/#C123">ICLR 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="attmask"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-attmask-bill-psomas" id="tog-attmask">
							<i class="left-60 tog far fa-chevron-down"></i>
							Attention-guided masked image modeling
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								I. Kakogeorgiou, B. Psomas
								(<span class="ref">advised by</span> S. Gidaris, Y. Avrithis, K. Karantzalos, N. Komodakis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gkakogeorgiou/attmask" title="Code repository">AttMask</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C125" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gkakogeorgiou/attmask/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gkakogeorgiou/attmask/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gkakogeorgiou/attmask/stargazers" title="Stars @ Github">56</a>
						</div>

					</div>
					<div class="collapse" id="col-attmask-bill-psomas">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/bytedance/ibot">iBOT</a>, <a href="https://github.com/facebookresearch/dino">DINO</a>, <a href="https://github.com/microsoft/unilm/tree/master/beit">BEiT</a>, <a href="https://github.com/MadryLab/backgrounds_challenge">ImageNet-9</a> <span class="bull"></span>
							<a href="https://github.com/gkakogeorgiou/attmask/blob/master/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C125">ECCV 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg"><img src="../data/pub/thumb/wide/conf/C125.eccv22.attmask.svg" alt="attmask thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In the context of self-supervised pretraining of vision transformers, this is a masking strategy that can be used as an alternative to random masking for dense distillation-based masked image modeliing (MIM) as well as plain distillation-based self-supervised learning on classification tokens. In particular, in the distillation-based setting, a teacher transformer encoder generates an attention map, which we use to guide masking for the student. The code allows the reproduction of the results of our <a href="/pub/#C125">ECCV 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="lluis-garcia-pueyo"></a>
					Pueyo, Lluis Garcia
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="logos27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-logos27-lluis-garcia-pueyo" id="tog-logos27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Flickr Logos 27
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, L.G. Pueyo, M. Trevisiol
								(<span class="ref">advised by</span> R. van Zwol, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/flickr_logos/" title="Home + download">Logos27</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C87" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-logos27-lluis-garcia-pueyo">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/groups/identitydesign/">Flickr Identity + Logo Design</a> <span class="bull"></span>
							published in <a href="/pub/#C87">ICMR 2011</a> <span class="bull"></span>
							2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C087.icmr11.logo.png"><img src="../data/pub/thumb/wide/conf/C087.icmr11.logo.png" alt="logos27 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is an annotated logo dataset downloaded from <a href="https://flickr.com/">Flickr</a> group <a href="https://www.flickr.com/groups/identitydesign/">Identity + Logo Design</a> and contains more than 4000 logo classes/brands in total. It consists of a training, a distractor and a query set, containing respectively 810 images with bounding boxes labeled into 27 classes, 4207 logo images/classes depicting clean logos and 270 images, half of which are annotated into 27 training classes and the other half do not depict logos.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="R"></a>
					R
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="filip-radenovic"></a>
					Radenovic, Filip
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="revop"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-revop-filip-radenovic" id="tog-revop">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting Oxford and Paris
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								F. Radenovic, A. Iscen
								(<span class="ref">advised by</span> G. Tolias, O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/filipradenovic/revisitop" title="Code repository">RevOP</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C107" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://cove.thecvf.com/datasets/652" title="Data repository">
								<i class="fal fa-cloud"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/filipradenovic/revisitop/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/filipradenovic/revisitop/stargazers" title="Stars @ Github">240</a>
						</div>

					</div>
					<div class="collapse" id="col-revop-filip-radenovic">
						<div class="pub-ref">
							Matlab, Python <span class="bull"></span>
							based on <a href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford5k</a>, <a href="https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/">Paris6k</a> <span class="bull"></span>
							published in <a href="/pub/#C107">CVPR 2018</a> <span class="bull"></span>
							2017
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/revop.png"><img src="../data/code/thumb/wide/data/revop.png" alt="revop thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									RevOP is an image retrieval benchmark. It is the result of revisiting the two most popular image retrieval datasets, <a href="https://www.robots.ox.ac.uk/~vgg/data/oxbuildings/">Oxford5k</a> and <a href="https://www.robots.ox.ac.uk/~vgg/data/parisbuildings/">Paris6k</a>. We provide new annotation for both datasets with an extra attention to the reliability of the ground truth. All co-authors have independently annotated the entire dataset; the final annotation is the result of merging all individual contributions with an automated voting process. We introduce 15 new, more difficult queries per dataset and update the evaluation protocol by introducing three new settings of varying difficulty. We also create a new set of one million challenging distractors. The package includes Matlab and Python code to download and process the data and evaluate results on the new benchmark.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="konstantinos-rapantzikos"></a>
					Rapantzikos, Konstantinos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="sfd"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-sfd-konstantinos-rapantzikos" id="tog-sfd">
							<i class="left-60 tog far fa-chevron-down"></i>
							Spatiotemporal feature detector
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								K. Rapantzikos
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SFD</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C83" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J20" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/spatiotemporal_feature_detection" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-sfd-konstantinos-rapantzikos">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							published in <a href="/pub/#C83">CVPR 2009</a>, <a href="/pub/#J20">CC 2011</a> <span class="bull"></span>
							2008-2009
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/sfd.svg"><img src="../data/code/thumb/wide/code/sfd.svg" alt="sfd thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a local feature detector originally applied to <a href="/pub/#C83">action recognition</a> and then to <a href="/pub/#J20">salient event detection</a> and <a href="/pub/#J21">movie summarization</a>. It uses a multi-scale volumetric representation of the video and involves spatiotemporal operations at the voxel level. Saliency is computed by a global minimization process constrained by pure volumetric constraints, each of them being related to an informative visual aspect, namely spatial proximity, scale and feature similarity (intensity, color, motion). Points are selected as the extrema of the saliency response and prove to balance well between density and informativeness. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="S"></a>
					S
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="georgios-samaras"></a>
					Samaras, Georgios
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="geraf"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-geraf-georgios-samaras" id="tog-geraf">
							<i class="left-60 tog far fa-chevron-down"></i>
							$k$-d Generalized Randomized Forests
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Samaras
								(<span class="ref">advised by</span> I. Emiris, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gsamaras/kd_GeRaF" title="Code repository">GeRaF</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C100" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gsamaras/kd_GeRaF/blob/master/LICENSE.txt" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gsamaras/kd_GeRaF/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gsamaras/kd_GeRaF/stargazers" title="Stars @ Github">10</a>
						</div>

					</div>
					<div class="collapse" id="col-geraf-georgios-samaras">
						<div class="pub-ref">
							C++ header-only <span class="bull"></span>
							<a href="https://github.com/gsamaras/kd_GeRaF/blob/master/LICENSE.txt">BSD</a> license <span class="bull"></span>
							published in <a href="/pub/#C100">CGI 2016</a> <span class="bull"></span>
							2015-2016
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C100.cgi16.geraf.svg"><img src="../data/pub/thumb/wide/conf/C100.cgi16.geraf.svg" alt="geraf thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									$k$-d GeRaF is a data structure and algorithm for approximate nearest neighbor search in high dimensions. It improves randomized forests by introducing new randomization techniques to specify a set of independently constructed trees where search is performed simultaneously, hence increasing accuracy. We omit backtracking, and we optimize distance computations.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="miaojing-shi"></a>
					Shi, Miaojing
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ebd"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ebd-miaojing-shi" id="tog-ebd">
							<i class="left-60 tog far fa-chevron-down"></i>
							Early burst detection (Demo version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Shi
								(<span class="ref">advised by</span> H. Jégou, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">EBD</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C97" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<script>ema2(ema_net(), "lnk ext mr2", "Email request");</script>
								<i class="fal fa-at"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ebd-miaojing-shi">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							based on <a href="/code/#asmk">ASMK</a> <span class="bull"></span>
							published in <a href="/pub/#C97">CVPR 2015</a> <span class="bull"></span>
							2014
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C097.cvpr15.burst.svg"><img src="../data/pub/thumb/wide/conf/C097.cvpr15.burst.svg" alt="ebd thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									EBD is a compact representation for image retrieval. It explicitly detects visual bursts in an image at an early stage, using clustering in the descriptor space. The bursty groups are merged into meta-features, which are used as input to image search systems. It achieves compressing image representations by more than 90% without significant loss in performance. <span class="alrt">This is a demo version, available upon request.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="ronan-sicre"></a>
					Sicre, Ronan
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="parts"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-parts-ronan-sicre" id="tog-parts">
							<i class="left-60 tog far fa-chevron-down"></i>
							Part learning for visual recognition
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								R. Sicre
								(<span class="ref">advised by</span> T. Furon, E. Kijak, F. Jurie, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Parts</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C104" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-parts-ronan-sicre">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							published in <a href="/pub/#C102">CVPR 2017</a>, <a href="/pub/#C104">CEFRL/ICCV 2017</a> <span class="bull"></span>
							2016-2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C102.cvpr17.uparts.svg"><img src="../data/pub/thumb/wide/conf/C102.cvpr17.uparts.svg" alt="parts thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									In image classification, it has been common to learn mid-level discriminative parts, even before deep learning. <a href="/pub/#C104">Discovery of discriminative parts</a> casts this as a quadratic assignment problem, allowing the use of a number of optimization algorithms on top of CNN representations. <a href="/pub/#C102">Unsupervised part learning</a> extends this work by dispensing the need for class labels during part learning. It is applied equally to classification and instance retrieval, bringing significant gains to both. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="oriane-simeoni"></a>
					Siméoni, Oriane
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ssal"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ssal-oriane-simeoni" id="tog-ssal">
							<i class="left-60 tog far fa-chevron-down"></i>
							Semi-supervised active learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								M. Budnik, O. Siméoni
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/osimeoni/RethinkingDeepActiveLearning" title="Code repository">SSAL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C113" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/osimeoni/RethinkingDeepActiveLearning/stargazers" title="Stars @ Github">15</a>
						</div>

					</div>
					<div class="collapse" id="col-ssal-oriane-simeoni">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/facebookresearch/deepcluster">DeepCluster</a>, <a href="/code/#dlp">DLP</a> <span class="bull"></span>
							<a href="https://github.com/osimeoni/RethinkingDeepActiveLearning/blob/main/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C113">ICPR 2020</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C113.icpr20.active.svg"><img src="../data/pub/thumb/wide/conf/C113.icpr20.active.svg" alt="ssal thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a deep active learning framework allowing a systematic evaluation of different acquisition functions with or without methods that make use of the unlabeled data during model training. In particular, this includes (i) unsupervised pre-training, as implemented by <a href="https://github.com/facebookresearch/deepcluster">DeepCluster</a>, and (ii) semi-supervised learning, as implemented by our <a href="/code/#dlp">deep label propagation</a>. The code allows the reproduction of the results of our <a href="/pub/#C113">ICPR 2020 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="dsm"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-dsm-oriane-simeoni" id="tog-dsm">
							<i class="left-60 tog far fa-chevron-down"></i>
							Deep spatial matching
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								O. Siméoni
								(<span class="ref">advised by</span> O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/osimeoni/DSM" title="Code repository">DSM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C110" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/osimeoni/DSM/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-dsm-oriane-simeoni">
						<div class="pub-ref">
							<a href="https://github.com/vlfeat/matconvnet">MatConvNet</a>, C++ <span class="bull"></span>
							based on <a href="https://github.com/filipradenovic/cnnimageretrieval">CIR</a> <span class="bull"></span>
							<a href="https://github.com/osimeoni/DSM/blob/master/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C110">CVPR 2019</a> <span class="bull"></span>
							2018-2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C110.cvpr19.spatial.svg"><img src="../data/pub/thumb/wide/conf/C110.cvpr19.spatial.svg" alt="dsm thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									DSM exploits the sparsity of convolutional activations to detect local features and provide spatial matching for image retrieval. Without modifying the network architecture or re-training, without even local descriptors or vocabularies, deep spatial matching sets a new state of the art in particular object retrieval with a compact representation. The code allows the reproduction of the results of our <a href="/pub/#C110">CVPR 2019 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="god"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-god-oriane-simeoni" id="tog-god">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph-based object discovery
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								O. Siméoni, A. Iscen
								(<span class="ref">advised by</span> G. Tolias, O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">GOD</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C105" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J28" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-god-oriane-simeoni">
						<div class="pub-ref">
							<a href="https://github.com/vlfeat/matconvnet">MatConvNet</a> <span class="bull"></span>
							based on <a href="/code/#diffusion">Diffusion</a>, <a href="/code/#agm">AGM</a> <span class="bull"></span>
							published in <a href="/pub/#C105">WACV 2018</a>, <a href="/pub/#J28">MVA 2019</a> <span class="bull"></span>
							2017-2018
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/god.svg"><img src="../data/code/thumb/wide/code/god.svg" alt="god thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									GOD captures discriminative patterns from regional CNN activations of an entire dataset, suppressing background clutter. A saliency measure is defined, based on a centrality measure of a nearest neighbor graph constructed from regional CNN representations of dataset images. Salient regions are then detected using an extended version of <a href="/code/#agm">expanding Gaussian mixture</a>. <span class="alrt">The code is not public yet.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="nikos-skalkotos"></a>
					Skalkotos, Nikos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ivl"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ivl-nikos-skalkotos" id="tog-ivl">
							<i class="left-60 tog far fa-chevron-down"></i>
							ivl
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								K. Kontosis, N. Skalkotos, S. Nathanail
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/ivl" title="Home + download">ivl</a>
							<a class="lnk mr" href="../data/code/pdf/code/ivl.by-example.pdf" title="Article PDF">
								<i class="faa fa-file-pdf2"></i>
							</a>
						</div>
						<div class="part nw">
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/ivl" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl" title="Repository">
								<i class="fab fa-github"></i>
							</a>
							<a class="lnk ext mr2" href="https://sourceforge.net/projects/ivl" title="Download">
								<i class="faa fa-sourceforge"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/cpplibivl/ivl/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ivl-nikos-skalkotos">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							<a href="https://github.com/cpplibivl/ivl/blob/master/LICENSE">GNU LGPL2/3</a> license <span class="bull"></span>
							2007-2013
							<div class="p">
								<span class='struct'>2007:</span> S. Nathanail <br>
								<span class='struct'>2008:</span> N. Skalkotos <br>
								<span class='struct'>2009-2013:</span> K. Kontosis <br>
							</div>
							<div class="p">
								<span class='struct'>ivl-lina:</span>
								N. Skalkotos <span class="bull"></span>
								Linear algebra (<a href="http://netlib.org/lapack/">LAPACK</a>)
								<span class="bull"></span> 2008
								<br>
								<span class='struct'>ivl-cv:</span>
								K. Kontosis <span class="bull"></span>
								Computer vision (<a href="https://opencv.org/">OpenCV</a>)
								<span class="bull"></span> 2009-2010
								<br>
								<span class='struct'>ivl-qt:</span>
								K. Kontosis <span class="bull"></span>
								GUI (<a href="https://qt-project.org/">Qt</a>)
								<span class="bull"></span> 2011-2012
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/ivl.svg"><img src="../data/code/thumb/wide/code/ivl.svg" alt="ivl thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ivl a full-header template C++98 general purpose library with convenient and powerful syntax. It extends C++ syntax towards mathematical notation, while making use of language features like classes, functions, operators, templates and type safety. It allows simple and expressive statements, while taking care of the underlying representation and optimization. Often resembling a new language, it targets abstract, concise, readable, yet efficient code. It supports the principle that <i>the path from theory through rapid prototyping to production quality software should be as short as possible</i>. In fact, the actual code should not differ much from pseudocode.
								</p>
								<p>
									
								</p>
								<p>
									ivl features static and dynamic arrays, ranges, tuples, matrices, images and function objects supporting multiple return arguments, left/right overloading, function pipelining and vectorization, expression templates, automatic lazy evaluation, and dynamic multi-threading. Other features include sub-arrays and other lazy views of one- or multi-dimensional arrays and tuples, STL-compatible and multidimensional iterators, and extended compound operators. It is easy to use, with most syntax being self explanatory. It is fully optimized, with minimal or no runtime overhead, no temporaries or copies, and with most expressions boiling down to a single <i>for</i> loop.
								</p>
								<p>
									
								</p>
								<p>
									ivl <i>core</i> is a header-only library, with no need for separate linking. It is fully template, supporting user-defined types. Separate modules are available that smoothly integrate with <a href="http://netlib.org/lapack/">LAPACK</a>, <a href="https://opencv.org/">OpenCV</a> and <a href="http://qt-project.org/">Qt</a> for linear algebra, computer vision and GUI respectively. In each case, ivl shares its data representation with the underlying external library and combines its convenient syntax with a rich collection of software. Separate linking is needed for the modules used, since external libraries are not template.
								</p>
								<p>
									
								</p>
								<p>
									The library is available as open source under a dual LGPL3.0 and GPL2.0 license at <a href="https://sourceforge.net/projects/ivl/">SourceForge</a> and at its dedicated <a href="http://image.ntua.gr/ivl/">web site</a>, which includes extended examples and documentation. A unique article <a href="http://image.ntua.gr/ivl/files/ivl-by-example.pdf">ivl by example</a> explains in less than eight pages how to build a <i>randomized decision forest classifier</i> from scratch with ivl, including the complete code of just 120 lines. The article and code behave like one entity, as in <a href="http://en.wikipedia.org/wiki/Literate_programming">literate programming</a>.
								</p>
								<p>
									
								</p>
								<p>
									Over the years, ivl has been influenced by several C++ numerical libraries, for instance <a href="http://eigen.tuxfamily.org/">Eigen</a>, or <a href="http://www.boost.org/doc/libs/1_49_0/libs/multi_array/">Boost.Multi-Array</a> and <a href="http://www.boost.org/doc/libs/1_49_0/libs/tuple/">Boost.Tuple</a> for data representation and manipulation. At a more foundational level, it includes its own <i>template metaprogramming</i> library similar to <a href="http://www.boost.org/doc/libs/1_50_0/libs/mpl/doc/">Boost.MPL</a>, heavily used for code optimization. A great motivation has been the Matlab language syntax, and in this sense a related project is <a href="http://arma.sourceforge.net/">Armadillo</a>. Most of this syntax is supported, without the computational overhead and other known issues. In fact, ivl provides a unique integration of all the above functionalities.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="chull-hwan-song"></a>
					Song, Chull Hwan
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="rgldv2-clean"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-rgldv2-clean-chull-hwan-song" id="tog-rgldv2-clean">
							<i class="left-60 tog far fa-chevron-down"></i>
							Revisiting Google Landmark Dataset v2 Clean
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								C.H. Song
								(<span class="ref">advised by</span> Y.H. Gu, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/dealicious-inc/RGLDv2-clean" title="Code repository">RGLDv2-clean</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C135" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/dealicious-inc/RGLDv2-clean/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/dealicious-inc/RGLDv2-clean/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/dealicious-inc/RGLDv2-clean/stargazers" title="Stars @ Github">2</a>
						</div>

					</div>
					<div class="collapse" id="col-rgldv2-clean-chull-hwan-song">
						<div class="pub-ref">
							<a href="https://github.com/dealicious-inc/RGLDv2-clean/blob/main/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C135">CVPR 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C135.cvpr24.gld-clean.svg"><img src="../data/pub/thumb/wide/conf/C135.cvpr24.gld-clean.svg" alt="rgldv2-clean thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									How important is it for training and evaluation sets to not have class overlap in image retrieval? We revisit Google Landmarks v2 clean, the most popular training set, by identifying and removing class overlap with Revisited Oxford and Paris, the most popular evaluation set. By comparing the original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art methods, our findings are striking. Not only is there a dramatic drop in performance, but it is inconsistent across methods, changing the ranking.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="evaggelos-spyrou"></a>
					Spyrou, Evaggelos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="scene-maps"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-scene-maps-evaggelos-spyrou" id="tog-scene-maps">
							<i class="left-60 tog far fa-chevron-down"></i>
							Scene maps
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, E. Spyrou, G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Scene-maps</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/scene_maps/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-scene-maps-evaggelos-spyrou">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="http://sourceforge.net/projects/lpsolve/">LPSolve</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2010-2011
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/scene-maps.png"><img src="../data/code/thumb/wide/code/scene-maps.png" alt="scene-maps thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									<i>Scene maps</i> refers to a representation of image collections used for <a href="/pub/#C86">large scale image search and mining</a>, and applied to <a href="/pub/#J19">location and landmark recognition</a>. Starting from a geo-tagged dataset, we first group images geographically and then visually, where each visual cluster is assumed to depict different views of the the same scene. We align all views to one reference image and construct a 2D <i>scene map</i> by preserving details from all images while discarding repeating visual features. A scene map thus collectively represents a scene as seen from different viewpoints. The indexing, retrieval and spatial matching scheme then operates directly on scene maps. All clustering operations are based on <i>kernel vector quantization</i> (KVQ). <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="viral"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-viral-evaggelos-spyrou" id="tog-viral">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias, M. Phinikettos, E. Spyrou, Ph. Mylonas
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://viral.image.ntua.gr/" title="Access online">VIRaL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/viral" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-viral-evaggelos-spyrou">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2008-2012
							<div class="p">
								<span class='struct'>Application interface:</span>
								M. Phinikettos <span class="bull"></span>
								PHP, Javascript
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Core search engine:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Explore/Routes:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++, PHP, Javascript
								<span class="bull"></span> based on <a href="/code/#scene-maps">Scene-maps</a>
								<span class="bull"></span> 2011
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/app/viral.png"><img src="../data/code/thumb/wide/app/viral.png" alt="viral thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									VIRaL is a visual search engine available online since 2008. The query is an image, either uploaded, fetched from a given URL, or chosen from the its database. Given this single image, it retrieves visually similar images and estimates its location on the map. It also suggests tags that may be attached to the query image, identifies known landmarks or points of interest, and provides links to relevant Wikipedia articles. Its database contains 2.7M <a href="https://www.flickr.com/">Flickr</a> images from 43 cities in the world. It is able to recognize tens of thousands of landmarks.
								</p>
								<p>
									
								</p>
								<p>
									Additional applications enhance its user experience. <a href="http://viral.image.ntua.gr/?explore">VIRaL Explore</a> enables browsing of the entire VIRaL image collection on the world map. Starting in a given city or at any zoom level on the map, it places icons corresponding to grouped photos, along with landmark names and Wikipedia links, if applicable. Photos are grouped off-line according to whether they depict the same object, building, or scene, and most popular groups are shown on the map, according to zoom level. <a href="http://viral.image.ntua.gr/?routes">VIRaL Routes</a> offers a unique browsing experience of personal photo collections. Collections are processed off-line to identify where they were taken and group them by scene; a route is then constructed on the map, showing icons of visited places.
								</p>
								<p>
									
								</p>
								<p>
									VIRaL targets general public to demonstrate results of our research. It has been disseminated in several technical and wide-audience venues. It is a unique application, and one of the very few non-commercial CBIR engines listed by <a href="http://en.wikipedia.org/wiki/List_of_CBIR_engines">Wikipedia</a> that is really operating online.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="T"></a>
					T
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="konstantinos-tertikas"></a>
					Tertikas, Konstantinos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="partnerf"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-partnerf-konstantinos-tertikas" id="tog-partnerf">
							<i class="left-60 tog far fa-chevron-down"></i>
							Part-aware editable 3D shape generation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								K. Tertikas
								(<span class="ref">advised by</span> D. Paschalidou, J.J. Park, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/ktertikas/part_nerf" title="Code repository">PartNeRF</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C127" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://ktertikas.github.io/part_nerf" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ktertikas/part_nerf/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ktertikas/part_nerf/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/ktertikas/part_nerf/stargazers" title="Stars @ Github">42</a>
						</div>

					</div>
					<div class="collapse" id="col-partnerf-konstantinos-tertikas">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							published in <a href="/pub/#C127">CVPR 2023</a> <span class="bull"></span>
							2023
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C127.cvpr23.part-nerf.png"><img src="../data/pub/thumb/wide/conf/C127.cvpr23.part-nerf.png" alt="partnerf thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									PartNeRF is a part-aware generative model for editable 3D shape synthesis. It does not require explicit 3D or part supervision and is able to produce textures. It generates objects as a set of locally defined NeRFs, augmented with an affine transformation. This enables editing operations such as applying transformations on parts, mixing parts from different objects etc. The color of each ray is only determined by a single NeRF. As a result, altering one part does not affect the appearance of the others. The code allows the reproduction of the results of our <a href="/pub/#C127">CVPR 2023 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="giorgos-tolias"></a>
					Tolias, Giorgos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="gcc"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-gcc-giorgos-tolias" id="tog-gcc">
							<i class="left-60 tog far fa-chevron-down"></i>
							Graph convolutional cleaning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, C. Schmid, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/google-research/noisy-fewshot-learning" title="Code repository">GCC</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C114" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/google-research/noisy-fewshot-learning/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/google-research/noisy-fewshot-learning/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/google-research/noisy-fewshot-learning/stargazers" title="Stars @ Github">23</a>
						</div>

					</div>
					<div class="collapse" id="col-gcc-giorgos-tolias">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/tkipf/gcn">GCN</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> <span class="bull"></span>
							<a href="https://github.com/google-research/noisy-fewshot-learning/blob/main/LICENSE">Apache-2.0</a> license <span class="bull"></span>
							published in <a href="/pub/#C114">ECCV 2020</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/gcc.png"><img src="../data/code/thumb/wide/code/gcc.png" alt="gcc thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We learn a classifier from few clean and many noisy labels. The structure of clean and noisy data is modeled by a graph per class and <a href="https://github.com/tkipf/gcn">graph convolutional networks</a> are used to predict class relevance of noisy examples. This cleaning method is evaluated on an extended version of a <i>few-shot learning</i> problem, where the few clean examples of novel classes are supplemented with additional noisy data. The code allows the reproduction of the results of our <a href="/pub/#C114">ECCV 2020 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="mom"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-mom-giorgos-tolias" id="tog-mom">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mining on manifolds
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gtolias/mom" title="Code repository">MoM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C108" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gtolias/mom/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gtolias/mom/stargazers" title="Stars @ Github">34</a>
						</div>

					</div>
					<div class="collapse" id="col-mom-giorgos-tolias">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a>, <a href="https://github.com/vlfeat/matconvnet">MatConvNet</a> <span class="bull"></span>
							based on <a href="https://github.com/vadimkantorov/metriclearningbench">MLbench</a>, <a href="/code/#dlp">DLP</a> <span class="bull"></span>
							published in <a href="/pub/#C108">CVPR 2018</a> <span class="bull"></span>
							2017-2018
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png"><img src="../data/pub/thumb/wide/conf/C108.cvpr18.mom.png" alt="mom thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									MoM is one of the very few <i>self-supervised</i> metric learning methods. Building on findings of manifold similarity, it learns a representation space where Euclidean neighbors are determined according to manifold neighbors in the original feature space. It is applied to fine-grained classification as well as particular object retrieval.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="diffusion"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-diffusion-giorgos-tolias" id="tog-diffusion">
							<i class="left-60 tog far fa-chevron-down"></i>
							Diffusion for image retrieval
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								A. Iscen, G. Tolias
								(<span class="ref">advised by</span> O. Chum, T. Furon, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/ahmetius/diffusion-retrieval" title="Code repository">Diffusion</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C103" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/diffusion-retrieval#readme" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/ahmetius/diffusion-retrieval/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/ahmetius/diffusion-retrieval/stargazers" title="Stars @ Github">87</a>
						</div>

					</div>
					<div class="collapse" id="col-diffusion-giorgos-tolias">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							based on <a href="http://yael.gforge.inria.fr/">Yael</a> <span class="bull"></span>
							<a href="https://github.com/ahmetius/diffusion-retrieval#readme">GNU GPL3+</a> license <span class="bull"></span>
							published in <a href="/pub/#C103">CVPR 2017</a> <span class="bull"></span>
							2016-2017
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png"><img src="../data/pub/thumb/wide/conf/C103.cvpr17.diffuse.png" alt="diffusion thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Diffusion is a <a href="/pub/#C103">manifold search</a> method that uses a random walk on the nearest neighbor graph of a dataset. It has been extended to a <a href="/pub/#C106">spectral approach</a> and a <a href="/pub/#C109">hybrid variant</a> of the two for image retrieval. The code and data allows the reproduction of the results of our <a href="/pub/#C103">CVPR 2017 paper</a>. In particular, we provide the descriptors used and the necessary ground-truth files for mAP evaluation. We also make available the approximate $k$-NN graph computed off-line for large-scale datasets.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="asmk-py"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-asmk-py-giorgos-tolias" id="tog-asmk-py">
							<i class="left-60 tog far fa-chevron-down"></i>
							Aggregated selective match kernel (Python version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								T. Jenicek, G. Tolias
								(<span class="ref">advised by</span> O. Chum)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/jenicek/asmk" title="Code repository">ASMK-py</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C93" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/jenicek/asmk/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/jenicek/asmk/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/jenicek/asmk/stargazers" title="Stars @ Github">67</a>
						</div>

					</div>
					<div class="collapse" id="col-asmk-py-giorgos-tolias">
						<div class="pub-ref">
							Python <span class="bull"></span>
							based on <a href="/code/#asmk">ASMK</a>, <a href="https://github.com/filipradenovic/cnnimageretrieval-pytorch">CIR-torch</a>, <a href="https://github.com/facebookresearch/faiss">FAISS</a> <span class="bull"></span>
							<a href="https://github.com/jenicek/asmk/blob/master/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C93">ICCV 2013</a>, <a href="https://arxiv.org/abs/2007.13172">HOW</a> <span class="bull"></span>
							2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/asmk-py.svg"><img src="../data/code/thumb/wide/code/asmk-py.svg" alt="asmk-py thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a Python implementation of <a href="/code/#asmk">ASMK</a>. There are minor differences compared to the original <a href="/pub/#C93">ASMK method (ICCV 2013)</a> and <a href="/code/#asmk">Matlab implementation</a>, which are described in the <a href="https://arxiv.org/abs/2007.13172">HOW paper (ECCV 2020)</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="asmk"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-asmk-giorgos-tolias" id="tog-asmk">
							<i class="left-60 tog far fa-chevron-down"></i>
							Aggregated selective match kernel (Matlab version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias
								(<span class="ref">advised by</span> H. Jégou, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/gtolias/asmk" title="Code repository">ASMK</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C93" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J25" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/asmk/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/gtolias/asmk/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/gtolias/asmk/stargazers" title="Stars @ Github">22</a>
						</div>

					</div>
					<div class="collapse" id="col-asmk-giorgos-tolias">
						<div class="pub-ref">
							Matlab <span class="bull"></span>
							based on <a href="http://yael.gforge.inria.fr/">Yael</a> <span class="bull"></span>
							published in <a href="/pub/#C93">ICCV 2013</a>, <a href="/pub/#J25">IJCV 2016</a> <span class="bull"></span>
							2013
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C093.iccv13a.asmk.svg"><img src="../data/pub/thumb/wide/conf/C093.iccv13a.asmk.svg" alt="asmk thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									ASMK is a method for image search using local features and a combination of inverted files with compact binary descriptors. This model encompasses as special cases aggregated representations like VLAD and matching techniques such as Hamming Embedding. Making the bridge between these approaches, it takes the best of existing methods by combining an aggregation procedure with a selective match kernel. It has been a state of the art method before <a href="https://arxiv.org/abs/1404.1777">deep learning</a> and it also applies to <a href="https://arxiv.org/abs/2007.13172">CNN features</a>. The code allows the reproduction of the results of our <a href="/pub/#C93">ICCV 2013 paper</a> as well as part of the experiments of <a href="/code/#revop">revisited Oxford and Paris</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="symcity"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-symcity-giorgos-tolias" id="tog-symcity">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature Selection by Symmetry
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">SymCity</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C90" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/symcity/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-symcity-giorgos-tolias">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#hpm">HPM</a>, <a href="/code/#ivl">ivl</a> <span class="bull"></span>
							published in <a href="/pub/#C90">ACM-MM 2012</a> <span class="bull"></span>
							2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png"><img src="../data/pub/thumb/wide/conf/C090.acm12a.symcity.png" alt="symcity thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									To reduce the space required for the index in large scale search, several methods focus on feature selection based on multiple views. In practice however, most images are unique, in the sense that they depict a unique view of an object or scene in the dataset and there is nothing to compare to. SymCity selects features in such unique images by <i>self-similarity</i>. In effect, we detect repeating patterns or local symmetries and select the participating features. The method itself is a variant of <a href="/code/#hpm">HPM</a>, called <i>Hough pyramid self-matching</i> (HPSM) and maintains the same retrieval performance using only 20% of the required memory. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="wc2m"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-wc2m-giorgos-tolias" id="tog-wc2m">
							<i class="left-60 tog far fa-chevron-down"></i>
							World Cities 2M
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/wc/" title="Home + download">WC2M</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C89" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-wc2m-giorgos-tolias">
						<div class="pub-ref">
							based on <a href="https://flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C89">ICCV 2011</a> <span class="bull"></span>
							2011
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/wc2m.png"><img src="../data/code/thumb/wide/data/wc2m.png" alt="wc2m thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									WC2M Consists of 2.2M geo-tagged images from 40 cities, crawled from <a href="https://flickr.com/">Flickr</a> using geographic queries covering a window of each city center. It is meant to be used as a distractor set along with any annotated test set for image retrieval. It also includes the test set of <a href="/code/#ec1m">EC1M</a> dataset and is a superset of both <a href="/code/#ec1m">EC1M</a> and <a href="/code/#ec50k">EC50k</a>. The dataset is challenging because both the test set and the distractors mostly depict urban scenery.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="hpm"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-hpm-giorgos-tolias" id="tog-hpm">
							<i class="left-60 tog far fa-chevron-down"></i>
							Hough pyramid matching (Public version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/tools/hpm/" title="Home + download">HPM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C89" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J22" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-hpm-giorgos-tolias">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a> <span class="bull"></span>
							published in <a href="/pub/#C89">ICCV 2011</a>, <a href="/pub/#J22">IJCV 2014</a> <span class="bull"></span>
							2010-2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C089.iccv11a.hpm.png"><img src="../data/pub/thumb/wide/conf/C089.iccv11a.hpm.png" alt="hpm thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									HPM is a spatial matching method applied to geometry re-ranking for large scale search. It is based on a relaxed spatial matching model, which applies pyramid matching to the Hough transformation space. It is invariant to similarity transformations and free of inlier-count verification. It imposes one-to-one mapping and is flexible, allowing non-rigid motion and multiple matching surfaces or objects. It is linear in the number of correspondences and extremely fast in practice.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ec1m"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ec1m-giorgos-tolias" id="tog-ec1m">
							<i class="left-60 tog far fa-chevron-down"></i>
							European Cities 1M
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/ec1m/" title="Home + download">EC1M</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ec1m-giorgos-tolias">
						<div class="pub-ref">
							based on <a href="https://flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a> <span class="bull"></span>
							2010
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/ec1m.png"><img src="../data/code/thumb/wide/data/ec1m.png" alt="ec1m thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									EC1M Consists of 909k geo-tagged images from 22 European cities, crawled from <a href="https://flickr.com/">Flickr</a> using geographic queries covering a window of each city center. A subset of 1,081 images from Barcelona is annotated into 35 groups depicting the same scene; 17 of the groups are landmark scenes and 18 are non-landmark. Annotation is based respectively on tags and visual search / manual clean-up. In total, 157 of those images are defined as queries (up to 5 per group). Images of the remaining 21 cities are used as distractors. Most depict urban scenery like the ground-truth, making a challenging distractor dataset.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="scene-maps"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-scene-maps-giorgos-tolias" id="tog-scene-maps">
							<i class="left-60 tog far fa-chevron-down"></i>
							Scene maps
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, E. Spyrou, G. Tolias
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">Scene-maps</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/scene_maps/" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-scene-maps-giorgos-tolias">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="http://sourceforge.net/projects/lpsolve/">LPSolve</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2010-2011
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/scene-maps.png"><img src="../data/code/thumb/wide/code/scene-maps.png" alt="scene-maps thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									<i>Scene maps</i> refers to a representation of image collections used for <a href="/pub/#C86">large scale image search and mining</a>, and applied to <a href="/pub/#J19">location and landmark recognition</a>. Starting from a geo-tagged dataset, we first group images geographically and then visually, where each visual cluster is assumed to depict different views of the the same scene. We align all views to one reference image and construct a 2D <i>scene map</i> by preserving details from all images while discarding repeating visual features. A scene map thus collectively represents a scene as seen from different viewpoints. The indexing, retrieval and spatial matching scheme then operates directly on scene maps. All clustering operations are based on <i>kernel vector quantization</i> (KVQ). <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ec50k"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ec50k-giorgos-tolias" id="tog-ec50k">
							<i class="left-60 tog far fa-chevron-down"></i>
							European Cities 50k
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/ec50k/" title="Home + download">EC50k</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C85" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-ec50k-giorgos-tolias">
						<div class="pub-ref">
							based on <a href="https://flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C85">ACM-MM 2010</a> <span class="bull"></span>
							2010
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/ec50k.jpg"><img src="../data/code/thumb/wide/data/ec50k.jpg" alt="ec50k thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									EC50k consists of 50,767 geo-tagged images from 14 European cities, crawled from <a href="https://flickr.com/">Flickr</a> using geographic queries covering a window of each city center. A subset of 778 images from 9 cities are annotated into 20 groups depicting the same scene. Annotation is based on tags and visual search / manual clean-up. In total, 100 of those images are defined as queries (5 per group). Images of the remaining 5 cities are used as distractors. Most depict urban scenery like the ground-truth, making a challenging distractor dataset.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="fmh-prod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-fmh-prod-giorgos-tolias" id="tog-fmh-prod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Feature map hashing/similarity (Production version)
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">FMH-prod</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C85" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J23" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/research/feature_map_hashing" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-fmh-prod-giorgos-tolias">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="/code/#fmh">FMH</a> <span class="bull"></span>
							published in <a href="/pub/#C85">ACM-MM 2010</a>, <a href="/pub/#J23">CVIU 2014</a> <span class="bull"></span>
							2009-2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg"><img src="../data/pub/thumb/wide/conf/C085.acm-mm10b.fmh.jpg" alt="fmh-prod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is the production version of <a href="/code/#fmh">FMH</a>. <span class="alrt">The code is not public.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="viral"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-viral-giorgos-tolias" id="tog-viral">
							<i class="left-60 tog far fa-chevron-down"></i>
							Visual Image Retrieval and Localization
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, G. Tolias, M. Phinikettos, E. Spyrou, Ph. Mylonas
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://viral.image.ntua.gr/" title="Access online">VIRaL</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C86" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J19" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="http://image.ntua.gr/iva/tools/viral" title="Project home">
								<i class="fal fa-home"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-viral-giorgos-tolias">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/">Flickr</a> <span class="bull"></span>
							published in <a href="/pub/#C86">ACM-MM 2010</a>, <a href="/pub/#J19">MTAP 2011</a> <span class="bull"></span>
							2008-2012
							<div class="p">
								<span class='struct'>Application interface:</span>
								M. Phinikettos <span class="bull"></span>
								PHP, Javascript
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Core search engine:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++
								<span class="bull"></span> 2008-2012
								<br>
								<span class='struct'>Explore/Routes:</span>
								Y. Kalantidis, G. Tolias <span class="bull"></span>
								C++, PHP, Javascript
								<span class="bull"></span> based on <a href="/code/#scene-maps">Scene-maps</a>
								<span class="bull"></span> 2011
								<br>
							</div>
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/app/viral.png"><img src="../data/code/thumb/wide/app/viral.png" alt="viral thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									VIRaL is a visual search engine available online since 2008. The query is an image, either uploaded, fetched from a given URL, or chosen from the its database. Given this single image, it retrieves visually similar images and estimates its location on the map. It also suggests tags that may be attached to the query image, identifies known landmarks or points of interest, and provides links to relevant Wikipedia articles. Its database contains 2.7M <a href="https://www.flickr.com/">Flickr</a> images from 43 cities in the world. It is able to recognize tens of thousands of landmarks.
								</p>
								<p>
									
								</p>
								<p>
									Additional applications enhance its user experience. <a href="http://viral.image.ntua.gr/?explore">VIRaL Explore</a> enables browsing of the entire VIRaL image collection on the world map. Starting in a given city or at any zoom level on the map, it places icons corresponding to grouped photos, along with landmark names and Wikipedia links, if applicable. Photos are grouped off-line according to whether they depict the same object, building, or scene, and most popular groups are shown on the map, according to zoom level. <a href="http://viral.image.ntua.gr/?routes">VIRaL Routes</a> offers a unique browsing experience of personal photo collections. Collections are processed off-line to identify where they were taken and group them by scene; a route is then constructed on the map, showing icons of visited places.
								</p>
								<p>
									
								</p>
								<p>
									VIRaL targets general public to demonstrate results of our research. It has been disseminated in several technical and wide-audience venues. It is a unique application, and one of the very few non-commercial CBIR engines listed by <a href="http://en.wikipedia.org/wiki/List_of_CBIR_engines">Wikipedia</a> that is really operating online.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="iva"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-iva-giorgos-tolias" id="tog-iva">
							<i class="left-60 tog far fa-chevron-down"></i>
							IVA visual representation, matching and search infrastructure
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								G. Tolias, Y. Kalantidis
								(<span class="ref">advised by</span> Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">iva</span>
						</div>
						<div class="part nw">
						</div>

					</div>
					<div class="collapse" id="col-iva-giorgos-tolias">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="/code/#ivl">ivl</a>, <a href="https://opencv.org/">OpenCV</a> <span class="bull"></span>
							2008-2014
						</div>
						<div class="pub-detail">
							<div class="just">
								<p>
									This is a collection of software that has been used internally as infrastructure within the <a href="http://image.ntua.gr/iva/">IVA research team</a> for several other projects, most notably <a href="/code/#scene-maps">Scene-maps</a>, <a href="/code/#fmh">FMH</a>, <a href="/code/#hpm">HPM</a>, <a href="/code/#symcity">SymCity</a>, <a href="/code/#agm">AGM</a>, <a href="/code/#drvq">DRVQ</a> and <a href="/code/#viral">VIRaL</a>. It provides a common interface to frequently used data structures and a number of individual software components to support common tasks. Such tasks include local feature detection and descriptor computation, nearest neighbor search and clustering, aggregated representations like histograms and sparse sets used e.g. for bag-of-words and related models, matching methods including pyramid matching, algorithms like radix sort, set operations like intersection and unique element count, inverted file structures for indexing, as well as dataset organization and evaluation protocols.
								</p>
								<p>
									
								</p>
								<p>
									Most software is using <a href="/code/#ivl">ivl</a>, which has evolved itself to support the needs of the software. In many cases <a href="https://opencv.org/">OpenCV</a> is also required, but otherwise dependencies are kept to a minimum and constrained to individual components. The software includes dozens of individual components and hundreds of source files. Each component is typically accompanied by a sample project in Linux and Windows, demonstrating its use. <span class="alrt">The code is not public.</span>
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="michele-trevisiol"></a>
					Trevisiol, Michele
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="logos27"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-logos27-michele-trevisiol" id="tog-logos27">
							<i class="left-60 tog far fa-chevron-down"></i>
							Flickr Logos 27
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Kalantidis, L.G. Pueyo, M. Trevisiol
								(<span class="ref">advised by</span> R. van Zwol, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/datasets/flickr_logos/" title="Home + download">Logos27</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C87" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-logos27-michele-trevisiol">
						<div class="pub-ref">
							based on <a href="https://www.flickr.com/groups/identitydesign/">Flickr Identity + Logo Design</a> <span class="bull"></span>
							published in <a href="/pub/#C87">ICMR 2011</a> <span class="bull"></span>
							2011
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C087.icmr11.logo.png"><img src="../data/pub/thumb/wide/conf/C087.icmr11.logo.png" alt="logos27 thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is an annotated logo dataset downloaded from <a href="https://flickr.com/">Flickr</a> group <a href="https://www.flickr.com/groups/identitydesign/">Identity + Logo Design</a> and contains more than 4000 logo classes/brands in total. It consists of a training, a distractor and a query set, containing respectively 810 images with bounding boxes labeled into 27 classes, 4207 logo images/classes depicting clean logos and 270 images, half of which are annotated into 27 training classes and the other half do not depict logos.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="V"></a>
					V
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="christos-varytimidis"></a>
					Varytimidis, Christos
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="wash"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-wash-christos-varytimidis" id="tog-wash">
							<i class="left-60 tog far fa-chevron-down"></i>
							Weighted alpha-shapes
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								C. Varytimidis
								(<span class="ref">advised by</span> K. Rapantzikos, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="http://image.ntua.gr/iva/research/wash/" title="Home + download">WaSH</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C92" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk mr2" href="../pub/#J26" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-wash-christos-varytimidis">
						<div class="pub-ref">
							C++ <span class="bull"></span>
							based on <a href="https://opencv.org/">OpenCV</a>, <a href="http://www.cgal.org/">CGAL</a>, <a href="http://www.boost.org/">Boost</a> <span class="bull"></span>
							published in <a href="/pub/#C92">ECCV 2012</a>, <a href="/pub/#J26">PR 2016</a> <span class="bull"></span>
							2011-2012
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C092.eccv12b.wash.png"><img src="../data/pub/thumb/wide/conf/C092.eccv12b.wash.png" alt="wash thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									WaSH is a local feature detector. Given an input image, it computes a list of detected features, optionally with descriptors. It begins from sampled edges and is based on shape stability measures across the <i>weighted $\alpha$-filtration</i>, a computational geometry construction that captures the shape of a non-uniform set of points. Detected features are blob-like and include non-extremal regions as well as regions determined by cavities of boundary shape.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h2 class="pub rule">
					<a class="anchor" id="shashanka-venkataramanan"></a>
					Venkataramanan, Shashanka
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="dora"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-dora-shashanka-venkataramanan" id="tog-dora">
							<i class="left-60 tog far fa-chevron-down"></i>
							multi-object Discovery and tRAcking
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								S. Venkataramanan
								(<span class="ref">advised by</span> J. Carreira, Y.M. Asano, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/shashankvkt/DoRA_ICLR24" title="Code repository">DoRA</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C134" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/shashankvkt/DoRA_ICLR24/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/shashankvkt/DoRA_ICLR24/stargazers" title="Stars @ Github">26</a>
						</div>

					</div>
					<div class="collapse" id="col-dora-shashanka-venkataramanan">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/facebookresearch/dino">DINO</a> <span class="bull"></span>
							published in <a href="/pub/#C134">ICLR 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/dora.svg"><img src="../data/code/thumb/wide/code/dora.svg" alt="dora thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									DoRA is a self-supervised image pretraining method tailored for learning from continuous videos. It leverages the attention from the [CLS] token of distinct heads in a vision transformer to identify and consistently track multiple objects within a given frame across temporal sequences. On these, a teacher-student distillation loss is then applied. Importantly, we do not use any off-the-shelf object tracker or optical flow network. This keeps our pipeline simple and does not require any additional data or training. The code allows the reproduction of the results of our <a href="/pub/#C134">ICLR 2024 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="wtours"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-wtours-shashanka-venkataramanan" id="tog-wtours">
							<i class="left-60 tog far fa-chevron-down"></i>
							Walking Tours
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								S. Venkataramanan
								(<span class="ref">advised by</span> J. Carreira, Y.M. Asano, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://uvaauas.figshare.com/articles/dataset/Dora_WalkingTours_Dataset_ICLR_2024_/25189275" title="Home + download">WTours</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C134" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://shashankvkt.github.io/dora" title="Project home">
								<i class="fal fa-home"></i>
							</a>
							<a class="lnk ext mr2" href="https://huggingface.co/datasets/shawshankvkt/Walking_Tours" title="Data repository">
								<i class="fal fa-cloud"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-wtours-shashanka-venkataramanan">
						<div class="pub-ref">
							published in <a href="/pub/#C134">ICLR 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/data/wtours.svg"><img src="../data/code/thumb/wide/data/wtours.svg" alt="wtours thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									The Walking Tours dataset is a unique collection of long-duration egocentric videos captured in urban environments from cities in Europe and Asia. It consists of 10 high-resolution videos, each showcasing a person walking through a different environment, ranging from city centers to parks to residential areas, under different lighting conditions. A video from a Wildlife safari is also included to diversify the dataset with natural environments. The dataset is completely unlabeled and uncurated, making it suitable for self-supervised pretraining.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="metrix"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-metrix-shashanka-venkataramanan" id="tog-metrix">
							<i class="left-60 tog far fa-chevron-down"></i>
							Mixup for Deep Metric Learning
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								B. Psomas, S. Venkataramanan
								(<span class="ref">advised by</span> E. Kijak, L. Amsaleg, K. Karantzalos, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/billpsomas/Metrix_ICLR22" title="Code repository">Metrix</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C123" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/billpsomas/Metrix_ICLR22/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/billpsomas/Metrix_ICLR22/stargazers" title="Stars @ Github">3</a>
						</div>

					</div>
					<div class="collapse" id="col-metrix-shashanka-venkataramanan">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/tjddus9597/Proxy-Anchor-CVPR2020">Proxy Anchor</a>, <a href="https://github.com/KevinMusgrave/pytorch-metric-learning">PyTorch Metric Learning</a>, <a href="https://github.com/billpsomas/Deep_Metric_Learning_Pytorch">DML Benchmark</a> <span class="bull"></span>
							published in <a href="/pub/#C123">ICLR 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/metrix.svg"><img src="../data/code/thumb/wide/code/metrix.svg" alt="metrix thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Metric Mix, or Metrix, is an algorithm for mixup-based interpolation as a data augmentation method for metric learning. It uses a generalized formulation that encompasses existing metric learning loss functions that is modified to accommodate for mixup. Mixing takes place at the input space, intermediate representations as well as the embedding space. It refers to both examples and target labels. The code allows the reproduction of the results of our <a href="/pub/#C123">ICLR 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="alignmix"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-alignmix-shashanka-venkataramanan" id="tog-alignmix">
							<i class="left-60 tog far fa-chevron-down"></i>
							Aligned feature interpolation
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								S. Venkataramanan
								(<span class="ref">advised by</span> E. Kijak, L. Amsaleg, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/shashankvkt/AlignMixup_CVPR22" title="Code repository">AlignMix</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#C124" title="Conference paper">
								<i class="fal fa-books"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/shashankvkt/AlignMixup_CVPR22/blob/master/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/shashankvkt/AlignMixup_CVPR22/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/shashankvkt/AlignMixup_CVPR22/stargazers" title="Stars @ Github">66</a>
						</div>

					</div>
					<div class="collapse" id="col-alignmix-shashanka-venkataramanan">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/dfdazac/wassdistance">WassDistance</a> <span class="bull"></span>
							<a href="https://github.com/shashankvkt/AlignMixup_CVPR22/blob/master/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#C124">CVPR 2022</a> <span class="bull"></span>
							2022
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/conf/C124.cvpr22.alignmix.svg"><img src="../data/pub/thumb/wide/conf/C124.cvpr22.alignmix.svg" alt="alignmix thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a mixup-based data augmentation method, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. The code allows the reproduction of the results of our <a href="/pub/#C124">CVPR 2022 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="X"></a>
					X
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="yonghao-xu"></a>
					Xu, Yonghao
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="ut-kd"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-ut-kd-yonghao-xu" id="tog-ut-kd">
							<i class="left-60 tog far fa-chevron-down"></i>
							Multi-Target Unsupervised Domain Adaptation without External Data
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Y. Xu
								(<span class="ref">advised by</span> P. Ghamisi, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/YonghaoXu/UT-KD" title="Code repository">UT-KD</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#R51" title="Technical report">
								<i class="fal fa-memo"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/YonghaoXu/UT-KD/blob/main/LICENSE" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/YonghaoXu/UT-KD/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/YonghaoXu/UT-KD/stargazers" title="Stars @ Github">2</a>
						</div>

					</div>
					<div class="collapse" id="col-ut-kd-yonghao-xu">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/wasidennis/AdaptSegNet">AdaptSegNet</a>, <a href="https://github.com/abhiskk/fast-neural-style">fast-neural-style</a>, <a href="https://github.com/YonghaoXu/SE-GAN">SE-GAN</a> <span class="bull"></span>
							<a href="https://github.com/YonghaoXu/UT-KD/blob/main/LICENSE">MIT</a> license <span class="bull"></span>
							published in <a href="/pub/#R51">arXiv 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/ut-kd.svg"><img src="../data/code/thumb/wide/code/ut-kd.svg" alt="ut-kd thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We introduce a new strategy called for semantic segmentation "multi-target UDA without external data". The segmentation model is initially trained on the external data. Then, it is adapted to a new unseen target domain without accessing any external data. This approach is thus more scalable than existing solutions and remains applicable when external data is inaccessible. We demonstrate this strategy using a simple method, "unseen target knowledge distillation" (UT-KD), that incorporates self-distillation and adversarial learning, where knowledge acquired from the external data is preserved during adaptation through "one-way" adversarial learning. The code allows the reproduction of the results of our <a href="/pub/#R51">arXiv 2024 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="Y"></a>
					Y
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="zhaohui-yang"></a>
					Yang, Zhaohui
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="nsod"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-nsod-zhaohui-yang" id="tog-nsod">
							<i class="left-60 tog far fa-chevron-down"></i>
							Nano-supervised object detection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								Z. Yang
								(<span class="ref">advised by</span> M. Shi, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<span class="empty but mr">NSOD</span>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#J31" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-nsod-zhaohui-yang">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/hbilen/WSDDN">WSDDN</a>, <a href="https://github.com/ppengtang/pcl.pytorch">PCL</a> <span class="bull"></span>
							published in <a href="/pub/#J31">PR 2021</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/code/thumb/wide/code/nsod.svg"><img src="../data/code/thumb/wide/code/nsod.svg" alt="nsod thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									We learn an object detector from few weakly-labeled images and a larger set of completely unlabeled images. The main idea is to learn a classifier first in a semi-supervised setting, then use it as a teacher to train a student network on a weakly-supervised object detection task. The student detector is based on <a href="https://github.com/ppengtang/pcl.pytorch">PCL</a> weakly supervised object detector. <span class="alrt">The code is not public yet.</span>
								</p>
							</div>
						</div>
					</div>
				</div>

				<h1 class="pub">
					<a class="anchor" id="Z"></a>
					Z
				</h1>
				<h2 class="pub rule">
					<a class="anchor" id="hanwei-zhang"></a>
					Zhang, Hanwei
				</h2>
				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="opti-cam"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-opti-cam-hanwei-zhang" id="tog-opti-cam">
							<i class="left-60 tog far fa-chevron-down"></i>
							Opti-CAM Saliency Maps for Interpretability
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								H. Zhang
								(<span class="ref">advised by</span> R. Sicre, Y. Avrithis, S. Ayache)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/hanwei0912/OptiCAM/tree/main" title="Code repository">Opti-CAM</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#J32" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
						</div>

					</div>
					<div class="collapse" id="col-opti-cam-hanwei-zhang">
						<div class="pub-ref">
							<a href="https://pytorch.org/">PyTorch</a> <span class="bull"></span>
							based on <a href="https://github.com/jacobgil/pytorch-grad-cam">PyTorch-GradCAM</a> <span class="bull"></span>
							published in <a href="/pub/#J32">CVIU 2024</a> <span class="bull"></span>
							2024
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J32.cviu24.opti-cam.png"><img src="../data/pub/thumb/wide/journ/J32.cviu24.opti-cam.png" alt="opti-cam thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									Opti-CAM combines ideas from CAM-based and masking-based approaches for interpretability. It produces a saliency map suggesting what regions are responsible for a classifier prediction. The saliency map is a linear combination of feature maps, where weights are optimized per image such that the logit of the masked image for a given class is maximized. The code allows the reproduction of the results of our <a href="/pub/#J32">CVIU 2024 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="bp"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-bp-hanwei-zhang" id="tog-bp">
							<i class="left-60 tog far fa-chevron-down"></i>
							Boundary projection
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								H. Zhang
								(<span class="ref">advised by</span> T. Furon, L. Amsaleg, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/hanwei0912/walking-on-the-edge-fast-low-distortion-adversarial-examples" title="Code repository">BP</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#J30" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/hanwei0912/walking-on-the-edge-fast-low-distortion-adversarial-examples#readme" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/hanwei0912/walking-on-the-edge-fast-low-distortion-adversarial-examples/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/hanwei0912/walking-on-the-edge-fast-low-distortion-adversarial-examples/stargazers" title="Stars @ Github">7</a>
						</div>

					</div>
					<div class="collapse" id="col-bp-hanwei-zhang">
						<div class="pub-ref">
							<a href="https://www.tensorflow.org/">TensorFlow</a> <span class="bull"></span>
							based on <a href="https://github.com/tensorflow/cleverhans">CleverHans</a> <span class="bull"></span>
							<a href="https://github.com/hanwei0912/walking-on-the-edge-fast-low-distortion-adversarial-examples#readme">GNU GPL2+</a> license <span class="bull"></span>
							published in <a href="/pub/#J30">TIFS 2021</a> <span class="bull"></span>
							2019-2020
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J30.tifs20.bp.png"><img src="../data/pub/thumb/wide/journ/J30.tifs20.bp.png" alt="bp thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									BP is an adversarial attack that reduces the distortion of the perturbation while operating under quantization at very few iterations. The attack is also used to build more robust models by using BP in adversarial training as a defense. The code allows the reproduction of the results of our <a href="/pub/#J30">TIFS 2021 paper</a>.
								</p>
							</div>
						</div>
					</div>
				</div>

				<div class="pub-entry">
					<div class="pub-title">
						<a class="anchor" id="sae"></a>
						<a class="rel toggle collapsed" data-toggle="collapse" href="#col-sae-hanwei-zhang" id="tog-sae">
							<i class="left-60 tog far fa-chevron-down"></i>
							Smooth adversarial examples
						</a>
					</div>
					<div class="pub-author">
						<div class="part mr">
							<span class="author">
								H. Zhang
								(<span class="ref">advised by</span> T. Furon, L. Amsaleg, Y. Avrithis)
							</span>
						</div>
						<div class="part nw">
							<a class="acro but mr" href="https://github.com/hanwei0912/SmoothAdversarialExamples" title="Code repository">SAE</a>
						</div>
						<div class="part nw">
							<a class="lnk mr2" href="../pub/#J29" title="Journal paper">
								<i class="fal fa-book-open"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/hanwei0912/SmoothAdversarialExamples#readme" title="License">
								<i class="fal fa-scale-balanced"></i>
							</a>
							<a class="lnk ext mr2" href="https://github.com/hanwei0912/SmoothAdversarialExamples/stargazers" title="Stars @ Github">
								<i class="fal fa-star"></i>
							</a>
							<a class="cite dense strong lnk ext ml-gs mr2" href="https://github.com/hanwei0912/SmoothAdversarialExamples/stargazers" title="Stars @ Github">6</a>
						</div>

					</div>
					<div class="collapse" id="col-sae-hanwei-zhang">
						<div class="pub-ref">
							<a href="https://www.tensorflow.org/">TensorFlow</a>, Matlab <span class="bull"></span>
							based on <a href="https://github.com/tensorflow/cleverhans">CleverHans</a>, <a href="https://github.com/carlini/nn_robust_attacks">C&W</a> <span class="bull"></span>
							<a href="https://github.com/hanwei0912/SmoothAdversarialExamples#readme">GNU GPL2+</a> license <span class="bull"></span>
							published in <a href="/pub/#J29">JIS 2020</a> <span class="bull"></span>
							2018-2019
						</div>
						<div class="pub-detail">
							<a href="../data/pub/thumb/wide/journ/J29.jis20.smooth.svg"><img src="../data/pub/thumb/wide/journ/J29.jis20.smooth.svg" alt="sae thumbnail" loading="lazy"></a>
							<div class="just">
								<p>
									This is a particular form of photorealistic on-manifold adversarial examples that are actually more effective than ordinary off-manifold examples, despite the spatial constraints: our attack has the same probability of success at lower distortion. The perturbation is locally smooth on the flat areas of the input image, but it may be noisy on its textured areas and sharp across its edges. This operation relies on Laplacian smoothing, which we integrate in the attack pipeline. The code allows the reproduction of the results of our <a href="/pub/#J29">JIS 2020 paper</a>.
								</p>
								<p>
									
								</p>
							</div>
						</div>
					</div>
				</div>


			</div>
		</main>

	</div>
</div>
<footer class="bd-footer">
	<div class="container-fluid p-3 p-md-5">
		<p>
			Built using <a href="https://shopify.github.io/liquid/">Liquid</a>, <a href="https://jekyllrb.com/">Jekyll</a> and <span class="bright">my own code</span> on top
			<span class="bull">
			</span> Styled using <a href="https://getbootstrap.com/">Bootstrap</a>, <a href="https://www.mathjax.org/">MathJax</a>, <a href="https://fontawesome.com/">Font Awesome</a> and <a href="https://fonts.google.com/specimen/Roboto/">Google Fonts</a>
		</p>
		<p>
			Hosted by <a href="https://github.com/iavr/iavr.github.io">GitHub</a>
			<span class="bull"></span>
			Domain by <a href="https://www.name.com/">name.com</a>
			<span class="bull"></span>
			Last updated <span class="bright">Nov 09, 2024</span>
		</p>
		<p>
			Copyright notice applies to all <a href="../pub/#copyright">publications</a>
			<span class="bull"></span>
			All remaining material licensed <a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>
		</p>
	</div>
</footer>

	</body>
</html>